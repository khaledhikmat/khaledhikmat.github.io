<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<link rel="self" href="http://khaledhikmat.github.io/" />
	<id>http://khaledhikmat.github.io/</id>
	<title>Khaled Hikmat</title>
	<rights>2018</rights>
	<updated>2018-02-20T10:41:39Z</updated>
	<logo>http://khaledhikmat.github.io/images/liquid.jpg</logo>
	<subtitle>Coding Thoughts</subtitle>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2018-01-06-kubernetes-vs-service-fabric" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2018-01-06-kubernetes-vs-service-fabric</id>
		<title>Kubernetes vs. Service Fabric</title>
		<updated>2018-01-06T00:00:00Z</updated>
		<content>&lt;p&gt;Having been exposed to &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-overview"&gt;Microsoft's Service Fabric&lt;/a&gt;, the following are some of my notes about both platforms:&lt;/p&gt;
&lt;h2 id="similarities"&gt;Similarities&lt;/h2&gt;
&lt;p&gt;They are both orchestrators and pretty much can handle:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Container Images Hosting&lt;/li&gt;
&lt;li&gt;Scaling&lt;/li&gt;
&lt;li&gt;Healing&lt;/li&gt;
&lt;li&gt;Monitoring&lt;/li&gt;
&lt;li&gt;Rolling Updates&lt;/li&gt;
&lt;li&gt;Service Location&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However, Service Fabric support for containers came recently. Initially, Azure Service Fabric was mostly an orchestrator for .NET processes running Windows only.&lt;/p&gt;
&lt;h2 id="strengths"&gt;Strengths&lt;/h2&gt;
&lt;h4 id="kubernetes"&gt;Kubernetes:&lt;/h4&gt;
&lt;p&gt;In 2017, k8s became an industry standard. Every Cloud vendor offers full support and some offer Kubernetes as a Service such as Azure Kubernetes Service (AKS) where the vendor takes care of the cluster creation, maintenance and management.&lt;/p&gt;
&lt;p&gt;Given this, it became obvious that if any business is planning to adopt Microservices as a developmemt strategy, they are most likely thinking about employing Kubernetes. Managed services offered by many Cloud vendors such as Azure AKS makes this decison a lot easier as dvelopers no longer have to worry about provisioning or maintaining k8s clusters.&lt;/p&gt;
&lt;p&gt;Besides the huge developer and industry support that it is currently receiving, K8s is a joy to work with. Deployments can be described in yaml or json and thrown at the cluster so it can make sure that the desired state is realized. Please refer to &lt;a href="2018-01-04-netpp-docker-k8s"&gt;this post&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;Not sure about these:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can k8s create singletons?&lt;/li&gt;
&lt;li&gt;Can I have multiple depoyments of the same Docker instance with different enviroment variables?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="service-fabric"&gt;Service Fabric:&lt;/h4&gt;
&lt;p&gt;In my opinion, one of the most differentiating factor for Service Fabric is its developer-friendly programming model. It supports reliable stateless, stateful and actor models in a powerful yet abstracted way which makes programming in Service Fabric safe and easy. Please refer to earlier posts &lt;a href="2016-12-15-service-fabric-fundamentals"&gt;here&lt;/a&gt; and &lt;a href="2017-01-10-service-fabric-notes"&gt;here&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;In addition, Service Fabric supports different ways to host code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guest Executable i.e. unmodified Win32 applications or services&lt;/li&gt;
&lt;li&gt;Guest Windows and Linux containers&lt;/li&gt;
&lt;li&gt;Reliable stateless and stateful services in .NET C#, F# and Java&lt;/li&gt;
&lt;li&gt;Reliable actors in .NET C#, F# and Java&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The concept of app in Service Fabric is quite sophisticated allowing developers to create an app type and instantiate many copies of it making the concept work well in multi-tenant deployments.&lt;/p&gt;
&lt;h2 id="weaknesses"&gt;Weaknesses&lt;/h2&gt;
&lt;h4 id="kubernetes-1"&gt;Kubernetes:&lt;/h4&gt;
&lt;p&gt;I am not in a position to state any weaknesses for k8s. But, from (perhaps) a very naive perspective, I would say that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The lack of standard programming models is definitely something that can be improved.&lt;/li&gt;
&lt;li&gt;K8s can only work with containers! So any piece of code that must deployed to k8s must be containerized first.&lt;/li&gt;
&lt;li&gt;Currently k8s is only a Linux orchestrator. Although a beta 1.9 version is said to support Windows containers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="service-fabric-1"&gt;Service Fabric&lt;/h4&gt;
&lt;p&gt;One of the major setbacks for Service Fabric (or at least the public version that was made available) is that it was conceived at a time when k8s is burgeoning into an industry standard. It is becoming very difficult for Microsoft to convince developers and businesses to adopt this semi-proprieytary platform when they can use k8s.&lt;/p&gt;
&lt;p&gt;There are also a couple of things that can be improved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Service Fabric relies on XML documents to describe services and configuration.&lt;/li&gt;
&lt;li&gt;Reliance on Visual Studio although it is possible to do things in Service Fabric without Visual Studio as demonstrated &lt;a href="2016-12-02-service-fabric-basics"&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="future"&gt;Future&lt;/h2&gt;
&lt;p&gt;I love Service Fabric! But unfortunately (I say unfortyantely because I actually did spend a lot of time on it), I don't think it has a particularly great future given the strength and the momentum of k8s. Ideally Microsoft should continue to suppprt k8s in a strong way, perhaps port its Service Fabric programming model to work on k8s using .NET Core and eventually phase out Service Fabric.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Having been exposed to &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-overview"&gt;Microsoft's Service Fabric&lt;/a&gt;, the following are some of my notes about both platforms:&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2018-01-04-netapp-docker-k8s" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2018-01-04-netapp-docker-k8s</id>
		<title>.NET Core, Docker and Kubernetes</title>
		<updated>2018-01-04T00:00:00Z</updated>
		<content>&lt;p&gt;I wanted to assess Azure Cosmos DB to see possibilities of converting some of our backend applications to use this database technology to provide a globally distributed database that we desperately need as our services now cover a bigger geographic location. However, this post is not about Cosmos DB specifically! It is mainly about notes about the architecture of the pieces that surround Cosmos and how I am thinking to implement them.&lt;/p&gt;
&lt;h2 id="macro-architecture"&gt;Macro Architecture&lt;/h2&gt;
&lt;p&gt;This is how I imagines our system to look like:&lt;/p&gt;
&lt;p&gt;&lt;img src="images/avalon-architecture.png" class="img-fluid" alt="Avalon Macro Architecture" /&gt;&lt;/p&gt;
&lt;p&gt;In this post, I would like to concentrate only on the components that are boxed in orange. Namely, the Web API layer and the two processors.&lt;/p&gt;
&lt;h2 id="azure-app-service-docker-kubernetes"&gt;Azure App Service, Docker &amp;amp; Kubernetes&lt;/h2&gt;
&lt;p&gt;We have been using Azure App Service for a while and we are happy with it. We have experience in managing it, scaling it and configuring it. I did not want to change that. So I decided to continue to use Azure App Service to host our Web API layer which will be written in .NET Core. This layer communicates with Cosmos DB to provide a multi-regional access to our customers.&lt;/p&gt;
&lt;p&gt;I wanted to monitor Cosmos DB changes to launch different Microservices in the form of Azure Functions, Logic Apps or other processes. I could use Azure Functions to track the Cosmos DB changes but I decided to write my own little .NET Core stand-alone Console app using the Microsoft Change Feed library which makes things quite easy.&lt;/p&gt;
&lt;p&gt;Normally, I use WebJobs to handle queue processing and I do have a lot of experience with this. However, in .NET Core, the deployment of a WebJob is not very clear to me so I decided to write a stand-alone console app based on WebJobs SDK but can be deployed somewhere else.&lt;/p&gt;
&lt;p&gt;To host and deploy the two stand-alone .NET core console apps i.e. Change Feeder and Queue Processor, opted to make them Docker images and deploy them to a Kubernetes cluster.&lt;/p&gt;
&lt;h2 id="containerizing-the-webjobs-console-app"&gt;Containerizing the WebJobs Console App&lt;/h2&gt;
&lt;p&gt;I based my code on this &lt;a href="http://matt-roberts.me/azure-webjobs-in-net-core-2-with-di-and-configuration/"&gt;blog post by Matt Roberts&lt;/a&gt;. My solution has several projects...but two are important for this step: &lt;code&gt;AvalonWebJobs&lt;/code&gt; and &lt;code&gt;AvalonWebDal&lt;/code&gt;. &lt;code&gt;AvalonWebDal&lt;/code&gt; is a class library that has common functionality that I depend on. I used the following Docker file to build the WebJobs console app:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM microsoft/dotnet:2.0-sdk as build

WORKDIR /app

COPY . .

WORKDIR /app/AvalonWebDal
RUN dotnet restore

WORKDIR /app/AvalonWebJobs
RUN dotnet restore

RUN dotnet publish --output /output --configuration Release

FROM microsoft/dotnet:2.0-runtime

COPY --from=build /output /app

WORKDIR /app

ENTRYPOINT [ &amp;quot;dotnet&amp;quot;, &amp;quot;AvalonWebJobs.dll&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I used the following Docker command to build the image:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build --file docketfile.webjobs --no-cache -t avalonwebjobs .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the following to test locally:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -ti avalonwebjobs
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="containerizing-the-change-feeder-app"&gt;Containerizing the Change Feeder App&lt;/h2&gt;
&lt;p&gt;I based my code on this &lt;a href="https://docs.microsoft.com/en-us/azure/cosmos-db/change-feed#change-feed-processor"&gt;documentation post by Microsoft&lt;/a&gt;. My solution has several projects...but two are important for this step: &lt;code&gt;AvalonChangeFeeder&lt;/code&gt; and &lt;code&gt;AvalonWebDal&lt;/code&gt;. &lt;code&gt;AvalonWebDal&lt;/code&gt; is a class library that has common functionality that I depend on. I used the following Docker file to build the WebJobs console app:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM microsoft/dotnet:2.0-sdk as build

WORKDIR /app

COPY . .

WORKDIR /app/AvalonWebDal
RUN dotnet restore

WORKDIR /app/AvalonChangeFeeder
RUN dotnet restore

RUN dotnet publish --output /output --configuration Release

FROM microsoft/dotnet:2.0-runtime

COPY --from=build /output /app

WORKDIR /app

ENTRYPOINT [ &amp;quot;dotnet&amp;quot;, &amp;quot;AvalonChangeFeeder.dll&amp;quot; ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I used the following Docker command to build the image:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker build --file docketfile.changefeeder --no-cache -t avalonchangefeeder .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the following to test locally:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -ti avalonchangefeeder
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Important note:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Make sure that the &lt;code&gt;.csproj&lt;/code&gt; project file contains the following item groups so that the appsettings will be available in the container. Failure to do so will cause an error message &lt;code&gt;unable to find appsettngs.json and it is not optional&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ItemGroup&amp;gt;
    &amp;lt;None Remove=&amp;quot;appsettings.json&amp;quot; /&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;

  &amp;lt;ItemGroup&amp;gt;
    &amp;lt;Content Include=&amp;quot;appsettings.json&amp;quot;&amp;gt;
      &amp;lt;CopyToPublishDirectory&amp;gt;PreserveNewest&amp;lt;/CopyToPublishDirectory&amp;gt;
    &amp;lt;/Content&amp;gt;
  &amp;lt;/ItemGroup&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="kubernetes"&gt;Kubernetes&lt;/h2&gt;
&lt;p&gt;I used &lt;a href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough"&gt;this documentation link&lt;/a&gt; to spawn a test 1-node k8s cluster in Azure. The process is really simple and quick. I tagged and published my two container images to an Azure Container Registry using &lt;a href="https://docs.microsoft.com/en-us/azure/container-registry/container-registry-get-started-azure-cli"&gt;this documentaion link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now time to actually deploy to the k8s cluster.&lt;/p&gt;
&lt;h3 id="deployments"&gt;Deployments&lt;/h3&gt;
&lt;p&gt;Becasue I wanted to scale the two web jobs and the change feeder separately, I opted to create two deployments: one for the web jobs and another for the change feeder. Alternatively, I could have used a two-pod deployment but this will have meant that my two containers will need to be scaled the same since the unit of scaling in k8s is the deployment ...not the pod. Please refer to &lt;a href="https://stackoverflow.com/questions/43217006/kubernetes-multi-pod-deployment"&gt;this stack overflow issue&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;I used the following &lt;code&gt;.yml&lt;/code&gt; file for the WebJobs k8s deployment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: avalon-webjobs-deploy
spec:
  replicas: 1
  minReadySeconds: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    metadata:
      labels:
        app: avalon-app
    spec:
      containers:
      - name: avalon-webjobs-pod
        image: avalonacr.azurecr.io/avalonwebjobs:v1
        imagePullPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the following &lt;code&gt;kubectl&lt;/code&gt; command to deploy:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f avalon-webjobs-app.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I used the following &lt;code&gt;.yml&lt;/code&gt; file for the Change feeder k8s deployment:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: avalon-changefeeder-deploy
spec:
  replicas: 1
  minReadySeconds: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    metadata:
      labels:
        app: avalon-app
    spec:
      containers:
      - name: avalon-changefeeder-pod
        image: avalonacr.azurecr.io/avalonchangefeeder:v1
        imagePullPolicy: Always
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the following &lt;code&gt;kubectl&lt;/code&gt; command to deploy:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f avalon-changefeeder-app.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="observations"&gt;Observations&lt;/h3&gt;
&lt;p&gt;Initialy I set the number of replicas to one so I can make sure that everything is running well before I scale the k8s deployments.&lt;/p&gt;
&lt;p&gt;After the deployment as above, I used &lt;code&gt;kubectl get pods&lt;/code&gt; to see the status of the nodes. I noticed that the webjobs pod is in &lt;code&gt;running&lt;/code&gt; state (which is desired) while the change feeder container is in &lt;code&gt;CrashLoopBackOff&lt;/code&gt; state. Humm....after some reasearch, I found &lt;a href="https://stackoverflow.com/questions/41604499/my-kubernetes-pods-keep-crashing-with-crashloopbackoff-but-i-cant-find-any-lo"&gt;this helpful stack overflow issue&lt;/a&gt;. So I used the following command to see the actual console logs:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl logs avalon-changefeeder-deploy-1476356743-35g55  -c avalon-changefeeder-pod
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After fiddling with it for a while, I discovered the problem has to do with how the change feeder console app was coded. It uses a typical &lt;code&gt;Console.ReadKey&lt;/code&gt; method to make the application run until a key is pressed. So I modified my code based on &lt;a href="https://carlos.mendible.com/2017/10/15/prepare-a-net-core-console-app-for-docker/"&gt;this useful code snippet&lt;/a&gt;, rebuild my container and re-deployed and yes....the change feeder pod is in &lt;code&gt;running&lt;/code&gt; state.&lt;/p&gt;
&lt;p&gt;It took me a while to get the above to work. This is because I was updating the image in the container registry without changing the tag. This did not force a re-pull and I ended up not using the latest image that I was deploying to the container. It seems that k8s caches the images unless you add to the deployment file a &lt;code&gt;imagePullPolicy: Always&lt;/code&gt; in the container spec. Doing so forces k8s to re-pull the image. The other option is to change the image tag.&lt;/p&gt;
&lt;p&gt;Now things look better ...but there is another problem. Upon termination, the change feeder container needs to de-register and clean up some resources. Unfortunately I noticed when I issue a &lt;code&gt;docker stop &amp;lt;id&amp;gt;&lt;/code&gt;, the process is abruplty terminated and there is no change for the change feeder thread to clean up. I found a good article that describes how to &lt;a href="https://www.ctl.io/developers/blog/post/gracefully-stopping-docker-containers/"&gt;gracefully stop Docker containers&lt;/a&gt; which goes into some detail to describe the best way to handle it. However, since I am using a .NET Core app, I really did not find an easy way to handle the two Linux signales: &lt;code&gt;SIGINT&lt;/code&gt; and &lt;code&gt;SIGTERM&lt;/code&gt;. I did find a lot of discussions about it &lt;a href="https://github.com/aspnet/Hosting/issues/870"&gt;here&lt;/a&gt;. As it stands now, if I run the container in an intercative mode using &lt;code&gt;docker run --rm -ti avalonchangefeeder&lt;/code&gt;, for example, and then perform control-c or control-break, the container shuts down gracefully. However, if I issue a &lt;code&gt;docker stop&lt;/code&gt;, the container abruplty exists without giving any chance for the change feeder to do any cleanup :-(&lt;/p&gt;
&lt;h2 id="application-insights"&gt;Application Insights&lt;/h2&gt;
&lt;p&gt;I used &lt;code&gt;Application Insights&lt;/code&gt; to trace and track what is happening inside the container. This provide a really powerful way to monitor what is happening inside the container. I noticed that the Application Insights has a Kubernetes extension....but I am not using it yet.&lt;/p&gt;
&lt;h2 id="scaling"&gt;Scaling&lt;/h2&gt;
&lt;p&gt;Now that everything is in Kubernetes, we can scale the deployments up and down as we need. Scaling the web jobs deployment up to multiple replicas provide several consumers of the queue and scaling the change feeder deployment up to multiple replicas automatically adjusts the divide the work among themselves.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I wanted to assess Azure Cosmos DB to see possibilities of converting some of our backend applications to use this database technology to provide a globally distributed database that we desperately need as our services now cover a bigger geographic location. However, this post is not about Cosmos DB specifically! It is mainly about notes about the architecture of the pieces that surround Cosmos and how I am thinking to implement them.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2018-01-02-azure-cli-login-options" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2018-01-02-azure-cli-login-options</id>
		<title>Azure CLI Notes</title>
		<updated>2018-01-02T00:00:00Z</updated>
		<content>&lt;p&gt;This is just a note about &lt;a href="https://docs.microsoft.com/en-us/cli/azure/overview?view=azure-cli-latest"&gt;Azure CLI&lt;/a&gt; login options.&lt;/p&gt;
&lt;h2 id="option-1"&gt;Option 1:&lt;/h2&gt;
&lt;p&gt;Login interactively via a browser&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;az login
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="option-2"&gt;Option 2:&lt;/h2&gt;
&lt;p&gt;The best way to login is to use an Azure Service Principal though. So I registered an application in Azure Directory i.e. &lt;code&gt;AzureCliScriptApp&lt;/code&gt; and assigned a service principal. I will use this service principal to login.&lt;/p&gt;
&lt;h3 id="create-a-service-principal"&gt;Create a service principal:&lt;/h3&gt;
&lt;p&gt;Make sure you are in the same tenant that you want to authenticate against. If not, use 'az account set --subscription &amp;quot;your-subs&amp;quot;' to set the account.&lt;/p&gt;
&lt;p&gt;To display the Azure Directory apps:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;az ad app list --display-name AzureCliScriptApp
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above will yield the app id ...a big string that looks like this: &lt;code&gt;e68ab97f-cff2-4b50-83d5-eec9fe266ccc&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;az ad sp create-for-rbac --name e68ab97f-cff2-4b50-83d5-eec9fe266ccc --password s0me_passw0rd
{
  &amp;quot;appId&amp;quot;: &amp;quot;some-app-id-you-will-use-to-sign-in&amp;quot;,
  &amp;quot;displayName&amp;quot;: &amp;quot;e68ab97f-cff2-4b50-83d5-eec9fe266ccc&amp;quot;,
  &amp;quot;name&amp;quot;: &amp;quot;http://e68ab97f-cff2-4b50-83d5-eec9fe266ccc&amp;quot;,
  &amp;quot;password&amp;quot;: &amp;quot;s0me_passw0rd&amp;quot;,
  &amp;quot;tenant&amp;quot;: &amp;quot;your-tenant-id&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To login with service principal:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;az login --service-principal -u some-app-id-you-will-use-to-sign-in -p s0me_passw0rd --tenant your-tenant-id
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="useful-commands"&gt;Useful Commands:&lt;/h2&gt;
&lt;p&gt;List all subscriptions&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;az account list --output table
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Set the default account&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;az account set --subscription &amp;quot;Mosaic&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;List the Clouds&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;az cloud list --output table
az cloud show --name AzureCloud --output json
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="kubernetes"&gt;Kubernetes&lt;/h2&gt;
&lt;p&gt;if you are using the Azure CLI to provision a Kubernetes cluster, you should use this command if you used the service principal to login&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;az aks create --resource-group $rgName --name $k8sClusterName --service-principal $spAppId --client-secret $spPassword --node-count $k8sNodesCount --generate-ssh-keys
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where:
&lt;code&gt;$rgName&lt;/code&gt; is the PowerShell variable that holds the resource group name
&lt;code&gt;$k8sClusterName&lt;/code&gt; is the PowerShell variable that holds the k8s cluster name
&lt;code&gt;$spAppId&lt;/code&gt; is the PowerShell variable that holds the service principal app id
&lt;code&gt;$spPassword&lt;/code&gt; is the PowerShell variable that holds the service principal password
&lt;code&gt;$k8sNodesCount&lt;/code&gt; is the PowerShell variable that holds the k8s cluster desired nodes count&lt;/p&gt;
&lt;p&gt;Refer to this &lt;a href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-service-principal"&gt;doc&lt;/a&gt; for more information&lt;/p&gt;
</content>
		<summary>&lt;p&gt;This is just a note about &lt;a href="https://docs.microsoft.com/en-us/cli/azure/overview?view=azure-cli-latest"&gt;Azure CLI&lt;/a&gt; login options.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2017-12-27-durable-functions" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2017-12-27-durable-functions</id>
		<title>Actors in Serverless</title>
		<updated>2017-12-27T00:00:00Z</updated>
		<content>&lt;p&gt;I started with this &lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-install"&gt;documentation page&lt;/a&gt; to learn about Azure durable Functions. I wanted to know if I can build a way to implement actors in Azure Functions. Actors Programming Model is pretty interesting and I did some work on it &lt;a href="http://khaledhikmat.github.io/posts/2016-12-15-service-fabric-fundamentals"&gt;here&lt;/a&gt;, &lt;a href="http://khaledhikmat.github.io/posts/2016-12-02-service-fabric-basics"&gt;here&lt;/a&gt; and &lt;a href="http://khaledhikmat.github.io/posts/2017-01-10-service-fabric-notes"&gt;here&lt;/a&gt; using &lt;a href="https://azure.microsoft.com/en-us/services/service-fabric/"&gt;Azure Service Fabric&lt;/a&gt; before.&lt;/p&gt;
&lt;p&gt;Following the Azure Functions sample instructions mentioned in the above link, I quickly got up and running. However, I wanted to answer the following questions about actors in Azure Functions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a new actor giving a provided actor id&lt;/li&gt;
&lt;li&gt;Signal an existing actor to perform something&lt;/li&gt;
&lt;li&gt;When do actors get created?&lt;/li&gt;
&lt;li&gt;When do actors get terminated?&lt;/li&gt;
&lt;li&gt;Can we read the actor's internal state?&lt;/li&gt;
&lt;li&gt;What about .NET Core and .NET Standard 2.0 and other stuff?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="create-a-new-actor"&gt;Create a new actor:&lt;/h2&gt;
&lt;p&gt;I created an HTTP trigger that looks like this where I provide a code that can be used as an instance id for the singleton i.e. membership actor. If the membership actor status is null or not running, then I start it with a &lt;code&gt;StartNewAsync&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[FunctionName(&amp;quot;HttpRefreshMemberships&amp;quot;)]
public static async Task&amp;lt;HttpResponseMessage&amp;gt; Run(
    [HttpTrigger(AuthorizationLevel.Function, methods: &amp;quot;post&amp;quot;, Route = &amp;quot;memberships/refresh/{code}&amp;quot;)] HttpRequestMessage req,
    [OrchestrationClient] DurableOrchestrationClient starter,
    string code,
    TraceWriter log)
{
    var membershipStatus = await starter.GetStatusAsync(code);
    string runningStatus = membershipStatus == null ? &amp;quot;NULL&amp;quot; : membershipStatus.RuntimeStatus.ToString();
    log.Info($&amp;quot;Instance running status: '{runningStatus}'.&amp;quot;);

    if (
        membershipStatus == null || 
        membershipStatus.RuntimeStatus != OrchestrationRuntimeStatus.Running
        )
    {
        var membership = new {
            Id = &amp;quot;asas&amp;quot;,
            Code = code,
            CardNumber = &amp;quot;977515900121213&amp;quot;
        };

        await starter.StartNewAsync(&amp;quot;E3_Membership&amp;quot;, code, membership);
        log.Info($&amp;quot;Started a new membership actor with code = '{code}'.&amp;quot;);
    }
    else
    {
        await starter.RaiseEventAsync(code, &amp;quot;operation&amp;quot;, &amp;quot;refresh&amp;quot;);
        log.Info($&amp;quot;Refreshed an existing membership actor with code = '{code}'.&amp;quot;);
    }

    var res = starter.CreateCheckStatusResponse(req, code);
    res.Headers.RetryAfter = new RetryConditionHeaderValue(TimeSpan.FromSeconds(10));
    return res;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="signal-an-existing-actor-to-perform-something"&gt;Signal an existing actor to perform something&lt;/h2&gt;
&lt;p&gt;If the membership actor does exist, we raise a &lt;code&gt;refresh&lt;/code&gt; event to wake up the singleton so it can do work:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;await starter.RaiseEventAsync(code, &amp;quot;operation&amp;quot;, &amp;quot;refresh&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The actual membership actor code looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public static class Membership
{
    [FunctionName(&amp;quot;E3_Membership&amp;quot;)]
    public static async Task&amp;lt;dynamic&amp;gt; Run(
        [OrchestrationTrigger] DurableOrchestrationContext context,
        TraceWriter log)
    {
        dynamic membership = context.GetInput&amp;lt;dynamic&amp;gt;();
        if (membership == null)
            log.Info($&amp;quot;Something is bad! I should start with a valid membership.&amp;quot;);

        var operation = await context.WaitForExternalEvent&amp;lt;string&amp;gt;(&amp;quot;operation&amp;quot;);
        log.Info($&amp;quot;***** received '{operation}' event.&amp;quot;);

        operation = operation?.ToLowerInvariant();
        if (operation == &amp;quot;refresh&amp;quot;)
        {
            membership = await Refresh(context, log);
        }

        if (operation != &amp;quot;end&amp;quot;)
        {
            context.ContinueAsNew(membership);
        }

        return membership;
    }

    public static async Task&amp;lt;dynamic&amp;gt; Refresh(DurableOrchestrationContext context,
                                              TraceWriter log)
    {
        // TODO: Do something to refresh the membership
        dynamic membership = new {
            Id = &amp;quot;asas&amp;quot;,
            Code = context.InstanceId,
            CardNumber = &amp;quot;977515900121213&amp;quot;
        };

        DateTime now = DateTime.Now;
        string formatDate = now.ToString(&amp;quot;MM/dd/yyyy hh:mm:ss.fff tt&amp;quot;);
        log.Info($&amp;quot;**** done refreshing '{context.InstanceId}' &amp;#64; {formatDate}&amp;quot;);
        return membership;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="multiple-signals"&gt;Multiple signals&lt;/h3&gt;
&lt;p&gt;But what happens if the actor is signaled frantically via raising an external event from an HTTP trigger, for example? The event signals are actually enqueued to the instance so they should run as many  times as they are sginaled.&lt;/p&gt;
&lt;p&gt;If you are observing the actor's streaming logs when you try this, it could get very confusing. This is because durable functions manage long-term running processes in short-lived functions is by taking advantage of state retrieved in the &lt;code&gt;context&lt;/code&gt; and replaying the function to resume at the next step (from &lt;a href="https://hackernoon.com/serverless-and-bitcoin-creating-price-watchers-dynamically-beea36ef194e"&gt;this article&lt;/a&gt;). Effectively what you will see if that functions are started, completed and re-started again to resume state.&lt;/p&gt;
&lt;h3 id="code-delays"&gt;Code Delays&lt;/h3&gt;
&lt;p&gt;Singletons should not use &lt;code&gt;Task&lt;/code&gt; functions such as &lt;code&gt;Task.Delay(millis)&lt;/code&gt; to simulate code delays. This will cause run-time errors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Function 'E3_Membership (Orchestrator)', version '' failed with an error. Reason: System.InvalidOperationException: Multithreaded execution was detected. his can happen if the orchestrator function previously resumed from an unsupported async callback.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The preferred way for delays or timeouts is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;await context.CreateTimer(deadline, CancellationToken.None);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;deadline&lt;/code&gt; is defined:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;DateTime deadline = context.CurrentUtcDateTime.AddMinutes(30);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is very important that we leverage the &lt;code&gt;context&lt;/code&gt; to provide accurate timer information as opposed to &lt;code&gt;TimeSpan&lt;/code&gt; and &lt;code&gt;DateTime.Now&lt;/code&gt;, etc. I have seen very varying (not correct) results when I used &lt;code&gt;TimeSpan.FromMinutes(30)&lt;/code&gt;, for example.&lt;/p&gt;
&lt;h3 id="wait-on-multiple-events"&gt;Wait on multiple events&lt;/h3&gt;
&lt;p&gt;What if we want the actor to wait on an external event or on a internal timeout event to perhaps refresh our membership periodically? I created another membership function i.e. &lt;code&gt;E3_MembershipWithTimer&lt;/code&gt; that awaits on either an operation event or a timeout event:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[FunctionName(&amp;quot;E3_MembershipWithTimer&amp;quot;)]
public static async Task&amp;lt;dynamic&amp;gt; RunWithTimer(
    [OrchestrationTrigger] DurableOrchestrationContext context,
    TraceWriter log)
{
    log.Info($&amp;quot;E3_MembershipWithTimer starting.....&amp;quot;);
    dynamic membership = context.GetInput&amp;lt;dynamic&amp;gt;();
    if (membership == null)
        log.Info($&amp;quot;Something is bad! I should start with a valid membership.&amp;quot;);

    string operation = &amp;quot;refresh&amp;quot;;
    using (var cts = new CancellationTokenSource())
    {
        var operationTask = context.WaitForExternalEvent&amp;lt;string&amp;gt;(&amp;quot;operation&amp;quot;);
        DateTime deadline = context.CurrentUtcDateTime.AddMinutes(30);
        var timeoutTask = context.CreateTimer(deadline, cts.Token);

        Task winner = await Task.WhenAny(operationTask, timeoutTask);
        if (winner == operationTask)
        {
            log.Info($&amp;quot;An operation event received!&amp;quot;);
            operation = operationTask.Result;
            cts.Cancel();
        }
        else
        {
            // Default the timeout task to mean a 'refresh' operation
            log.Info($&amp;quot;A timeout event received!&amp;quot;);
            operation = &amp;quot;refresh&amp;quot;;
        }
    }

    log.Info($&amp;quot;***** received '{operation}' event.&amp;quot;);

    operation = operation?.ToLowerInvariant();
    if (operation == &amp;quot;refresh&amp;quot;)
    {
        membership = await Refresh(context, log);
    }

    if (operation != &amp;quot;end&amp;quot;)
    {
        context.ContinueAsNew(membership);
    }

    return membership;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="when-do-actors-get-created"&gt;When do actors get created?&lt;/h2&gt;
&lt;p&gt;Actors or singletons actually do persist in storage (please see the section about termination)......this is how an Azure Functions knows how to start them when it restarts. So if you create actors with specific instance ids (or actor ids), shut down the functions and restart it, the singleton instances are available. When you want to trigger an instance, you must check its running state and then invoke the proper API:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var membershipStatus = await starter.GetStatusAsync(code);
string runningStatus = membershipStatus == null ? &amp;quot;NULL&amp;quot; : membershipStatus.RuntimeStatus.ToString();
log.Info($&amp;quot;Instance running status: '{runningStatus}'.&amp;quot;);

if (
    membershipStatus == null || 
    membershipStatus.RuntimeStatus != OrchestrationRuntimeStatus.Running
    )
{
    var membership = new {
        Id = &amp;quot;asas&amp;quot;,
        Code = code,
        CardNumber = &amp;quot;977515900121213&amp;quot;
    };

    await starter.StartNewAsync(&amp;quot;E3_Membership&amp;quot;, code, membership);
    log.Info($&amp;quot;Started a new membership actor with code = '{code}'.&amp;quot;);
}
else
{
    await starter.RaiseEventAsync(code, &amp;quot;operation&amp;quot;, &amp;quot;refresh&amp;quot;);
    log.Info($&amp;quot;Refreshed an existing membership actor with code = '{code}'.&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="when-do-actors-get-terminated"&gt;When do actors get terminated?&lt;/h2&gt;
&lt;p&gt;They can be easily terminated using the &lt;code&gt;TerminateAsync&lt;/code&gt; API. So I created a little HTTP trugger that would terminate instances:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[FunctionName(&amp;quot;HttpTerminateMemberships&amp;quot;)]
public static async Task&amp;lt;HttpResponseMessage&amp;gt; Run(
    [HttpTrigger(AuthorizationLevel.Function, methods: &amp;quot;post&amp;quot;, Route = &amp;quot;memberships/terminate/{code}&amp;quot;)] HttpRequestMessage req,
    [OrchestrationClient] DurableOrchestrationClient starter,
    string code,
    TraceWriter log)
{
    try
    {
        await starter.TerminateAsync(code, &amp;quot;&amp;quot;);
        return req.CreateResponse&amp;lt;dynamic&amp;gt;(HttpStatusCode.OK);
    }
    catch (Exception ex)
    {
        return req.CreateResponse&amp;lt;dynamic&amp;gt;(HttpStatusCode.BadRequest, ex.Message);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Azure Durable Functions maintain a state of all running instances in a task hub which is basically a storage resource with control queues, qork-item queues, a history table and lease blobs. You can read more about this &lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-task-hubs"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Effectively, the &lt;code&gt;host.json&lt;/code&gt; &lt;code&gt;durableTask&lt;/code&gt; indicate the hub name:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;durableTask&amp;quot;: {
    &amp;quot;HubName&amp;quot;: &amp;quot;TestDurableFunctionsHub&amp;quot;
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The run-time environment stores related information about running instances in storage keyed by the hub name.&lt;/p&gt;
&lt;h2 id="the-actor-state"&gt;The actor state&lt;/h2&gt;
&lt;p&gt;Each actor has an internal state! It is initially read by the singleton as an input:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dynamic membership = context.GetInput&amp;lt;dynamic&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and it is updated using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;context.ContinueAsNew(membership);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But it seems that the internal state is actually not persisted anywhere ...it is transient. When actors are initially created, a state is passed as an input i.e. &lt;code&gt;context.GetInput&amp;lt;dynamic&amp;gt;()&lt;/code&gt; and the actor updates it with a call to &lt;code&gt;ContinueAsNew&lt;/code&gt; which actually restarts itself with a new state.&lt;/p&gt;
&lt;p&gt;The internal state can be read by using one of the APIs of the instance management:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var status = await client.GetStatusAsync(instanceId);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;client&lt;/code&gt; is &lt;code&gt;DurableOrchestrationClient&lt;/code&gt;. The status input is the actor's internal state:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;Name&amp;quot;: &amp;quot;E3_MembershipWithTimer&amp;quot;,
    &amp;quot;InstanceId&amp;quot;: &amp;quot;U7CCR&amp;quot;,
    &amp;quot;CreatedTime&amp;quot;: &amp;quot;2017-12-29T21:12:24.8229285Z&amp;quot;,
    &amp;quot;LastUpdatedTime&amp;quot;: &amp;quot;2017-12-29T21:12:25.5309613Z&amp;quot;,
    &amp;quot;Input&amp;quot;: {
        &amp;quot;$type&amp;quot;: &amp;quot;&amp;lt;&amp;gt;f__AnonymousType0`3[[System.String, mscorlib],[System.String, mscorlib],[System.String, mscorlib]], VSSample&amp;quot;,
        &amp;quot;Id&amp;quot;: &amp;quot;asas&amp;quot;,
        &amp;quot;Code&amp;quot;: &amp;quot;U7CCR&amp;quot;,
        &amp;quot;CardNumber&amp;quot;: &amp;quot;977515900121213&amp;quot;
    },
    &amp;quot;Output&amp;quot;: null,
    &amp;quot;RuntimeStatus&amp;quot;: 0
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am not sure if the actor internal state is meant to hold big state though. Perhaps it is better if the actor exposes its state externally so HTTP triggers, for example, can read it directly from the external store.&lt;/p&gt;
&lt;p&gt;One way of doing this is to modify the code to look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[FunctionName(&amp;quot;HttpRefreshMemberships&amp;quot;)]
public static async Task&amp;lt;HttpResponseMessage&amp;gt; Run(
    [HttpTrigger(AuthorizationLevel.Function, methods: &amp;quot;post&amp;quot;, Route = &amp;quot;memberships/refresh/{code}&amp;quot;)] HttpRequestMessage req,
    [OrchestrationClient] DurableOrchestrationClient starter,
    string code,
    TraceWriter log)
{
    var membershipStatus = await starter.GetStatusAsync(code);
    string runningStatus = membershipStatus == null ? &amp;quot;NULL&amp;quot; : membershipStatus.RuntimeStatus.ToString();
    log.Info($&amp;quot;Instance running status: '{runningStatus}'.&amp;quot;);

    if (
        membershipStatus == null || 
        membershipStatus.RuntimeStatus != OrchestrationRuntimeStatus.Running
        )
    {
		// Given the membership code, read from an external source
        var membership = await RetriveFromCosmosDB(code);
        await starter.StartNewAsync(&amp;quot;E3_Membership&amp;quot;, code, membership);
        log.Info($&amp;quot;Started a new membership actor with code = '{code}'.&amp;quot;);
    }
    else
    {
        await starter.RaiseEventAsync(code, &amp;quot;operation&amp;quot;, &amp;quot;refresh&amp;quot;);
        log.Info($&amp;quot;Refreshed an existing membership actor with code = '{code}'.&amp;quot;);
    }

    var res = starter.CreateCheckStatusResponse(req, code);
    res.Headers.RetryAfter = new RetryConditionHeaderValue(TimeSpan.FromSeconds(10));
    return res;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the membership actor:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public static class Membership
{
    [FunctionName(&amp;quot;E3_Membership&amp;quot;)]
    public static async Task&amp;lt;dynamic&amp;gt; Run(
        [OrchestrationTrigger] DurableOrchestrationContext context,
        TraceWriter log)
    {
        dynamic membership = context.GetInput&amp;lt;dynamic&amp;gt;();
        if (membership == null)
		{
            // Read from an external source 
            membership = await RetriveFromCosmosDB(context.InstanceId);
		}

        var operation = await context.WaitForExternalEvent&amp;lt;string&amp;gt;(&amp;quot;operation&amp;quot;);
        log.Info($&amp;quot;***** received '{operation}' event.&amp;quot;);

        operation = operation?.ToLowerInvariant();
        if (operation == &amp;quot;refresh&amp;quot;)
        {
            membership = await Refresh(context, log);
        }

        if (operation != &amp;quot;end&amp;quot;)
        {
            context.ContinueAsNew(membership);
        }

        return membership;
    }

    public static async Task&amp;lt;dynamic&amp;gt; Refresh(DurableOrchestrationContext context,
                                              TraceWriter log)
    {
        // TODO: Do something to refresh the membership
        dynamic membership = new {
            Id = &amp;quot;asas&amp;quot;,
            Code = context.InstanceId,
            CardNumber = &amp;quot;977515900121213&amp;quot;
        };

		// TODO: Store to an external source
        await StoreToCosmosDB(context.InstanceId, membership);

        DateTime now = DateTime.Now;
        string formatDate = now.ToString(&amp;quot;MM/dd/yyyy hh:mm:ss.fff tt&amp;quot;);
        log.Info($&amp;quot;**** done refreshing '{context.InstanceId}' &amp;#64; {formatDate}&amp;quot;);
        return membership;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the HTTP trigger that retrieves the membership actor state from an extenal source without dealing with the actor:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[FunctionName(&amp;quot;HttpGetMembership&amp;quot;)]
public static async Task&amp;lt;HttpResponseMessage&amp;gt; Run(
    [HttpTrigger(AuthorizationLevel.Function, methods: &amp;quot;get&amp;quot;, Route = &amp;quot;memberships/{code}&amp;quot;)] HttpRequestMessage req,
    [OrchestrationClient] DurableOrchestrationClient starter,
    string code,
    TraceWriter log)
{
    var status = await starter.GetStatusAsync(code);
    if (status != null)
    {
        return req.CreateResponse&amp;lt;dynamic&amp;gt;(HttpStatusCode.OK, await RetriveFromCosmosDB(code));
    }
    else
    {
        return req.CreateResponse&amp;lt;dynamic&amp;gt;(HttpStatusCode.BadRequest, $&amp;quot;{code} membership actor is not found!&amp;quot;);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So unlike regular actor implementation, Azure Functions singletons do not expose any method to be called from the outside! The platform only allows starting/creating, querying and terminating instances.&lt;/p&gt;
&lt;h2 id="comments-anamolies"&gt;Comments &amp;amp; Anamolies&lt;/h2&gt;
&lt;h3 id="net-core-and.net-standard-2.0"&gt;.NET Core and .NET Standard 2.0&lt;/h3&gt;
&lt;p&gt;It is work in progress! It is best to use the .NET full framework with Azure Durable Functions. Hopefully this will change soon and we will be able to use .NET Core reliably.&lt;/p&gt;
&lt;h3 id="local-debugging"&gt;Local Debugging&lt;/h3&gt;
&lt;p&gt;I had a very hard time with this. The symptoms that I experienced are unfortutanely not experienced by other developers who tried this as I could not see similar reported issues. I am using Vs2017 and Azure Functions Extension ...the latest at the time of writing DEC/2017.&lt;/p&gt;
&lt;p&gt;Here are my comments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you want to debug locally, make sure you set both the local.setting.json and host.json file to &lt;code&gt;copy always&lt;/code&gt;. You do this from the properties window.&lt;/li&gt;
&lt;li&gt;On both of my developer machines, I hit F5, it prompts me to install the Azure Functions Core tools and things look quite good. I was able to run locally.&lt;/li&gt;
&lt;li&gt;But then subsequent F5, I get very different results ranging from:
&lt;ul&gt;
&lt;li&gt;The CLI starts and exits on its own ...I could not find out what the reason is&lt;/li&gt;
&lt;li&gt;The CLI starts and displays the functions URls. But it also complains about some files were changed and the host needs to restart. The URls are not responsive and there is nothing you can do except to terminate and restart.&lt;/li&gt;
&lt;li&gt;The CLI statrs and actually works.....does not happen often ...but I have seen it work&lt;/li&gt;
&lt;li&gt;F5 mostly causes the CLI to start and exit. Control-F5 does not exit ...but the function URLs are not accessible due to this &lt;code&gt;change detected&lt;/code&gt; message.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Effectively, local debugging did not work for me at all. It was a very frustrating experience. So I had to deploy everything (see deplyment below) to Azure and debug there....another frustrating experience.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="deployment"&gt;Deployment&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The only effective way I was able to find out how to deploy a Durable Functions App is via Visual Studio. I have heard some people got it to work with VSTS. But, given that this is a test run, I did not really explore this option.&lt;/li&gt;
&lt;li&gt;However, if you just click the &lt;code&gt;publish&lt;/code&gt; button in VS, it will auto-create a storage account for you which names things really weird. My recommendation is to create the App Service, Service Plan, Storage and App Insights in portal or via Azure CLI and then use Visual Studio to publish into it.&lt;/li&gt;
&lt;li&gt;If you renamed a function and re-deployed, Visual Studio will publish the new functions app with the new function. But the old function will still be there (you can see it from the portal). You can then use &lt;code&gt;Kudo&lt;/code&gt;, navigate to the directory and actually delete the old function folder.&lt;/li&gt;
&lt;li&gt;The local.settings.json entries are not deployed to Azure! This means you have to manually create them in the portal app settings or in Visual Studio deployment window.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="storage"&gt;Storage&lt;/h3&gt;
&lt;p&gt;As mentioned, an Azure Storage is required to maintain teh durable instances. They are keyed off the hub name you specify in the host. There are entries in blob, tables, files and queues.&lt;/p&gt;
&lt;h3 id="logging"&gt;Logging&lt;/h3&gt;
&lt;p&gt;Unless you turn on streaming on a function in the portal, you don't get anything (or at least, I could not find a way to do it). But watching the log from the portal is very difficult as it times out and it is not very user friendly. This is one area that requires better UX in the portal. The logs are also stored on the app's file system which you can access from &lt;code&gt;Kudo&lt;/code&gt;. However, I noticed that, unless you request stream logging on a function, these files are not created.&lt;/p&gt;
&lt;p&gt;So the story of logging is a little frustrating at best! I had to use App Insights trace to see what is going on.&lt;/p&gt;
&lt;h3 id="timers"&gt;Timers&lt;/h3&gt;
&lt;p&gt;As mentioned above, it is very important that we leverage the context to provide accurate timer information as opposed to &lt;code&gt;TimeSpan&lt;/code&gt; and &lt;code&gt;DateTime.Now&lt;/code&gt;, etc. Initially I used &lt;code&gt;TimeSpan.FromMinutes(30)&lt;/code&gt; to wait for 30 minutes....but the way to do it is to always use the &lt;code&gt;context&lt;/code&gt; such as &lt;code&gt;DateTime deadline = context.CurrentUtcDateTime.AddMinutes(30);&lt;/code&gt;. After doing that, I started getting conistent timeout periods and things worked normally.&lt;/p&gt;
&lt;h3 id="instance-termination"&gt;Instance Termination&lt;/h3&gt;
&lt;p&gt;Although &lt;code&gt;TerminateAsync&lt;/code&gt; on an instance works, I am not exactly sure if it works the way it is supposed to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If I have a running instance and that instance is actually waiting on an external or time out event, &lt;code&gt;TerminateAsync&lt;/code&gt; does not do anything. I guess because a message is enqueued to the instance but the instance is waiting on other events ....so it did not get the &lt;code&gt;terminate&lt;/code&gt; signal yet.&lt;/li&gt;
&lt;li&gt;If the instance is not waiting on anything, &lt;code&gt;TerminateAsync&lt;/code&gt; replays the instance which runs code that you don't necessarily want to run. For example, I had an instance that triggers a logic app once it receives an &lt;code&gt;end&lt;/code&gt; operation which works. However, if I terminate the instance using &lt;code&gt;TerminateAync&lt;/code&gt;, the code triggers the logic app again because it was replayed!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Not sure if this behavior is correct and what the &lt;code&gt;terminate&lt;/code&gt; signal actually do.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Reflecting on the little time that I spent with Azure Durable Functions, I think they will play an important part of my future solutions. I think they have what it takes to use as actors especially that the Azure Durable Functions are designed to &lt;a href="https://social.msdn.microsoft.com/Forums/en-US/b6ffeae3-f62a-4e6d-a68a-e5dc6f9ffd62/durable-singletons?forum=AzureFunctions"&gt;support 1000's of instances&lt;/a&gt; . If we externalize the actor's state, we will be able to query the external store as opposed to query the actors themselves to retrieve their state.&lt;/p&gt;
&lt;p&gt;Azure Durable Actors can also employ reminders and other sophisticated techniques found in Service Fabric actors such as long-running, stateful, single-threaded, location-transparent and globally addressable (taken from the overview &lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-overview"&gt;documentation page&lt;/a&gt;). However, as stated above and unlike other actor implementations, Azure Functions singletons do not expose methods that can be called from the outside.&lt;/p&gt;
&lt;p&gt;In any case, the Azure Durable Functions are still in preview. So we can expect many great features to be added soon.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I started with this &lt;a href="https://docs.microsoft.com/en-us/azure/azure-functions/durable-functions-install"&gt;documentation page&lt;/a&gt; to learn about Azure durable Functions. I wanted to know if I can build a way to implement actors in Azure Functions. Actors Programming Model is pretty interesting and I did some work on it &lt;a href="http://khaledhikmat.github.io/posts/2016-12-15-service-fabric-fundamentals"&gt;here&lt;/a&gt;, &lt;a href="http://khaledhikmat.github.io/posts/2016-12-02-service-fabric-basics"&gt;here&lt;/a&gt; and &lt;a href="http://khaledhikmat.github.io/posts/2017-01-10-service-fabric-notes"&gt;here&lt;/a&gt; using &lt;a href="https://azure.microsoft.com/en-us/services/service-fabric/"&gt;Azure Service Fabric&lt;/a&gt; before.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2017-12-21-point-to-site-connectivity" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2017-12-21-point-to-site-connectivity</id>
		<title>Point to Site Connectivity in Azure</title>
		<updated>2017-12-21T00:00:00Z</updated>
		<content>&lt;p&gt;This PowerShell script creates self-signed root and client certificates, export them and import what is needed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Assume you are on Windows 10
$myPassword = &amp;quot;some_password&amp;quot;;
$certsPath = &amp;quot;C:\YourDir\Certificates&amp;quot;
$certNamePrefix = &amp;quot;YourNameP2S&amp;quot;;
$date = Get-date &amp;quot;2040-01-01&amp;quot;;

# Create a self-signed ROOT cert 
$rootCert = New-SelfSignedCertificate -Type Custom -KeySpec Signature -Subject &amp;quot;CN=$($certNamePrefix)Cert&amp;quot; -KeyExportPolicy Exportable -HashAlgorithm sha256 -KeyLength 2048 -CertStoreLocation &amp;quot;Cert:\CurrentUser\My&amp;quot; -KeyUsageProperty Sign -KeyUsage CertSign -NotAfter $date

# Export the cert to base64 so it can be uploaded to the Point-to-Site VPN connection: refer to https://docs.microsoft.com/en-us/azure/vpn-gateway/vpn-gateway-certificates-point-to-site
# Upload the .cer ending with '_Encoded'
Export-Certificate -Cert $rootCert -FilePath &amp;quot;$certsPath\$($certNamePrefix)Cert.cer&amp;quot; 
Start-Process -FilePath 'certutil.exe' -ArgumentList &amp;quot;-encode $certsPath\$($certNamePrefix)Cert.cer $certsPath\$($certNamePrefix)Cert_Encoded.cer&amp;quot; -WindowStyle Hidden

# NOTE: Download the VPN Client from Azure AFTER you upload the encoded certificate i.e. .cer file

# Generate a client certificate from the self-signed certificate
# NOTE: The self-siged root cert and the client cert must have the same subject!!!
$clientCert = New-SelfSignedCertificate -Type Custom -KeySpec Signature -Subject &amp;quot;CN=$($certNamePrefix)Cert&amp;quot; -KeyExportPolicy Exportable -HashAlgorithm sha256 -KeyLength 2048 -CertStoreLocation &amp;quot;Cert:\CurrentUser\My&amp;quot; -Signer $rootCert -TextExtension &amp;#64;(&amp;quot;2.5.29.37={text}1.3.6.1.5.5.7.3.2&amp;quot;) -NotAfter $date

# Export the client certificate as PFX
Export-PfxCertificate -Cert $clientCert -ChainOption BuildChain -FilePath  &amp;quot;$certsPath\$($certNamePrefix)Cert.pfx&amp;quot; -Password $(ConvertTo-SecureString -String $myPassword -AsPlainText -Force)

# Import the PFX client cert into the user store
Import-PfxCertificate -CertStoreLocation Cert:\CurrentUser\my\ -FilePath &amp;quot;$certsPath\$($certNamePrefix)Cert.pfx&amp;quot; -Exportable -Password $(ConvertTo-SecureString -String $myPassword -AsPlainText -Force)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope it helps someone.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;This PowerShell script creates self-signed root and client certificates, export them and import what is needed:&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2017-03-30-deletes-in-docdb" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2017-03-30-deletes-in-docdb</id>
		<title>Document Deletion in Azure DocumentDB</title>
		<updated>2017-03-30T00:00:00Z</updated>
		<content>&lt;p&gt;I saw many posts about deleting documents in Azure DocumentDB...but none of them worked quite well for me. So I spent a few hours on this and finally got it to work. Below is my solution. The following posts helped me tremendously (thank you):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://talkingaboutdata.wordpress.com/2015/08/24/deleting-multiple-documents-from-azure-documentdb/"&gt;https://talkingaboutdata.wordpress.com/2015/08/24/deleting-multiple-documents-from-azure-documentdb/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tutorialspoint.com/documentdb/documentdb_delete_document.htm"&gt;https://www.tutorialspoint.com/documentdb/documentdb_delete_document.htm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/29137708/how-to-delete-all-the-documents-in-documentdb-through-c-sharp-code"&gt;http://stackoverflow.com/questions/29137708/how-to-delete-all-the-documents-in-documentdb-through-c-sharp-code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://azure.microsoft.com/en-us/blog/working-with-dates-in-azure-documentdb-4/"&gt;https://azure.microsoft.com/en-us/blog/working-with-dates-in-azure-documentdb-4/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I basically wanted to delete aging documents (based on number of hours) from a collection. So my final routine looks like this. Below is some explanation:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public async Task&amp;lt;int&amp;gt; DeleteAgingDocuments(Uri docDbUri, string docDbKey, string databaseName, string collectionName, int hours)
{
    using (var client = new DocumentClient(docDbUri, docDbKey))
    {
		try
		{
	        var dbs = this._docDbClient.CreateDatabaseQuery().ToList();
	        if (dbs == null)
	            throw new Exception(&amp;quot;No databases in the Docdb account!&amp;quot;);
	        var db = dbs.Where(d =&amp;gt; d.Id == databaseName).FirstOrDefault();
	        if (db == null)
	            throw new Exception($&amp;quot;No database [{databaseName}] in the Docdb account!&amp;quot;);
	        var collections = this._docDbClient.CreateDocumentCollectionQuery(db.CollectionsLink).ToList();
	        if (collections == null)
	            throw new Exception($&amp;quot;No collections in the [{databaseName}] database in the Docdb account!&amp;quot;);
	        var collection = this._docDbClient.CreateDocumentCollectionQuery(db.CollectionsLink).Where(c =&amp;gt; c.Id == collectionName).ToList().FirstOrDefault();
	        if (collection == null)
	            throw new Exception($&amp;quot;No collection [{collectionName}] in the [{databaseName}] database in the Docdb account!&amp;quot;);

            int epocDateTime = DateTime.UtcNow.AddHours(-1 * hours).ToEpoch();
            var dbQuery = &amp;quot;SELECT VALUE {\&amp;quot;link\&amp;quot;: c._self, \&amp;quot;source\&amp;quot;: c.source} FROM c WHERE c._ts &amp;lt; &amp;quot; + epocDateTime;
            var docs = this._docDbClient.CreateDocumentQuery(collection.SelfLink, dbQuery, new FeedOptions { EnableCrossPartitionQuery = true }).ToList();
            foreach (var doc in docs)
            {
                var link = (string)doc.link;
                var source = (string)doc.source;
                await this._docDbClient.DeleteDocumentAsync(link, new RequestOptions() { PartitionKey = new Microsoft.Azure.Documents.PartitionKey(source) });
            }

            return docs.Count;
		}
		catch (Exception ex)
		{
			// some debug 
		}
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="time"&gt;Time&lt;/h3&gt;
&lt;p&gt;The first problem I encountered is how to select the aging documents! It turned out the best way to do this is to compare numbers as opposed to dates. This &lt;a href="https://azure.microsoft.com/en-us/blog/working-with-dates-in-azure-documentdb-4/"&gt;post&lt;/a&gt; helped me understand what the problem is and how to go around doing it properly. I ended it up using the built-in time stamp value stored as meta data in every DocDB document i.e. &lt;code&gt;_ts&lt;/code&gt;. This may or may not work for every case. In my case my collection document date i.e. &lt;code&gt;eventDate&lt;/code&gt; is actually the real UTC time ....so it was no problem. If this is not the case, you many need to store your own time stamp (in addition to the date) so u can do the query to pull the aging documents based on time.&lt;/p&gt;
&lt;p&gt;so this query does exactly that:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int epocDateTime = DateTime.UtcNow.AddHours(-1 * hours).ToEpoch();
var dbQuery = $&amp;quot;SELECT * FROM c WHERE c._ts &amp;lt; {epocDateTime}&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how I am using the Epoc time for my aging time stamp. The &lt;code&gt;DateTime&lt;/code&gt; extension is written this way:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;public static int ToEpoch(this DateTime date)
{
    if (date == null) return int.MinValue;
    DateTime epoch = new DateTime(1970, 1, 1);
    TimeSpan epochTimeSpan = date - epoch;
    return (int)epochTimeSpan.TotalSeconds;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="partition-key"&gt;Partition Key&lt;/h3&gt;
&lt;p&gt;My collection was partitioned over a value in the document i.e. &lt;code&gt;source&lt;/code&gt;, but I wanted to trim all aging documents across all partitions...not against a single partition. So I used this query options to force the query to span multiple partitions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FeedOptions queryOptions = new FeedOptions { EnableCrossPartitionQuery = true };
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="deletion"&gt;Deletion&lt;/h3&gt;
&lt;p&gt;Finally, I wanted to loop through all aging documents and delete:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int epocDateTime = DateTime.UtcNow.AddHours(-1 * hours).ToEpoch();
var dbQuery = &amp;quot;SELECT VALUE {\&amp;quot;link\&amp;quot;: c._self, \&amp;quot;source\&amp;quot;: c.source} FROM c WHERE c._ts &amp;lt; &amp;quot; + epocDateTime;
var docs = this._docDbClient.CreateDocumentQuery(collection.SelfLink, dbQuery, new FeedOptions { EnableCrossPartitionQuery = true }).ToList();
foreach (var doc in docs)
{
    var link = (string)doc.link;
    var source = (string)doc.source;
    await this._docDbClient.DeleteDocumentAsync(link, new RequestOptions() { PartitionKey = new Microsoft.Azure.Documents.PartitionKey(source) });
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id="query"&gt;Query&lt;/h4&gt;
&lt;p&gt;Please note that the query that I used above uses a projection to get only the document link and the partition key....we really do not need the entire document:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var dbQuery = &amp;quot;SELECT VALUE {\&amp;quot;link\&amp;quot;: c._self, \&amp;quot;source\&amp;quot;: c.source} FROM c WHERE c._ts &amp;lt; &amp;quot; + epocDateTime;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also please note that I am using the &lt;code&gt;VALUE&lt;/code&gt; modifier in the query so to force DocDB to return the value only. This will return a payload that looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[
  {
    &amp;quot;link&amp;quot;: &amp;quot;dbs/XEthAA==/colls/XEthAL4dCwA=/docs/XEthAL4dCwABAAAAAAAAAA==/&amp;quot;,
    &amp;quot;source&amp;quot;: &amp;quot;Digital Controller&amp;quot;
  },
  {
    &amp;quot;link&amp;quot;: &amp;quot;dbs/XEthAA==/colls/XEthAL4dCwA=/docs/XEthAL4dCwACAAAAAAAAAA==/&amp;quot;,
    &amp;quot;source&amp;quot;: &amp;quot;Profiler&amp;quot;
  }
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I don't include the &lt;code&gt;VALUE&lt;/code&gt; modifier, I get this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[
  {
    &amp;quot;$1&amp;quot;: {
      &amp;quot;link&amp;quot;: &amp;quot;dbs/XEthAA==/colls/XEthAL4dCwA=/docs/XEthAL4dCwABAAAAAAAAAA==/&amp;quot;,
      &amp;quot;source&amp;quot;: &amp;quot;Digital Controller&amp;quot;
    }
  },
  {
    &amp;quot;$1&amp;quot;: {
      &amp;quot;link&amp;quot;: &amp;quot;dbs/XEthAA==/colls/XEthAL4dCwA=/docs/XEthAL4dCwACAAAAAAAAAA==/&amp;quot;,
      &amp;quot;source&amp;quot;: &amp;quot;Profiler&amp;quot;
    }
  }
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I chose the first one :-)&lt;/p&gt;
&lt;h4 id="deletion-1"&gt;Deletion&lt;/h4&gt;
&lt;p&gt;Finally, we pull the documents and delete one at a time:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var docs = this._docDbClient.CreateDocumentQuery(collection.SelfLink, dbQuery, new FeedOptions { EnableCrossPartitionQuery = true }).ToList();
foreach (var doc in docs)
{
    var link = (string)doc.link;
    var source = (string)doc.source;
    await this._docDbClient.DeleteDocumentAsync(link, new RequestOptions() { PartitionKey = new Microsoft.Azure.Documents.PartitionKey(source) });
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Initially, I only got the document link from the query thinking that this was the only requirement. So I did something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var dbQuery = &amp;quot;SELECT VALUE c._self FROM c WHERE c._ts &amp;lt; &amp;quot; + epocDateTime;
var docs = this._docDbClient.CreateDocumentQuery(collection.SelfLink, dbQuery, new FeedOptions { EnableCrossPartitionQuery = true }).ToList();
foreach (var doc in docs)
{
    await this._docDbClient.DeleteDocumentAsync(doc);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This did not work! I needed to pass the partition key....this is why i changed the query to a projection so I can get the partition key. In my case the partition key is the &lt;code&gt;source&lt;/code&gt;. There is a comment in this &lt;a href="http://stackoverflow.com/questions/29137708/how-to-delete-all-the-documents-in-documentdb-through-c-sharp-code"&gt;post&lt;/a&gt; that gave me a clue that the request option must include the partition key.&lt;/p&gt;
&lt;p&gt;Thank you for reading! I hope this helps someone.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I saw many posts about deleting documents in Azure DocumentDB...but none of them worked quite well for me. So I spent a few hours on this and finally got it to work. Below is my solution. The following posts helped me tremendously (thank you):&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2017-02-21-service-fabric-secure-cluster" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2017-02-21-service-fabric-secure-cluster</id>
		<title>Service Fabric Secure Cluster Deployment</title>
		<updated>2017-02-21T00:00:00Z</updated>
		<content>&lt;p&gt;In this post, I just used the Service Fabric team article &lt;a href="https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-creation-via-arm"&gt;https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-creation-via-arm&lt;/a&gt; to create a PowerShell script that will do the entire deployment. I also downloaded all the required helper PowerShell modules and placed them in one &lt;a href="https://github.com/khaledhikmat/service-fabric-secure-deployment"&gt;repository&lt;/a&gt; so it would be easier for others to work with the deployment.&lt;/p&gt;
&lt;p&gt;Here are some of my notes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The account you use to log in to Azure with must be a Global admin.&lt;/li&gt;
&lt;li&gt;In case of errors during deployment, please check the Azure Activity Logs. It is pretty good and provides a very useful insight to what went wrong.&lt;/li&gt;
&lt;li&gt;After a deployment is successful, you can modify the ARM template and re-deploy. This will update the cluster. For example, if you added a new LN port and re-deployed using the PowerShell script, that new port will be available.&lt;/li&gt;
&lt;li&gt;To connect to a secure cluster, use this guide: &lt;a href="https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-connect-to-secure-cluster"&gt;https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-connect-to-secure-cluster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Connect to the cluster using the browser &lt;a href="https://your-cluster.westus.cloudapp.azure.com:19080/Explorer/index.html"&gt;https://your-cluster.westus.cloudapp.azure.com:19080/Explorer/index.html &lt;/a&gt; will cause a certificate error. This is expected as the script uses a self-signed certificate. Just proceed.&lt;/li&gt;
&lt;li&gt;To log in to the fabric explorer requires that you complete the steps where you go to the AD in which the cluster belongs to, select the app that was created and assign an admin role to it as described in the above article. This must be done from the classic portal.&lt;/li&gt;
&lt;li&gt;To connect using PowerShell, use &lt;code&gt;Connect-ServiceFabricCluster -ConnectionEndpoint ${dnsName}:19000 -ServerCertThumbprint &amp;quot;6C84CEBF914FF489551385BA128542BA63A16222&amp;quot; -AzureActiveDirectory&lt;/code&gt;. Please note that, similar to the browser, this requires that the user be assigned as in the previous step.&lt;/li&gt;
&lt;li&gt;Please note that securing the cluster does not mean that your own application endpoint is secured. You must do whatever you need to do to enable HTTPs in your own application and provide some sort of token authentication.&lt;/li&gt;
&lt;li&gt;I noticed that the only VM size that worked reliably was the Standard_D2. Anything less than that causes health issues due to disk space, etc. I heard from Microsoft &lt;a href="https://social.msdn.microsoft.com/Forums/en-US/04915062-63fd-4608-94fb-f018c32e15c3/will-there-be-a-service-fabric-managed-service?forum=AzureServiceFabric"&gt;here&lt;/a&gt; that they are working on ways to reduce the cost of the VMs, particularly by allowing us to use smaller VMs and still get the reasonable reliability/durability levels, which would help reduce costs without sacrificing the safety or uptime of our service.&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;In this post, I just used the Service Fabric team article &lt;a href="https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-creation-via-arm"&gt;https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-cluster-creation-via-arm&lt;/a&gt; to create a PowerShell script that will do the entire deployment. I also downloaded all the required helper PowerShell modules and placed them in one &lt;a href="https://github.com/khaledhikmat/service-fabric-secure-deployment"&gt;repository&lt;/a&gt; so it would be easier for others to work with the deployment.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2017-02-15-web-test-thoughts" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2017-02-15-web-test-thoughts</id>
		<title>Web Tests Thoughts</title>
		<updated>2017-02-15T00:00:00Z</updated>
		<content>&lt;p&gt;If a Web App is deployed on Azure, both the &lt;a href="https://azure.microsoft.com/en-us/services/application-insights/"&gt;App Insights&lt;/a&gt; and &lt;a href="https://azure.microsoft.com/en-us/services/app-service/web/"&gt;Web Apps&lt;/a&gt; offer a utility that can hammer the app's endpoints from different regions. While this functionality is quite nice and comes bundled in, it is considered an alert-based system and is slightly rudimentary as one cannot customize the test or get access to the full results easily. This post describes an alternative approach that uses &lt;a href="https://azure.microsoft.com/en-us/services/functions/"&gt;Azure Functions&lt;/a&gt; or &lt;a href="https://azure.microsoft.com/en-us/services/service-fabric/"&gt;Service Fabric&lt;/a&gt; to implement a web test that can test and endpoint and report its test to PowerBI in real time.&lt;/p&gt;
&lt;p&gt;What I really wanted to do is to conduct a web test for a duration of time from different regions against a new product's endpoints at launch time and immediately view the test results with executives.&lt;/p&gt;
&lt;h2 id="azure-functions"&gt;Azure Functions&lt;/h2&gt;
&lt;p&gt;Briefly, here is what I decided to do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use PowerShell to provision resource groups in four different regions. Each resource group contains an Azure Function App that loads its source code from a code repository.&lt;/li&gt;
&lt;li&gt;Use source code changes to trigger updates to all the functions apps at the same time.&lt;/li&gt;
&lt;li&gt;Configure the Azure Functions App to test a Web URL or a collection. This test can be sophisticated because we have the full power of an Azure Function to conduct the test. In other words, the test can be running through a use case scenario as opposed to just posting to a URL, for example.&lt;/li&gt;
&lt;li&gt;Auto-trigger the Azure Functions by a timer (i.e. configurable) to repeat the test.&lt;/li&gt;
&lt;li&gt;Report the source (i.e region), duration, URL, date time and status code to a PowerBI real-time data set at the end of every test iteration.&lt;/li&gt;
&lt;li&gt;Create a real-time visualization to see, in real-time, the test results.&lt;/li&gt;
&lt;li&gt;Use PowerShell script to de-provision the resource groups when the test is no longer needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The source code can be found &lt;a href="https://github.com/khaledhikmat/serverless-webtest"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="macro-architecture"&gt;Macro Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/2NMuHBC.png" class="img-fluid" alt="Web Test using Azure Functions" /&gt;&lt;/p&gt;
&lt;h3 id="azure-resource-group-template"&gt;Azure Resource Group Template&lt;/h3&gt;
&lt;p&gt;I downloaded an Azure Function template and modified it to suit my needs. The main thing in the template is that it defines the repository URL and branch from which the source code is to be imported and the definitions of the app strings:&lt;/p&gt;
&lt;p&gt;Here is the entire template:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;$schema&amp;quot;: &amp;quot;http://schemas.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#&amp;quot;,
    &amp;quot;contentVersion&amp;quot;: &amp;quot;1.0.0.0&amp;quot;,
    &amp;quot;parameters&amp;quot;: {
        &amp;quot;appName&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;metadata&amp;quot;: {
                &amp;quot;description&amp;quot;: &amp;quot;The name of the function app that you wish to create.&amp;quot;
            }
        },
        &amp;quot;storageAccountType&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;defaultValue&amp;quot;: &amp;quot;Standard_LRS&amp;quot;,
            &amp;quot;allowedValues&amp;quot;: [
                &amp;quot;Standard_LRS&amp;quot;,
                &amp;quot;Standard_GRS&amp;quot;,
                &amp;quot;Standard_ZRS&amp;quot;,
                &amp;quot;Premium_LRS&amp;quot;
            ],
            &amp;quot;metadata&amp;quot;: {
                &amp;quot;description&amp;quot;: &amp;quot;Storage Account type&amp;quot;
            }
        },
        &amp;quot;repoURL&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;defaultValue&amp;quot;: &amp;quot;https://&amp;lt;ourown&amp;gt;.visualstudio.com/DefaultCollection/Misc/_git/WebTest&amp;quot;,
            &amp;quot;metadata&amp;quot;: {
                &amp;quot;description&amp;quot;: &amp;quot;The URL for the GitHub repository that contains the project to deploy.&amp;quot;
            }
        },
        &amp;quot;branch&amp;quot;: {
            &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,
            &amp;quot;defaultValue&amp;quot;: &amp;quot;master&amp;quot;,
            &amp;quot;metadata&amp;quot;: {
                &amp;quot;description&amp;quot;: &amp;quot;The branch of the GitHub repository to use.&amp;quot;
            }
        }
    },
    &amp;quot;variables&amp;quot;: {
        &amp;quot;functionAppName&amp;quot;: &amp;quot;[parameters('appName')]&amp;quot;,
        &amp;quot;hostingPlanName&amp;quot;: &amp;quot;[parameters('appName')]&amp;quot;,
        &amp;quot;storageAccountName&amp;quot;: &amp;quot;[concat(uniquestring(resourceGroup().id), 'azfunctions')]&amp;quot;,
        &amp;quot;storageAccountid&amp;quot;: &amp;quot;[concat(resourceGroup().id,'/providers/','Microsoft.Storage/storageAccounts/', variables('storageAccountName'))]&amp;quot;
    },
    &amp;quot;resources&amp;quot;: [
        {
            &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Storage/storageAccounts&amp;quot;,
            &amp;quot;name&amp;quot;: &amp;quot;[variables('storageAccountName')]&amp;quot;,
            &amp;quot;apiVersion&amp;quot;: &amp;quot;2015-06-15&amp;quot;,
            &amp;quot;location&amp;quot;: &amp;quot;[resourceGroup().location]&amp;quot;,
            &amp;quot;properties&amp;quot;: {
                &amp;quot;accountType&amp;quot;: &amp;quot;[parameters('storageAccountType')]&amp;quot;
            }
        },
        {
            &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Web/serverfarms&amp;quot;,
            &amp;quot;apiVersion&amp;quot;: &amp;quot;2015-04-01&amp;quot;,
            &amp;quot;name&amp;quot;: &amp;quot;[variables('hostingPlanName')]&amp;quot;,
            &amp;quot;location&amp;quot;: &amp;quot;[resourceGroup().location]&amp;quot;,
            &amp;quot;properties&amp;quot;: {
                &amp;quot;name&amp;quot;: &amp;quot;[variables('hostingPlanName')]&amp;quot;,
                &amp;quot;computeMode&amp;quot;: &amp;quot;Dynamic&amp;quot;,
                &amp;quot;sku&amp;quot;: &amp;quot;Dynamic&amp;quot;
            }
        },
        {
            &amp;quot;apiVersion&amp;quot;: &amp;quot;2015-08-01&amp;quot;,
            &amp;quot;type&amp;quot;: &amp;quot;Microsoft.Web/sites&amp;quot;,
            &amp;quot;name&amp;quot;: &amp;quot;[variables('functionAppName')]&amp;quot;,
            &amp;quot;location&amp;quot;: &amp;quot;[resourceGroup().location]&amp;quot;,
            &amp;quot;kind&amp;quot;: &amp;quot;functionapp&amp;quot;,            
            &amp;quot;dependsOn&amp;quot;: [
                &amp;quot;[resourceId('Microsoft.Web/serverfarms', variables('hostingPlanName'))]&amp;quot;,
                &amp;quot;[resourceId('Microsoft.Storage/storageAccounts', variables('storageAccountName'))]&amp;quot;
            ],
            &amp;quot;properties&amp;quot;: {
                &amp;quot;serverFarmId&amp;quot;: &amp;quot;[resourceId('Microsoft.Web/serverfarms', variables('hostingPlanName'))]&amp;quot;,
                &amp;quot;siteConfig&amp;quot;: {
                    &amp;quot;appSettings&amp;quot;: [
                        {
                            &amp;quot;name&amp;quot;: &amp;quot;AzureWebJobsDashboard&amp;quot;,
                            &amp;quot;value&amp;quot;: &amp;quot;[concat('DefaultEndpointsProtocol=https;AccountName=', variables('storageAccountName'), ';AccountKey=', listKeys(variables('storageAccountid'),'2015-05-01-preview').key1)]&amp;quot;
                        },
                        {
                            &amp;quot;name&amp;quot;: &amp;quot;AzureWebJobsStorage&amp;quot;,
                            &amp;quot;value&amp;quot;: &amp;quot;[concat('DefaultEndpointsProtocol=https;AccountName=', variables('storageAccountName'), ';AccountKey=', listKeys(variables('storageAccountid'),'2015-05-01-preview').key1)]&amp;quot;
                        },
                        {
                            &amp;quot;name&amp;quot;: &amp;quot;WEBSITE_CONTENTAZUREFILECONNECTIONSTRING&amp;quot;,
                            &amp;quot;value&amp;quot;: &amp;quot;[concat('DefaultEndpointsProtocol=https;AccountName=', variables('storageAccountName'), ';AccountKey=', listKeys(variables('storageAccountid'),'2015-05-01-preview').key1)]&amp;quot;
                        },
                        {
                            &amp;quot;name&amp;quot;: &amp;quot;WEBSITE_CONTENTSHARE&amp;quot;,
                            &amp;quot;value&amp;quot;: &amp;quot;[toLower(variables('functionAppName'))]&amp;quot;
                        },
                        {
                            &amp;quot;name&amp;quot;: &amp;quot;FUNCTIONS_EXTENSION_VERSION&amp;quot;,
                            &amp;quot;value&amp;quot;: &amp;quot;~1&amp;quot;
                        },
                        {
                            &amp;quot;name&amp;quot;: &amp;quot;WEBSITE_NODE_DEFAULT_VERSION&amp;quot;,
                            &amp;quot;value&amp;quot;: &amp;quot;6.5.0&amp;quot;
                        },
                        {
                            &amp;quot;name&amp;quot;: &amp;quot;location&amp;quot;,
                            &amp;quot;value&amp;quot;: &amp;quot;[resourceGroup().location]&amp;quot;
                        },
                        {
                            &amp;quot;name&amp;quot;: &amp;quot;testUrl&amp;quot;,
                            &amp;quot;value&amp;quot;: &amp;quot;http://your-own.azurewebsites.net&amp;quot;
                        }
                    ]
                }
            }, 
            &amp;quot;resources&amp;quot;: [
                {
                    &amp;quot;apiVersion&amp;quot;: &amp;quot;2015-08-01&amp;quot;,
                    &amp;quot;name&amp;quot;: &amp;quot;web&amp;quot;,
                    &amp;quot;type&amp;quot;: &amp;quot;sourcecontrols&amp;quot;,
                    &amp;quot;dependsOn&amp;quot;: [
                        &amp;quot;[resourceId('Microsoft.Web/Sites', variables('functionAppName'))]&amp;quot;
                    ],
                    &amp;quot;properties&amp;quot;: {
                        &amp;quot;RepoUrl&amp;quot;: &amp;quot;[parameters('repoURL')]&amp;quot;,
                        &amp;quot;branch&amp;quot;: &amp;quot;[parameters('branch')]&amp;quot;,
                        &amp;quot;isManualIntegration&amp;quot;: false
                    }
                }
            ]                     
        }
    ]
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is the template parameters file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;$schema&amp;quot;: &amp;quot;http://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#&amp;quot;,
  &amp;quot;contentVersion&amp;quot;: &amp;quot;1.0.0.0&amp;quot;,
  &amp;quot;parameters&amp;quot;: {
    &amp;quot;appName&amp;quot;: {
      &amp;quot;value&amp;quot;: &amp;quot;WebTestFunctions&amp;quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="powershell-script"&gt;PowerShell Script&lt;/h3&gt;
&lt;p&gt;The PowerShell script is the main driver that orchestrates the deployment and of the different Azure Functions to different resource groups. Here is the complete script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Login to Azure first
Login-AzureRmAccount

# Select the subscription
Get-AzureRmSubscription | select SubscriptionName
$subscr = &amp;quot;YourOwn&amp;quot;
Select-AzureRmSubscription -SubscriptionName $subscr

# 1. create a new resource group in west US
New-AzureRmResourceGroup -Name WebTest4WestUS -Location &amp;quot;West US&amp;quot;

# 1.5. deploy the template to the west us resource group
New-AzureRmResourceGroupDeployment -Name WebTest4WestUSDeployment -ResourceGroupName WebTest4WestUS `
  -TemplateFile azuredeploy.json  

# 2. create a new resource group in west europe
New-AzureRmResourceGroup -Name WebTest4WestEurope -Location &amp;quot;West Europe&amp;quot;

# 2.5. deploy the template to the west europe resource group
New-AzureRmResourceGroupDeployment -Name WebTest4WestEuropeDeployment -ResourceGroupName WebTest4WestEurope `
  -TemplateFile azuredeploy.json

# 3. create a new resource group in West Japan
New-AzureRmResourceGroup -Name WebTest4WestJapan -Location &amp;quot;Japan West&amp;quot;

# 3.5. deploy the template to the west japan resource group
New-AzureRmResourceGroupDeployment -Name WebTest4WestJapanDeployment -ResourceGroupName WebTest4WestJapan `
  -TemplateFile azuredeploy.json    

# 4. create a new resource group in South Brazil
New-AzureRmResourceGroup -Name WebTest4SouthBrazil -Location &amp;quot;Brazil South&amp;quot;

# 4.5. deploy the template to the south brazil resource group
New-AzureRmResourceGroupDeployment -Name WebTest4SouthBrazilDeployment -ResourceGroupName WebTest4SouthBrazil `
  -TemplateFile azuredeploy.json  
  
######

# Delete the resource groups
Remove-AzureRmResourceGroup -Name WebTest4WestUS -Force
Remove-AzureRmResourceGroup -Name WebTest4WestEurope -Force
Remove-AzureRmResourceGroup -Name WebTest4WestJapan -Force
Remove-AzureRmResourceGroup -Name WebTest4SouthBrazil -Force   
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you run the deployments, u will see something like this in your subscription resource groups:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/8LgDuHA.png" class="img-fluid" alt="Resource Groups" /&gt;&lt;/p&gt;
&lt;h3 id="powerbi-real-time-dataset"&gt;PowerBI Real-Time Dataset&lt;/h3&gt;
&lt;p&gt;Using this &lt;a href="https://powerbi.microsoft.com/en-us/documentation/powerbi-service-real-time-streaming/"&gt;nifty feature&lt;/a&gt; in PowerBI, I defined a real-time dataset that looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/x6SvVgS.png" class="img-fluid" alt="PowerBI Real Time Dataset" /&gt;&lt;/p&gt;
&lt;p&gt;This gave me a URL that I can use from Azure Functions to pump data into this Real-time dataset. Also please note that I enbaled the &lt;code&gt;historic data analysis&lt;/code&gt; to allow me to report on the data and visualize it in real-time and beyond.&lt;/p&gt;
&lt;h3 id="azure-function-source-code"&gt;Azure Function Source Code&lt;/h3&gt;
&lt;p&gt;Finally, the Azure Function source code that conducts the test and reports to PowerBI:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#r &amp;quot;Newtonsoft.Json&amp;quot;

using System;
using System.Text;
using System.Net;
using System.Net.Http;
using System.Net.Http.Headers;
using Newtonsoft.Json;

public static async Task Run(TimerInfo cacheTimer, TraceWriter log)
{
    var location = GetEnvironmentVariable(&amp;quot;location&amp;quot;);
    log.Info($&amp;quot;Web Test trigger executed at {DateTime.Now} from {location}&amp;quot;);    

    try 
    {
        var testUrl = GetEnvironmentVariable(&amp;quot;testUrl&amp;quot;);
        if (!string.IsNullOrEmpty(testUrl))
        {
            string [] results = await TestUrl(testUrl, log);                
            if (results != null &amp;amp;&amp;amp; results.Length == 2)
            {
                // Condition the event to meet the Real-Time PowerBI expectation
                var realTimeEvent = new {
                    time = DateTime.Now,
                    source = GetEnvironmentVariable(&amp;quot;location&amp;quot;),
                    url  = testUrl,
                    duration  = Double.Parse(results[1]),
                    result = results[0]
                };

                var events = new List&amp;lt;dynamic&amp;gt;();
                events.Add(realTimeEvent);
                await PostToPowerBI(events, log);
            }
            else
            {
                log.Info($&amp;quot;Bad results from testing url!&amp;quot;);
            }
        }
        else
            log.Info($&amp;quot;No Test URL!&amp;quot;);
    }
    catch (Exception e)
    {
        log.Info($&amp;quot;Encountered a failure: {e.Message}&amp;quot;);
    }
}

private async static Task&amp;lt;string []&amp;gt; TestUrl(string url, TraceWriter log)
{
    var results = new string[2];
    var statusCode = &amp;quot;&amp;quot;;
    HttpClient client = null;
    DateTime startTime = DateTime.Now;
    DateTime endTime = DateTime.Now;

    try
    {
        client = new HttpClient();

        HttpResponseMessage response = await client.GetAsync(url);
        statusCode = response.StatusCode.ToString();    
    }
    catch (Exception ex)
    {
        log.Info($&amp;quot;TestUrl failed: {ex.Message}&amp;quot;);
        statusCode = &amp;quot;500&amp;quot;;
    }
    finally
    {
        if (client != null)
            client.Dispose();
    }

    endTime = DateTime.Now;
    results[0] = statusCode;
    results[1] = (endTime - startTime).TotalSeconds + &amp;quot;&amp;quot;;
    return results;
}

private async static Task PostToPowerBI(object realTimeEvents, TraceWriter log)
{
    HttpClient client = null;
    // The URL for PowerBI Real Time Dataset
    var url = &amp;quot;https://api.powerbi.com/beta/your-own&amp;quot;; // Should be made into an app setting

    try
    {
        client = new HttpClient();

        var postData = Newtonsoft.Json.JsonConvert.SerializeObject(realTimeEvents);
        HttpContent httpContent = new StringContent(postData, Encoding.UTF8, &amp;quot;application/json&amp;quot;);
        HttpResponseMessage response = await client.PostAsync(url , httpContent);
        string responseString = await response.Content.ReadAsStringAsync();

        if (!response.IsSuccessStatusCode)
        {
            throw new Exception(&amp;quot;Bad return code: &amp;quot; + response.StatusCode);
        }
    }
    catch (Exception ex)
    {
        log.Info($&amp;quot;PostToPowerBI failed: {ex.Message}&amp;quot;);
    }
    finally
    {
        if (client != null)
            client.Dispose();
    }
}

public static string GetEnvironmentVariable(string name)
{
    return System.Environment.GetEnvironmentVariable(name, EnvironmentVariableTarget.Process);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="results-visualization"&gt;Results Visualization&lt;/h3&gt;
&lt;p&gt;If we deploy the Azure Functions and collect the results in PowerBI, we can get real-time results that look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/Dc1B6Jm.png" class="img-fluid" alt="Web Test Results 1" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/ZI3Qfps.png" class="img-fluid" alt="Web Test Results 2" /&gt;&lt;/p&gt;
&lt;p&gt;Hopefully this visualization helps executives to see a clear indication that the product launch is not that successful :-)&lt;/p&gt;
&lt;h2 id="service-fabric"&gt;Service Fabric&lt;/h2&gt;
&lt;p&gt;I also wanted to mantion that Azure Service Fabric could also be used to conduct a web test from hammering the test site from multiple instances. Briefly, here is what I thought of doing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a Web Test Application Type. The Service Fabric app contains a single stateless service which does work on its RunAsync method.&lt;/li&gt;
&lt;li&gt;Use a PowerShell script to instantiate multiple tenants (or named applications): one for each region. Please note that the tenants simulate the different regions as they are all sitting in the same cluster!&lt;/li&gt;
&lt;li&gt;Update (i.e increase or decrease the number of the stateless service instances) each tenant (or named application) independently. This means that, unlike the Azure Functions, the system under-test can be hammered from multiple instances within the same tenant if need be.&lt;/li&gt;
&lt;li&gt;Write sophisticated tests within Service Fabric because the stateless service is doing it and the service can have a lot of configuration to govern that process.&lt;/li&gt;
&lt;li&gt;Report the source (i.e region), duration, URL, date time and status code to a PowerBI real-time data set at the end of every test iteration.&lt;/li&gt;
&lt;li&gt;Create a real-time visualization to see, in real-time, the test results.&lt;/li&gt;
&lt;li&gt;Use PowerShell script to de-provision the resource groups when the test is no longer needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I will publish the Service Fabric web test solution in a different blog post. For now, here is the macro architecture that I am thinking about:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/3f1bA7X.png" class="img-fluid" alt="Azure Service Fabric Macro Architecture" /&gt;&lt;/p&gt;
</content>
		<summary>&lt;p&gt;If a Web App is deployed on Azure, both the &lt;a href="https://azure.microsoft.com/en-us/services/application-insights/"&gt;App Insights&lt;/a&gt; and &lt;a href="https://azure.microsoft.com/en-us/services/app-service/web/"&gt;Web Apps&lt;/a&gt; offer a utility that can hammer the app's endpoints from different regions. While this functionality is quite nice and comes bundled in, it is considered an alert-based system and is slightly rudimentary as one cannot customize the test or get access to the full results easily. This post describes an alternative approach that uses &lt;a href="https://azure.microsoft.com/en-us/services/functions/"&gt;Azure Functions&lt;/a&gt; or &lt;a href="https://azure.microsoft.com/en-us/services/service-fabric/"&gt;Service Fabric&lt;/a&gt; to implement a web test that can test and endpoint and report its test to PowerBI in real time.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2017-01-10-service-fabric-notes" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2017-01-10-service-fabric-notes</id>
		<title>Service Fabric Notes</title>
		<updated>2017-01-10T00:00:00Z</updated>
		<content>&lt;p&gt;I am compiling notes when working with &lt;a href="https://azure.microsoft.com/en-us/services/service-fabric/"&gt;Service Fabric&lt;/a&gt; from the folks at Microsoft! In this post, I will enumerate a few things that I ran into which I think might be helpful to others who are learning the environment as well. This will not be a complete list ....so I will add to it as I go along.&lt;/p&gt;
&lt;h3 id="actor-turn-based-concurrency"&gt;Actor Turn-based Concurrency&lt;/h3&gt;
&lt;p&gt;Actors are single threaded! They only allow one thread to be acting on them at any given time. In fact, in the Service Fabric terminology, this is referred to as Turn-based treading. From my observation, it seems that this is how the platform and the actors&lt;/p&gt;
&lt;p&gt;What happens to the clients who are calling the actors? If two clients are trying to access an actor at the same time, one blocks until the actor finishes the first client method. This is to ensure that an actor works on one thing at a time.&lt;/p&gt;
&lt;p&gt;Let us say, we have an actor that has two methods like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public Task&amp;lt;string&amp;gt; GetThing1()
{
    // Simulate long work
    Thread.Sleep(5000);
    return Task.FromResult&amp;lt;string&amp;gt;(&amp;quot;thing1&amp;quot;);
}

public Task&amp;lt;string&amp;gt; GetThing2()
{
    // Simulate long work
    Thread.Sleep(10000);
    return Task.FromResult&amp;lt;string&amp;gt;(&amp;quot;thing2&amp;quot;);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you call &lt;code&gt;GetThing1&lt;/code&gt; from one client and immediately call &lt;code&gt;GetThing2&lt;/code&gt; from another client (or Postman session), the second client will wait at least 15 seconds to get the string &lt;code&gt;thing2&lt;/code&gt; response.&lt;/p&gt;
&lt;p&gt;Given this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I think it is best to front-end actors with a service that can invoke methods on actors when it receives requests from a queue. This way the service is waiting on actors to complete processing while it is in its &lt;code&gt;RunAsync&lt;/code&gt; method.&lt;/li&gt;
&lt;li&gt;It is important to realize that actors should really not be queried and that actors should employ several things:
&lt;ul&gt;
&lt;li&gt;Backup state to an external store such as DocumentDB or others. This way the external store can be queried instead.&lt;/li&gt;
&lt;li&gt;Aggregate result externally perhaps via Azure Function into some outside store so queries could run against this store&lt;/li&gt;
&lt;li&gt;Aggregate result to an aggregator actor that can quickly respond to queries which will relieve the processing actors from worrying about query requests.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="actor-reminders"&gt;Actor Reminders&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-actors-timers-reminders/"&gt;Actor Reminders&lt;/a&gt; are really nice to have. In the sample app that I am working on, I use them to schedule processing after I return to the caller. Effectively they seem to give me the ability to run things asynchronously and return to the caller right away. Without this, the actor processing throughput may not be at best if the the processing takes a while to complete.&lt;/p&gt;
&lt;p&gt;In addition to firing a future event, they do allow me to pack an item or object that can be retrieved when the reminder triggers. This makes a lot of scenarios possible because we are able to pack the item that we want the reminder to work on.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please note, however, that when a reminder is running in an actor, that actor cannot respond to other method invocation! This is because the platform makes sure that there is only a single threaded operating on an actor at any time.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The best way I found out to schedule reminders to fire immediately is something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public async Task Process(SomeItem item)
{
	var error = &amp;quot;&amp;quot;;

	try
	{
		if (item == null)
		{
			...removed for brevity
			return;
		}

		await this.RegisterReminderAsync(
			ReprocessReminder,
			ObjectToByteArray(item),
			TimeSpan.FromSeconds(0),            // If 0, remind immediately
			TimeSpan.FromMilliseconds(-1));     // Disable periodic firing
	}
	catch (Exception ex)
	{
		error = ex.Message;
	}
	finally
	{
		...removed for brevity
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When the reminder triggers, &lt;code&gt;ReprocessReminder&lt;/code&gt; is called to process the item that was packed within the reminder: &lt;code&gt;ObjectToByteArray(item)&lt;/code&gt;. Here are possible implementation of packing and unpacking the item:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private byte[] ObjectToByteArray(Object obj)
{
    if (obj == null)
        return null;

    BinaryFormatter bf = new BinaryFormatter();

    try
    {
        using (var ms = new MemoryStream())
        {
            bf.Serialize(ms, obj);
            return ms.ToArray();
        }

    }
    catch (Exception ex)
    {
        return null;
    }
}

private Object ByteArrayToObject(byte[] arrBytes)
{
    try
    {
        using (var memStream = new MemoryStream())
        {
            var binForm = new BinaryFormatter();
            memStream.Write(arrBytes, 0, arrBytes.Length);
            memStream.Seek(0, SeekOrigin.Begin);
            var obj = binForm.Deserialize(memStream);
            return obj;
        }
    }
    catch (Exception ex)
    {
        return null;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="actor-interface"&gt;Actor Interface&lt;/h3&gt;
&lt;p&gt;From this &lt;a href="https://azure.microsoft.com/en-us/documentation/articles/service-fabric-reliable-actors-notes-on-actor-type-serialization/"&gt;article&lt;/a&gt;: &lt;em&gt;The arguments of all methods, result types of the tasks returned by each method in an actor interface, and objects stored in an actor's State Manager must be Data Contract serializable. This also applies to the arguments of the methods defined in actor event interfaces. (Actor event interface methods always return void.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In my actor interface, I had many methods and everything was working great until I added these two methods:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;Task&amp;lt;SomeView&amp;gt; GetView(int year, int month);
Task&amp;lt;SomeView&amp;gt; GetView(int year);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you to compile a Service Fabric solution that has an interface that looks like the above, you will be met with a very strange compilation error:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/cO972hG.png" class="img-fluid" alt="Actor Compilation Error" /&gt;&lt;/p&gt;
&lt;p&gt;What? What is that? Why? After hours, it turned out you can actually &lt;a href="http://stackoverflow.com/questions/35820191/how-to-ignore-a-servicetype-from-servicefabric-manifest-file-on-build-deploy"&gt;turn off&lt;/a&gt; this error. From the above Stack Overflow post:&lt;/p&gt;
&lt;p&gt;By changing the project file .csproj of the project containing the actors and setting property:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;UpdateServiceFabricManifestEnabled&amp;gt;false&amp;lt;/UpdateServiceFabricManifestEnabled&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this tool can be disabled!! But still why is this happening? It turned out that the actor interfaces may not have overridden methods!! So the tool was complaining about the interface containing just that i.e. overridden methods. If the above interface is changed to the below, everything will work well:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;Task&amp;lt;SomeView&amp;gt; GetViewByYearNMonth(int year, int month);
Task&amp;lt;SomeView&amp;gt; GetViewByYear(int year);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition, the actor event methods may not return anything but &lt;code&gt;void&lt;/code&gt;. So if you have something like this, you will get the same &lt;code&gt;FabActUtil.exe&lt;/code&gt; error:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public interface IMyActorEvents : IActorEvents
{
	Task MeasuresRecalculated(....);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I am hoping to add to this post as I go along. Hopefully this has been helpful.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I am compiling notes when working with &lt;a href="https://azure.microsoft.com/en-us/services/service-fabric/"&gt;Service Fabric&lt;/a&gt; from the folks at Microsoft! In this post, I will enumerate a few things that I ran into which I think might be helpful to others who are learning the environment as well. This will not be a complete list ....so I will add to it as I go along.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2016-12-23-how-to-generate-this-site" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2016-12-23-how-to-generate-this-site</id>
		<title>How to generate a static site</title>
		<updated>2016-12-23T00:00:00Z</updated>
		<content>&lt;p&gt;I use &lt;a href="https://wyam.io/"&gt;Wyam&lt;/a&gt; to statically generate this site. This is to document the steps I take to run &lt;a href="https://wyam.io/"&gt;Wyam&lt;/a&gt; locally, test my posts and push the site to &lt;a href="https://www.github.com"&gt;GitHub&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;I have a directory structure that looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://mosaicapi.blob.core.windows.net/images/1788cf26-c285-4f7b-b9ad-da65f73574b6.png" class="img-fluid" alt="Wyam Dir Structure" /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Input&lt;/code&gt; directory looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://mosaicapi.blob.core.windows.net/images/5cbc7cab-bd19-4e81-af36-ce2b8cb3157c.png" class="img-fluid" alt="Wyam input Dir Structure" /&gt;&lt;/p&gt;
&lt;p&gt;I place my posts in &lt;code&gt;Markdown&lt;/code&gt; in the &lt;code&gt;posts&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;run.cmd&lt;/code&gt; contains the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;#64;echo off
..\..\Wyam-Exec\Wyam-v0.15.1-beta\Wyam.exe -r Blog -t CleanBlog -pw
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This assumes that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I have &lt;code&gt;Wyam-Exec&lt;/code&gt; directory two directories above my blog directory&lt;/li&gt;
&lt;li&gt;I have different Wyam versions I can test with. For example, I am using a beta version here&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;config.wyam&lt;/code&gt; contains the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Settings.Host = &amp;quot;khaledhikmat.github.io&amp;quot;;
GlobalMetadata[&amp;quot;Title&amp;quot;] = &amp;quot;Khaled Hikmat&amp;quot;;
GlobalMetadata[&amp;quot;Description&amp;quot;] = &amp;quot;Coding Thoughts&amp;quot;;
GlobalMetadata[&amp;quot;Intro&amp;quot;] = &amp;quot;Software old timer&amp;quot;;
GlobalMetadata[&amp;quot;Image&amp;quot;] = &amp;quot;images/liquid.jpg&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This drives the site's metadata.&lt;/p&gt;
&lt;p&gt;While I am in my blog directory, I open up a command box and type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cmd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This starts Wyam, launches its internal web site listening on port 5080 and it also monitors the input directory for any changes. I add my blogs, edit them and test locally using the 5080 site.&lt;/p&gt;
&lt;p&gt;Once I am satisfied, I copy the content of the &lt;code&gt;output&lt;/code&gt; directory:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://mosaicapi.blob.core.windows.net/images/949ce5de-3540-460f-ae44-b82ae6bda923.png" class="img-fluid" alt="Wyam output Dir Structure" /&gt;&lt;/p&gt;
&lt;p&gt;and paste it into my site &lt;a href="https://pages.github.com/"&gt;GitHub pages&lt;/a&gt; structure and replace all files. To publish to GitHub, I issue the following &lt;code&gt;git&lt;/code&gt; commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git add --all
git commit -am &amp;quot;Added/Edited new/existing posts&amp;quot;
git push
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This publishes my changes to &lt;a href="https://pages.github.com/"&gt;GitHub pages&lt;/a&gt;.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I use &lt;a href="https://wyam.io/"&gt;Wyam&lt;/a&gt; to statically generate this site. This is to document the steps I take to run &lt;a href="https://wyam.io/"&gt;Wyam&lt;/a&gt; locally, test my posts and push the site to &lt;a href="https://www.github.com"&gt;GitHub&lt;/a&gt;:&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2016-12-15-service-fabric-fundamentals" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2016-12-15-service-fabric-fundamentals</id>
		<title>Service Fabric Fundamentals</title>
		<updated>2016-12-15T00:00:00Z</updated>
		<content>&lt;p&gt;The &lt;a href="https://github.com/Azure-Samples/service-fabric-dotnet-iot"&gt;Service Fabric iOT sample app&lt;/a&gt; is a great sample to follow for our own Service Fabric apps. In this post, I used code snippets and concepts from the iOT sample to build a small app to demonstrate some fundamentals concepts that I feel are important.&lt;/p&gt;
&lt;p&gt;The source code for this post is available &lt;a href="https://github.com/khaledhikmat/service-fabric-fundamentals"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="the-app-scenario"&gt;The app scenario&lt;/h2&gt;
&lt;p&gt;The app is called Rate Aggregator where we have an app that monitors hotel rate requests coming in from somewhere (presumably from some site) and aggregates the result by city. I also wanted the app to be multi-tenant so we can have an app instance for each rate service provider i.e. Contoso and Fabrican.&lt;/p&gt;
&lt;p&gt;The app is quite simple and consists of two services: Web Service to act as a front-end and a rates service to actually process the rates and aggregate them:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/trNQnSS.png" class="img-fluid" alt="App Components" /&gt;&lt;/p&gt;
&lt;h2 id="source-code"&gt;Source Code&lt;/h2&gt;
&lt;p&gt;The solution source code consists of 4 different projects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Common Project - a class library that has the common classes that are shared across the other projects. Please note that this library must be built using the &lt;code&gt;x64&lt;/code&gt; platform.&lt;/li&gt;
&lt;li&gt;Web Service - a stateless Service Fabric service created using the Visual Studio ASP.NET Core template.&lt;/li&gt;
&lt;li&gt;Rates Service - a stateful Service Fabric service created using the Visual Studio ASP.NET Core template.&lt;/li&gt;
&lt;li&gt;An app project to contain the Service Fabric instances and provide application manifests.&lt;/li&gt;
&lt;li&gt;A collection of PowerShell scripts that manage the deployment, un-deployment, update, upgrade and test. We will go through those files in this post.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I created the solution using VS 2015 Service Fabric template. But actually the projects are regular projects that include Service Fabric NuGet packages. The only project that is quite specific to Service Fabric is the app project i.e. &lt;code&gt;RateAggregatorApp&lt;/code&gt; ...but as demonstrated in a &lt;a href="../posts/2016-12-02-service-fabric-basics"&gt;previous post&lt;/a&gt;, the app manifests and packaging can be easily generated manually.&lt;/li&gt;
&lt;li&gt;The ASP.NET Code template in Service Fabric is still in preview. I noticed some odd stuff about it:
&lt;ul&gt;
&lt;li&gt;The template assumes that you are building stateless services! To create Stateful services using the ASP.NET template, manual intervention have to take place which I will note in this post&lt;/li&gt;
&lt;li&gt;The useful &lt;code&gt;ServiceEventSource.cs&lt;/code&gt; class is not included in the generated project. So if you want to use ETW logging, you must create this file manually (copy it from another SF project)&lt;/li&gt;
&lt;li&gt;The template includes, in the &lt;code&gt;Program.cs&lt;/code&gt; file the Service Fabric registration code and the Service class. It is useful to break up apart and create a class (using the name of the service) to describe the service i.e. &lt;code&gt;WebService&lt;/code&gt; and &lt;code&gt;RatesService&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The Service Fabric &lt;code&gt;RateAggregatorApp&lt;/code&gt; &lt;code&gt;APplicationManifest.xml&lt;/code&gt; file has a section for &lt;code&gt;DefaultServices&lt;/code&gt; which automatically deploys the default services whenever an app is deployed. I usually remove the default services from the manifest file so i can better control the named app instance and service creation process (which I will demo in this post).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="fundamental-concepts"&gt;Fundamental Concepts&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://github.com/Azure-Samples/service-fabric-dotnet-iot"&gt;iOT&lt;/a&gt; sample includes really nice code utilities that can be used to build &lt;code&gt;Uri&lt;/code&gt;s for services especially when the service exposes HTTP endpoints. The most important concepts that I would like to convey are:&lt;/p&gt;
&lt;h3 id="http-endpoints"&gt;HTTP Endpoints&lt;/h3&gt;
&lt;p&gt;If you would like to expose HTTP Endpoints for your service, Microsoft strongly recommends that you build the URL as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/qQdvX9h.png" class="img-fluid" alt="HTTP Endpoint URL" /&gt;&lt;/p&gt;
&lt;p&gt;Examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Http://localhost:8084/ContosoRateAggregatorApp/7217642a-2ac8-4b29-b52d-3e92303ce1b2/131262989332452689/f74f07f7-d92f-47b9-8d6b-c86966c78d09&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Http://localhost:8084/FabricanRateAggregatorApp/13049e47-9727-4e02-9086-8fd6e2457313/131262989924122573/3b3455d8-487e-4ec4-9bd8-64ba8e658662&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For stateful services, this makes total sense! The combination of &lt;code&gt;partitionId&lt;/code&gt; and &lt;code&gt;instanceId&lt;/code&gt; are great for diagnostics and the &lt;code&gt;guid&lt;/code&gt; makes every endpoint unique which is really useful because services are sometimes moved around. However, for Stateless services, I think we can easily omit the &lt;code&gt;partitionId&lt;/code&gt;, the &lt;code&gt;instanceId&lt;/code&gt; and the &lt;code&gt;guid&lt;/code&gt; since stateless service endpoints are usually load balanced as they accept traffic from the Internet. Examples of stateless services endpoints:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Http://localhost:8082/FabricanRateAggregatorApp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Http://localhost:8082/ContosoRateAggregatorApp&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you are planning to expose multiple stateless web services in each app instances, then perhaps adding the service name to the end of the URL would make sense.Examples:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Http://localhost:8082/FabricanRateAggregatorApp/WebService&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Http://localhost:8082/ContosoRateAggregatorApp/WebService&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The demo app source code common project includes a &lt;code&gt;WebHostCommunicationListener&lt;/code&gt; class (which is borrowed from the iOT sample) shows a really good implementation of how to manage this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;string ip = this.serviceContext.NodeContext.IPAddressOrFQDN;
EndpointResourceDescription serviceEndpoint = this.serviceContext.CodePackageActivationContext.GetEndpoint(this.endpointName);
EndpointProtocol protocol = serviceEndpoint.Protocol;
int port = serviceEndpoint.Port;
string host = &amp;quot;+&amp;quot;;

string listenUrl;
string path = this.appPath != null ? this.appPath.TrimEnd('/') + &amp;quot;/&amp;quot; : &amp;quot;&amp;quot;;

if (this.serviceContext is StatefulServiceContext)
{
    StatefulServiceContext statefulContext = this.serviceContext as StatefulServiceContext;
    listenUrl = $&amp;quot;{serviceEndpoint.Protocol}://{host}:{serviceEndpoint.Port}/{path}{statefulContext.PartitionId}/{statefulContext.ReplicaId}/{Guid.NewGuid()}&amp;quot;;
}
else
{
    listenUrl = $&amp;quot;{serviceEndpoint.Protocol}://{host}:{serviceEndpoint.Port}/{path}&amp;quot;;
}

this.webHost = this.build(listenUrl);
this.webHost.Start();

return Task.FromResult(listenUrl.Replace(&amp;quot;://+&amp;quot;, &amp;quot;://&amp;quot; + ip));
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="http-web-apis"&gt;HTTP Web APIs&lt;/h3&gt;
&lt;p&gt;Using ASP.NET Core to implement the Stateless and Stateful services has the distinct advantage of allowing the services expose a Web API layer that can be used by clients to call on the services. The Web API layer has regular controllers with normal Web API decoration to allow the services be called from regular HTTP clients:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;    [Route(&amp;quot;api/[controller]&amp;quot;)]
    public class RatesController : Controller
    {
        private readonly IReliableStateManager stateManager;
        private readonly StatefulServiceContext context;
        private readonly CancellationTokenSource serviceCancellationSource;

        public RatesController(IReliableStateManager stateManager, StatefulServiceContext context, CancellationTokenSource serviceCancellationSource)
        {
            this.stateManager = stateManager;
            this.context = context;
            this.serviceCancellationSource = serviceCancellationSource;
        }

        [HttpGet]
        [Route(&amp;quot;queue/length&amp;quot;)]
        public async Task&amp;lt;IActionResult&amp;gt; GetQueueLengthAsync()
        {
			....
		}
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note that the service has the &lt;code&gt;IReliableStateManager&lt;/code&gt;, the &lt;code&gt;StatefulServiceContext&lt;/code&gt; and the &lt;code&gt;CancellationSource&lt;/code&gt; injected. This allows the Web API controller to use the service reliable collections and anything else related to service context. For example, this is the implementation of the queue length Web API method:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;        [HttpGet]
        [Route(&amp;quot;queue/length&amp;quot;)]
        public async Task&amp;lt;IActionResult&amp;gt; GetQueueLengthAsync()
        {
            IReliableQueue&amp;lt;RateRequest&amp;gt; queue = await this.stateManager.GetOrAddAsync&amp;lt;IReliableQueue&amp;lt;RateRequest&amp;gt;&amp;gt;(RatesService.RateQueueName);

            using (ITransaction tx = this.stateManager.CreateTransaction())
            {
                long count = await queue.GetCountAsync(tx);

                return this.Ok(count);
            }
        }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how the API controller uses the injected &lt;code&gt;StateManager&lt;/code&gt; to gain access to the reliable queue and reports on its length.&lt;/p&gt;
&lt;p&gt;Since the service interface is implemented as regular Web API controllers (or controllers), they can also be exposed as &lt;code&gt;Swagger&lt;/code&gt; and allow other an API management layer to front-end these services.&lt;/p&gt;
&lt;p&gt;To make this possible, the service must override the &lt;code&gt;CreateServiceInstanceListeners&lt;/code&gt; in case of stateless services and &lt;code&gt;CreateServiceReplicaListeners&lt;/code&gt; in case of stateful services. Here is an example of the Stateful service:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;protected override IEnumerable&amp;lt;ServiceReplicaListener&amp;gt; CreateServiceReplicaListeners()
{
    return new ServiceReplicaListener[1]
    {
        new ServiceReplicaListener(
            context =&amp;gt;
            {
                string tenantName = new Uri(context.CodePackageActivationContext.ApplicationName).Segments.Last();

                return new WebHostCommunicationListener(
                    context,
                    tenantName,
                    &amp;quot;ServiceEndpoint&amp;quot;,
                    uri =&amp;gt;
                    {
                        ServiceEventSource.Current.Message($&amp;quot;Listening on {uri}&amp;quot;);

                        return new WebHostBuilder().UseWebListener()
                            .ConfigureServices(
                                services =&amp;gt; services
                                    .AddSingleton&amp;lt;StatefulServiceContext&amp;gt;(this.Context)
                                    .AddSingleton&amp;lt;IReliableStateManager&amp;gt;(this.StateManager)
                                    .AddSingleton&amp;lt;CancellationTokenSource&amp;gt;(this._webApiCancellationSource))
                            .UseContentRoot(Directory.GetCurrentDirectory())
                            .UseStartup&amp;lt;Startup&amp;gt;()
                            .UseUrls(uri)
                            .Build();
                    });
            })
    };
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please note the use of the &lt;code&gt;WebHostCommunicationListener&lt;/code&gt; and how we inject the service context, state manager and the cancellation token.&lt;/p&gt;
&lt;p&gt;In our demo app, both statelss and stateful services implement their interface as Web API.&lt;/p&gt;
&lt;h3 id="http-vs.rcp-endpoints"&gt;HTTP vs. RCP Endpoints&lt;/h3&gt;
&lt;p&gt;Instead of HTTP Web API, Services (especially stateful) can expose an interface using a built-in RCP communicaton listener. In this case, the service implements an interface and make it easy for clients to call upon the service using the interface. For example, a stateful service might have an interface that looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;    public interface ILookupService : IService
    {
        Task EnqueueEvent(SalesEvent sEvent);
        Task&amp;lt;string&amp;gt; GetNodeName();
        Task&amp;lt;int&amp;gt; GetEventsCounter(CancellationToken ct);
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The service will then be implemented this way:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;    internal sealed class LookupService : StatefulService, ILookupService
    {
	...
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The service will override the CreateServiceReplicaListeners as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;    protected override IEnumerable&amp;lt;ServiceReplicaListener&amp;gt; CreateServiceReplicaListeners()
    {
        return new[]
            {
                new ServiceReplicaListener(context =&amp;gt;
                    this.CreateServiceRemotingListener(context),
                    &amp;quot;rpcPrimaryEndpoint&amp;quot;,
                    false)
            };
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Although this looks nice and complies with Object Oriented programming, I think it should only be used with internal stateful services (those that do not expose an outside interface). Stateless services that are used by external clients are better off using an HTTP Web API interface which makes them easily consumable by many clients in different languages.&lt;/p&gt;
&lt;h3 id="reliable-collections"&gt;Reliable Collections&lt;/h3&gt;
&lt;p&gt;Since we have the state manager injected in the stateful service Web API controllers, it makes all the service reliable collections available to the Web API controllers. In our demo, the &lt;code&gt;RatesService&lt;/code&gt; Web API controller i.e. &lt;code&gt;RatesController&lt;/code&gt; uses the reliable queue to get the queue length and enqueue rate requests to the service. The service processes the incoming &lt;code&gt;RateRequest&lt;/code&gt; in its &lt;code&gt;RunAsyc&lt;/code&gt; method and aggregates the results in a reliable dictionary indexed by city/country:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;protected override async Task RunAsync(CancellationToken cancellationToken)
{
    cancellationToken.Register(() =&amp;gt; this._webApiCancellationSource.Cancel());

    IReliableDictionary&amp;lt;string, RateAggregation&amp;gt; citiesDictionary = await this.StateManager.GetOrAddAsync&amp;lt;IReliableDictionary&amp;lt;string, RateAggregation&amp;gt;&amp;gt;(RateCitiesDictionaryName);
    IReliableQueue&amp;lt;RateRequest&amp;gt; queue = await this.StateManager.GetOrAddAsync&amp;lt;IReliableQueue&amp;lt;RateRequest&amp;gt;&amp;gt;(RateQueueName);

    while (true)
    {
        cancellationToken.ThrowIfCancellationRequested();

        try
        {
            using (var tx = this.StateManager.CreateTransaction())
            {
                var result = await queue.TryDequeueAsync(tx);

                if (result.HasValue)
                {
                    RateRequest request = result.Value;

                    // TODO: Process the request
                    // TODO: Go against the reservation provider to pick up the rate
                    // TODO: How do I determine the reservation provider per tenant?
                    int nights = (request.CheckOutDate - request.CheckInDate).Days;
                    int netAmount = _random.Next(500) * nights;
                    var newAggregation = new RateAggregation();
                    newAggregation.Transactions = 1;
                    newAggregation.Nights = nights;
                    newAggregation.Amount = (double) netAmount;

                    await citiesDictionary.AddOrUpdateAsync(tx, $&amp;quot;{request.City}/{request.Country}&amp;quot;, newAggregation, (key, currentValue) =&amp;gt;
                    {
                        currentValue.Transactions += newAggregation.Transactions;
                        currentValue.Nights += newAggregation.Nights;
                        currentValue.Amount += newAggregation.Amount;
                        return currentValue;
                    });

                    // This commits the add to dictionary and the dequeue operation.
                    await tx.CommitAsync();
                }
            }
        }
        catch (Exception e)
        {

        }

        await Task.Delay(TimeSpan.FromMilliseconds(500), cancellationToken);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reliable dictionary is then used in the service contrloller to return the aggregated result in an API call.&lt;/p&gt;
&lt;h3 id="partitions-replicas-and-instances"&gt;Partitions, Replicas and Instances&lt;/h3&gt;
&lt;p&gt;In our demo app, we use partitions in the Stateful service i.e. &lt;code&gt;RatesService&lt;/code&gt; to partition our data in 4 different buckets:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Rate Requests for the United States&lt;/li&gt;
&lt;li&gt;Rate Requests for Canada&lt;/li&gt;
&lt;li&gt;Rate Requests for Australia&lt;/li&gt;
&lt;li&gt;Rate Requests for other countries&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hence our partition key range is 0 (Low Key) to 3 (High Key). We use a very simple method to select the appropriate partition based on the request's country code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private long GetPartitionKey(RateRequest request)
{
    if (request.Country == &amp;quot;USA&amp;quot;)
        return 0;
    else if (request.Country == &amp;quot;CAN&amp;quot;)
        return 1;
    else if (request.Country == &amp;quot;AUS&amp;quot;)
        return 2;
    else // all others
        return 3;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To allow for high availability, Service Fabric uses replicas for stateful services and instances for stateless services. In Service Fabric literature, the term replicas and instances are often exchanged.&lt;/p&gt;
&lt;p&gt;In order to guarantee high availability of stateful service state, the state for each partition is usually replicated. The number of replicas is decided at the time of deploying the service (as we will see soon in the PowerShell script). This means that, if a stateful service has 4 partitions and the target replica count is 3, for example, then there are 12 instances of that service in Service Fabric.&lt;/p&gt;
&lt;p&gt;In order to guarantee high availability of stateless services, Service Fabric allows the instantiation of multiple instances. Usually the number of instances matches the number of nodes in the Service Fabric cluster which allows Service Fabric to distribute an instance on each node. The load balancer then distribute the load across all nodes.&lt;/p&gt;
&lt;p&gt;Please note, however, that, unlike stateless service instances, a stateful service partitions cannot be changed at run-time once the service is deployed. The number of partitions must be decided initially before the service is deployed to the cluster. Of course, if the service state can be discarded, then of course changes to the partition are allowed. Stateless services number of instances can be updated at any time (up or down) at any time. In fact, this is one of the great features of Service Fabric.&lt;/p&gt;
&lt;h3 id="result-aggregation"&gt;Result Aggregation&lt;/h3&gt;
&lt;p&gt;Since the state is partitioned, does this mean that we have the reliable collections (i.e. queues and dictionaries) scattered among the different partitions? The answer is yes! For example, in order to get the queue length of a stateful service, the client has to query all partitions and ask each service instance about the queue length and add them together to determine the overall queue length for the stateful service:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[HttpGet]
[Route(&amp;quot;queue/length&amp;quot;)]
public async Task&amp;lt;IActionResult&amp;gt; GetQueueLengthAsync()
{
    ServiceUriBuilder uriBuilder = new ServiceUriBuilder(RatesServiceName);
    Uri serviceUri = uriBuilder.Build();

    // service may be partitioned.
    // this will aggregate the queue lengths from each partition
    ServicePartitionList partitions = await this.fabricClient.QueryManager.GetPartitionListAsync(serviceUri);

    HttpClient httpClient = new HttpClient(new HttpServiceClientHandler());

    long count = 0;
    foreach (Partition partition in partitions)
    {
        Uri getUrl = new HttpServiceUriBuilder()
            .SetServiceName(serviceUri)
            .SetPartitionKey(((Int64RangePartitionInformation)partition.PartitionInformation).LowKey)
            .SetServicePathAndQuery($&amp;quot;/api/rates/queue/length&amp;quot;)
            .Build();

        HttpResponseMessage response = await httpClient.GetAsync(getUrl, this.cancellationSource.Token);

        if (response.StatusCode != System.Net.HttpStatusCode.OK)
        {
            return this.StatusCode((int)response.StatusCode);
        }

        string result = await response.Content.ReadAsStringAsync();

        count += Int64.Parse(result);
    }

    return this.Ok(count);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;FabricClient&lt;/code&gt; is the .NET client used to provide all sorts of management capabilities. It is injected in the Web Service controller to allow them to communicate with each partitition replica and get the needed results as shown above. Then the Web Service adds the count of each partition and return the total lenth of all partitions queues.&lt;/p&gt;
&lt;p&gt;Similarly, the Web Service uses the &lt;code&gt;FabricClient&lt;/code&gt; to communicate with the each partition replica to get and aggregate the result of each country cities:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[HttpGet]
[Route(&amp;quot;cities&amp;quot;)]
public async Task&amp;lt;IActionResult&amp;gt; GetCitiesAsync()
{
    ServiceUriBuilder uriBuilder = new ServiceUriBuilder(RatesServiceName);
    Uri serviceUri = uriBuilder.Build();

    // service may be partitioned.
    // this will aggregate cities from all partitions
    ServicePartitionList partitions = await this.fabricClient.QueryManager.GetPartitionListAsync(serviceUri);

    HttpClient httpClient = new HttpClient(new HttpServiceClientHandler());

    List&amp;lt;CityStats&amp;gt; cities = new List&amp;lt;CityStats&amp;gt;();
    foreach (Partition partition in partitions)
    {
        Uri getUrl = new HttpServiceUriBuilder()
            .SetServiceName(serviceUri)
            .SetPartitionKey(((Int64RangePartitionInformation)partition.PartitionInformation).LowKey)
            .SetServicePathAndQuery($&amp;quot;/api/rates/cities&amp;quot;)
            .Build();

        HttpResponseMessage response = await httpClient.GetAsync(getUrl, this.cancellationSource.Token);

        if (response.StatusCode != System.Net.HttpStatusCode.OK)
        {
            return this.StatusCode((int)response.StatusCode);
        }

        JsonSerializer serializer = new JsonSerializer();
        using (StreamReader streamReader = new StreamReader(await response.Content.ReadAsStreamAsync()))
        {
            using (JsonTextReader jsonReader = new JsonTextReader(streamReader))
            {
                List&amp;lt;CityStats&amp;gt; result = serializer.Deserialize&amp;lt;List&amp;lt;CityStats&amp;gt;&amp;gt;(jsonReader);

                if (result != null)
                {
                    cities.AddRange(result);
                }
            }
        }
    }

    return this.Ok(cities);
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="multi-tenancy"&gt;Multi-Tenancy&lt;/h3&gt;
&lt;p&gt;One of the great features of Service Fabric is its ability to allow the creation of multi-tenant scanarios. In our demo case, we may launch an app for Contoso rates and another one for Fabrican rates. We want these two apps to be of the same type but they should be completely isolated of each other. So we create two named app instances: &lt;code&gt;ConosoRateAggretor&lt;/code&gt; and &lt;code&gt;FabricanRateAggregator&lt;/code&gt;. This means that we have different set of services for each app operated independely and perhaps scaled, updated and upgraded independently.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/e6YmFNy.png" class="img-fluid" alt="Named App Instances" /&gt;&lt;/p&gt;
&lt;p&gt;This is really useful in many scenarios and allows for many great advantages. In the next section, we will see how easy it is to actually deploy, un-deploy, update and upgrade these named instances.&lt;/p&gt;
&lt;h3 id="configuration"&gt;Configuration&lt;/h3&gt;
&lt;p&gt;Given that we have multiple named app instances, how do we pass different parameters for each named instance? In the &lt;code&gt;RatesService&lt;/code&gt;, we would like to have the name of the provider (and probably other configuration items) so we can communicate with the provider to pull rates. In our demo app, we are not actually communicating with the provider.&lt;/p&gt;
&lt;p&gt;To do this, we define parameters for the &lt;code&gt;RatesService&lt;/code&gt; in the Service Settings file as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot; ?&amp;gt;
&amp;lt;Settings xmlns:xsd=&amp;quot;http://www.w3.org/2001/XMLSchema&amp;quot; xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; xmlns=&amp;quot;http://schemas.microsoft.com/2011/01/fabric&amp;quot;&amp;gt;
  &amp;lt;!-- Add your custom configuration sections and parameters here --&amp;gt;
  &amp;lt;Section Name=&amp;quot;ParametersSection&amp;quot;&amp;gt;
    &amp;lt;Parameter Name=&amp;quot;ProviderName&amp;quot; Value=&amp;quot;&amp;quot; /&amp;gt;
  &amp;lt;/Section&amp;gt;
&amp;lt;/Settings&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The section name can be anything. In our case, it is &lt;code&gt;ParametersSection&lt;/code&gt;. To be able to override this value for a specific application instance, we create a &lt;code&gt;ConfigOverride&lt;/code&gt; when we import the service manifest in the application manifest:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ServiceManifestImport&amp;gt;
&amp;lt;ServiceManifestRef ServiceManifestName=&amp;quot;RatesServicePkg&amp;quot; ServiceManifestVersion=&amp;quot;1.0.0&amp;quot; /&amp;gt;
&amp;lt;ConfigOverrides&amp;gt;
  &amp;lt;ConfigOverride Name=&amp;quot;Config&amp;quot;&amp;gt;
    &amp;lt;Settings&amp;gt;
      &amp;lt;Section Name=&amp;quot;ParametersSection&amp;quot;&amp;gt;
        &amp;lt;Parameter Name=&amp;quot;ProviderName&amp;quot; Value=&amp;quot;[RatesService_ProviderName]&amp;quot; /&amp;gt;
      &amp;lt;/Section&amp;gt;
    &amp;lt;/Settings&amp;gt;
  &amp;lt;/ConfigOverride&amp;gt;
&amp;lt;/ConfigOverrides&amp;gt;
&amp;lt;/ServiceManifestImport&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;The convention is to name the value as &lt;code&gt;ServiceName_ParameterName&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, we must adjust the application manifest to include the required parameter:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;ApplicationManifest xmlns:xsd=&amp;quot;http://www.w3.org/2001/XMLSchema&amp;quot; xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; ApplicationTypeName=&amp;quot;RateAggregatorAppType&amp;quot; ApplicationTypeVersion=&amp;quot;1.0.0&amp;quot; xmlns=&amp;quot;http://schemas.microsoft.com/2011/01/fabric&amp;quot;&amp;gt;
  &amp;lt;Parameters&amp;gt;
    &amp;lt;Parameter Name=&amp;quot;RatesService_ProviderName&amp;quot; DefaultValue=&amp;quot;&amp;quot; /&amp;gt;
  &amp;lt;/Parameters&amp;gt;
  ...
&amp;lt;/ApplicationManifest&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, at the deployment time (as you will see in more detail in the deployment script), we will specify a PowerShell &lt;code&gt;Hashtable&lt;/code&gt; to override these parameters per named instance:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;New-ServiceFabricApplication -ApplicationTypeName $appTypeName -ApplicationTypeVersion $version -ApplicationName $appName -ApplicationParameter &amp;#64;{&amp;quot;RatesService_ProviderName&amp;quot; = &amp;quot;Contoso&amp;quot;}  

New-ServiceFabricApplication -ApplicationTypeName $appTypeName -ApplicationTypeVersion $version -ApplicationName $appName -ApplicationParameter &amp;#64;{&amp;quot;RatesService_ProviderName&amp;quot; = &amp;quot;Fabrican&amp;quot;}  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;RatesService&lt;/code&gt; code will then make sure of this parameter to contact the instance-bound provider.&lt;/p&gt;
&lt;h2 id="powershell-management-scripts"&gt;PowerShell Management Scripts&lt;/h2&gt;
&lt;h3 id="deployment"&gt;Deployment&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;This PowerShell script assumes that you used Visual Studio to generate the Service Fabric app package info (right-click the Service Fabric App and select &lt;code&gt;Package&lt;/code&gt;) or you built the app package manually as demonstrated in a previous &lt;a href="../posts/2016-12-02-service-fabric-basics"&gt;post&lt;/a&gt;. The created package directory is expected to have the following format &lt;code&gt;v1.0.0&lt;/code&gt; where 1.0.0 is the version number&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This PowerShell script copies the package to the cluster, registers the app type and creates two name app instances (i.e. Contoso and Fabrican). In each app instance, create two services: Web Service as a front-end and Rates service as a back-end.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$clusterUrl = &amp;quot;localhost&amp;quot;
$imageStoreConnectionString = &amp;quot;file:C:\SfDevCluster\Data\ImageStoreShare&amp;quot;   # Use this with OneBox
If ($clusterUrl -ne &amp;quot;localhost&amp;quot;)
{
    $imageStoreConnectionString = &amp;quot;fabric:ImageStore&amp;quot;                       # Use this when not using OneBox
}

# Used only for the inmage store....it can be any name!!!
$appPkgName = &amp;quot;RateAggregatorAppTypePkg&amp;quot;

# Define the app and service types
$appTypeName = &amp;quot;RateAggregatorAppType&amp;quot;
$webServiceTypeName = &amp;quot;WebServiceType&amp;quot;
$ratesServiceTypeName = &amp;quot;RatesServiceType&amp;quot;

# Define the version
$version = &amp;quot;1.0.0&amp;quot;

# Connect PowerShell session to a cluster
Connect-ServiceFabricCluster -ConnectionEndpoint ${clusterUrl}:19000

# Copy the application package to the cluster
Copy-ServiceFabricApplicationPackage -ApplicationPackagePath &amp;quot;RateAggregatorApp\pkg\v$version&amp;quot; -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Register the application package's application type/version
Register-ServiceFabricApplicationType -ApplicationPathInImageStore $appPkgName

# After registering the package's app type/version, you can remove the package
Remove-ServiceFabricApplicationPackage -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Deploy the first aplication name (i.e. Contoso)
$appName = &amp;quot;fabric:/ContosoRateAggregatorApp&amp;quot;
$webServiceName = $appName + &amp;quot;/WebService&amp;quot;
$ratesServiceName = $appName + &amp;quot;/RatesService&amp;quot;

# Create a named application from the registered app type/version
New-ServiceFabricApplication -ApplicationTypeName $appTypeName -ApplicationTypeVersion $version -ApplicationName $appName -ApplicationParameter &amp;#64;{&amp;quot;RatesService_ProviderName&amp;quot; = &amp;quot;Contoso&amp;quot;}  

# Create a named service within the named app from the service's type
New-ServiceFabricService -ApplicationName $appName -ServiceTypeName $webServiceTypeName -ServiceName $webServiceName -Stateless -PartitionSchemeSingleton -InstanceCount 1

# Create a named service within the named app from the service's type
# For stateful services, it is important to indicate in the service manifest that the service is stateful and that it has a persisted state:
# &amp;lt;StatefulServiceType ServiceTypeName=&amp;quot;RatesServiceType&amp;quot; HasPersistedState=&amp;quot;true&amp;quot;/&amp;gt;
# Actually all of these switches are important on the PowerShell command:
# -PartitionSchemeUniformInt64 $true -PartitionCount 4 -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -LowKey 0 -HighKey 3 -HasPersistedState
New-ServiceFabricService -ApplicationName $appName -ServiceTypeName $ratesServiceTypeName -ServiceName $ratesServiceName -PartitionSchemeUniformInt64 $true -PartitionCount 4 -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -LowKey 0 -HighKey 3 -HasPersistedState

# Deploy the second aplication name (i.e. Fabrican)
$appName = &amp;quot;fabric:/FabricanRateAggregatorApp&amp;quot;
$webServiceName = $appName + &amp;quot;/WebService&amp;quot;
$ratesServiceName = $appName + &amp;quot;/RatesService&amp;quot;

# Create a named application from the registered app type/version
New-ServiceFabricApplication -ApplicationTypeName $appTypeName -ApplicationTypeVersion $version -ApplicationName $appName -ApplicationParameter &amp;#64;{&amp;quot;RatesService_ProviderName&amp;quot; = &amp;quot;Fabrican&amp;quot;}  

# Create a named service within the named app from the service's type
New-ServiceFabricService -ApplicationName $appName -ServiceTypeName $webServiceTypeName -ServiceName $webServiceName -Stateless -PartitionSchemeSingleton -InstanceCount 1

# Create a named service within the named app from the service's type
New-ServiceFabricService -ApplicationName $appName -ServiceTypeName $ratesServiceTypeName -ServiceName $ratesServiceName -PartitionSchemeUniformInt64 $true -PartitionCount 4 -MinReplicaSetSize 2 -TargetReplicaSetSize 3 -LowKey 0 -HighKey 3 -HasPersistedState
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="obliterate"&gt;Obliterate&lt;/h3&gt;
&lt;p&gt;This PowerShell script removes all application name instances and their services from the selected cluster. It does this based on the application type.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$clusterUrl = &amp;quot;localhost&amp;quot;

# Define the app and service types
$applicationTypes = &amp;quot;RateAggregatorAppType&amp;quot;

# Connect PowerShell session to a cluster
Connect-ServiceFabricCluster -ConnectionEndpoint ${clusterUrl}:19000

# Remove all application names instances and their services
Get-ServiceFabricApplication | Where-Object { $applicationTypes -contains $_.ApplicationTypeName } | Remove-ServiceFabricApplication -Force
Get-ServiceFabricApplicationType | Where-Object { $applicationTypes -contains $_.ApplicationTypeName } | Unregister-ServiceFabricApplicationType -Force

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="update"&gt;Update&lt;/h3&gt;
&lt;p&gt;This PowerShell script updates the web service in each app named instance to have 5 instances. Please note that this works if the number of instances does  not exceed the number of nodes in the cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$clusterUrl = &amp;quot;localhost&amp;quot;

# Deploy the first aplication name (i.e. Contoso)
$appName = &amp;quot;fabric:/ContosoRateAggregatorApp&amp;quot;
$webServiceName = $appName + &amp;quot;/WebService&amp;quot;

# Dynamically change the named service's number of instances (the cluster must have at least 5 nodes)
Update-ServiceFabricService -ServiceName $webServiceName -Stateless -InstanceCount 5 -Force

# Deploy the first aplication name (i.e. Fabrican)
$appName = &amp;quot;fabric:/FabricanRateAggregatorApp&amp;quot;
$webServiceName = $appName + &amp;quot;/WebService&amp;quot;

# Dynamically change the named service's number of instances (the cluster must have at least 5 nodes) 
Update-ServiceFabricService -ServiceName $webServiceName -Stateless -InstanceCount 5 -Force
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="upgrade"&gt;Upgrade&lt;/h3&gt;
&lt;p&gt;This PowerShell script upgrades the application named instances to a higher version i.e. 1.1.0. As noted earlier, this assumes that you have a new folder named v1.1.0 which contains the upgraded application package. The script uses &lt;code&gt;monitored&lt;/code&gt; upgrade modes and performs the upgrade using upgrade domains.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$clusterUrl = &amp;quot;localhost&amp;quot;
$imageStoreConnectionString = &amp;quot;file:C:\SfDevCluster\Data\ImageStoreShare&amp;quot;   # Use this with OneBox
If ($clusterUrl -ne &amp;quot;localhost&amp;quot;)
{
    $imageStoreConnectionString = &amp;quot;fabric:ImageStore&amp;quot;                       # Use this when not using OneBox
}

# Used only for the inmage store....it can be any name!!!
$appPkgName = &amp;quot;RateAggregatorAppTypePkg&amp;quot;

# Define the new version
$version = &amp;quot;1.1.0&amp;quot;

# Connect PowerShell session to a cluster
Connect-ServiceFabricCluster -ConnectionEndpoint ${clusterUrl}:19000

# Copy the application package to the cluster
Copy-ServiceFabricApplicationPackage -ApplicationPackagePath &amp;quot;RateAggregatorApp\pkg\v$version&amp;quot; -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Register the application package's application type/version
Register-ServiceFabricApplicationType -ApplicationPathInImageStore $appPkgName

# After registering the package's app type/version, you can remove the package
Remove-ServiceFabricApplicationPackage -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Upgrade the first aplication name (i.e. Contoso)
$appName = &amp;quot;fabric:/ContosoRateAggregatorApp&amp;quot;

# Upgrade the application to the new version
Start-ServiceFabricApplicationUpgrade -ApplicationName $appName -ApplicationTypeVersion $version -Monitored -UpgradeReplicaSetCheckTimeoutSec 100

# Upgrade the second aplication name (i.e. Fabrican)
$appName = &amp;quot;fabric:/FabricanRateAggregatorApp&amp;quot;

# Upgrade the application to the new version
Start-ServiceFabricApplicationUpgrade -ApplicationName $appName -ApplicationTypeVersion $version -Monitored -UpgradeReplicaSetCheckTimeoutSec 100

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="test"&gt;Test&lt;/h3&gt;
&lt;p&gt;This PowerShell scripts defines functions to exercise the Service Fabric Web service APIs for each named application instance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Function Generate-RateRequests($appName = 'Contoso', $iterations = 20)
{
    Try {
        Write-Host &amp;quot;Generating $iterations random rate requests against $appName ....&amp;quot; -ForegroundColor Green

        $url = &amp;quot;Http://localhost:8082/$appName&amp;quot; + &amp;quot;RateAggregatorApp/api/requests&amp;quot;

        foreach($i in 1..$iterations)
        {
            $checkInDate = get-date -Year (get-random -minimum 2012 -maximum 2016) -Month (get-random -minimum 1 -maximum 12) -Day (get-random -minimum 1 -maximum 28)
            $nights = get-random -minimum 1 -maximum 30
            $checkOutDate = $checkInDate.AddDays($nights)
            $hotelId = get-random -input &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot; -count 1
            $body = &amp;#64;{
                CheckInDate = get-date $checkInDate -Format &amp;quot;yyy-MM-ddTHH:mm:ss&amp;quot;;
                CheckOutDate = get-date $checkOutDate -Format &amp;quot;yyy-MM-ddTHH:mm:ss&amp;quot;;
                HotelId = $hotelId;
                HotelName = &amp;quot;Hotel$hotelId&amp;quot;;
                City = &amp;quot;City$hotelId&amp;quot;;
                Country = get-random -input &amp;quot;USA&amp;quot;, &amp;quot;USA&amp;quot;, &amp;quot;USA&amp;quot;, &amp;quot;CAN&amp;quot;, &amp;quot;CAN&amp;quot;, &amp;quot;CAN&amp;quot;, &amp;quot;AUS&amp;quot;, &amp;quot;AUS&amp;quot;, &amp;quot;AUS&amp;quot;, &amp;quot;FRA&amp;quot;, &amp;quot;GER&amp;quot;, &amp;quot;UAE&amp;quot; -count 1
            }
            Write-Host &amp;quot;This is the JSON we are generating for iteration # $i....&amp;quot; -ForegroundColor yellow
            $json = ConvertTo-Json $body -Depth 3
            $json

	        $result = Invoke-RestMethod -Uri $url -Headers &amp;#64;{&amp;quot;Content-Type&amp;quot;=&amp;quot;application/json&amp;quot; } -Body $json -Method POST -TimeoutSec 600
        }        
    } Catch {
        Write-Host &amp;quot;Failure message: $_.Exception.Message&amp;quot; -ForegroundColor red
        Write-Host &amp;quot;Failure stack trace: $_.Exception.StackTrace&amp;quot; -ForegroundColor red
        Write-Host &amp;quot;Failure inner exception: $_.Exception.InnerException&amp;quot; -ForegroundColor red
    }
}

Function View-QueueLength($appName = 'Contoso')
{
    Try {
        Write-Host &amp;quot;View Queue Length for $appName....&amp;quot; -ForegroundColor Green

        $url = &amp;quot;Http://localhost:8082/$appName&amp;quot; + &amp;quot;RateAggregatorApp/api/stats/queue/length&amp;quot;
	    $result = Invoke-RestMethod -Uri $url -Headers &amp;#64;{&amp;quot;Content-Type&amp;quot;=&amp;quot;application/json&amp;quot; } -Method GET -TimeoutSec 600
        $json = ConvertTo-Json $result -Depth 3
        $json
    } Catch {
        Write-Host &amp;quot;Failure message: $_.Exception.Message&amp;quot; -ForegroundColor red
        Write-Host &amp;quot;Failure stack trace: $_.Exception.StackTrace&amp;quot; -ForegroundColor red
        Write-Host &amp;quot;Failure inner exception: $_.Exception.InnerException&amp;quot; -ForegroundColor red
    }
}

Function View-Cities($appName = 'Contoso')
{
    Try {
        Write-Host &amp;quot;View cities for $appName....&amp;quot; -ForegroundColor Green

        $url = &amp;quot;Http://localhost:8082/$appName&amp;quot; + &amp;quot;RateAggregatorApp/api/stats/cities&amp;quot;
	    $result = Invoke-RestMethod -Uri $url -Headers &amp;#64;{&amp;quot;Content-Type&amp;quot;=&amp;quot;application/json&amp;quot; } -Method GET -TimeoutSec 600
        $json = ConvertTo-Json $result -Depth 3
        $json
    } Catch {
        Write-Host &amp;quot;Failure message: $_.Exception.Message&amp;quot; -ForegroundColor red
        Write-Host &amp;quot;Failure stack trace: $_.Exception.StackTrace&amp;quot; -ForegroundColor red
        Write-Host &amp;quot;Failure inner exception: $_.Exception.InnerException&amp;quot; -ForegroundColor red
    }
}

Generate-RateRequests -appName Contoso -iterations 100
Generate-RateRequests -appName Fabrican -iterations 100

View-QueueLength -appName Contoso
View-QueueLength -appName Fabrican

View-Cities -appName Contoso
View-Cities -appName Fabrican

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="what-is-next"&gt;What is next?&lt;/h2&gt;
&lt;p&gt;I think Service Fabric has a lot of great and useful features that make it is a great candidate for a lot of scenarios. I will post more articles about Service Fabric as I expand my knowledge in this really cool technology.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;The &lt;a href="https://github.com/Azure-Samples/service-fabric-dotnet-iot"&gt;Service Fabric iOT sample app&lt;/a&gt; is a great sample to follow for our own Service Fabric apps. In this post, I used code snippets and concepts from the iOT sample to build a small app to demonstrate some fundamentals concepts that I feel are important.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2016-12-02-service-fabric-basics" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2016-12-02-service-fabric-basics</id>
		<title>Service Fabric Basics</title>
		<updated>2016-12-02T00:00:00Z</updated>
		<content>&lt;p&gt;Service Fabric is a cool technology from Microsoft! It has advanced features that allows many scenarios. But in this post, we will only cover basic concepts that are usually misunderstood by a lot of folks.&lt;/p&gt;
&lt;p&gt;For the purpose of this demo, we are going to develop a very basic guest executable service written as a console app. We will use very basic application and service manifests and PowerShell script to deploy to Service Fabric and show how Service Fabric monitors services, reports their health and allows for upgrade and update.&lt;/p&gt;
&lt;p&gt;The source code for this post is available &lt;a href="https://github.com/khaledhikmat/service-fabric-basics"&gt;here&lt;/a&gt;. Most of the code and ideas are credited to Jeff Richter of the Service Fabric Team.&lt;/p&gt;
&lt;h2 id="guest-service"&gt;Guest Service&lt;/h2&gt;
&lt;p&gt;The Guest service is a basic &lt;code&gt;Win32&lt;/code&gt; console app that invokes an &lt;code&gt;HttpListener&lt;/code&gt; on a port that is passed in the argument. The little web server responds to requests like so:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/YqTjqBD.png" class="img-fluid" alt="Web Server" /&gt;&lt;/p&gt;
&lt;p&gt;Note that the service is NOT running the Service Fabric cluster.&lt;/p&gt;
&lt;p&gt;That is it!! This simple web server accepts a command called &lt;code&gt;crash&lt;/code&gt; which will kill the service completely:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;http://localhost:8800?cmd=crash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In fact, it does support multiple commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var command = request.QueryString[&amp;quot;cmd&amp;quot;];
if (!string.IsNullOrEmpty(command))
{
    switch (command.ToLowerInvariant())
    {
        case &amp;quot;delay&amp;quot;:
            Int32.TryParse(request.QueryString[&amp;quot;delay&amp;quot;], out _delay);
            break;
        case &amp;quot;crash&amp;quot;:
            Environment.Exit(-1);
            break;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to make this service highly available, let us see how we can package this service to run within Service Fabric. Please note that this service is not cognizant of any Service Fabric. It is purely a simple &lt;code&gt;Win32&lt;/code&gt; service written as a console app.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To debug the service locally from Visual Studio, you need to start VS in administrator mode.&lt;/li&gt;
&lt;li&gt;Service Fabric requires the projects be &lt;code&gt;X64&lt;/code&gt;! So you must change your projects to use &lt;code&gt;X64&lt;/code&gt; by using the Visual Studio Configuration Manager.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="application-package"&gt;Application Package&lt;/h2&gt;
&lt;p&gt;Application Package in Service Fabric is nothing but a folder that contains certain manifests in specific sub-folders! We will build the directory by hand instead of using Visual Studio so we can find out exactly how to do these steps. Let us create a directory called &lt;code&gt;BasicAvailabilityApp&lt;/code&gt; (i.e.  &lt;code&gt;c:\BasicAvailabilityApp&lt;/code&gt;) to describe the Service Fabric application.&lt;/p&gt;
&lt;h3 id="the-root-folder"&gt;The root folder&lt;/h3&gt;
&lt;p&gt;The root folder contains the application manifest and a sub-folder for each service in contains. Here is how the application manifest looks like for this demo application:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt;
&amp;lt;ApplicationManifest ApplicationTypeName=&amp;quot;BasicAvailabilityAppType&amp;quot; ApplicationTypeVersion=&amp;quot;1.0.0&amp;quot;
                     xmlns:xsd=&amp;quot;http://www.w3.org/2001/XMLSchema&amp;quot; 
                     xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; 
                     xmlns=&amp;quot;http://schemas.microsoft.com/2011/01/fabric&amp;quot;&amp;gt;
   &amp;lt;ServiceManifestImport&amp;gt;
      &amp;lt;ServiceManifestRef ServiceManifestName=&amp;quot;CrashableServiceTypePkg&amp;quot; ServiceManifestVersion=&amp;quot;1.0.0&amp;quot; /&amp;gt;
   &amp;lt;/ServiceManifestImport&amp;gt;
&amp;lt;/ApplicationManifest&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are several pieces of information in this manifest:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The application type: &lt;code&gt;BasicAvailabilityAppType&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The application version: &lt;code&gt;1.0.0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The application contains a single service type &lt;code&gt;CrashableServiceTypePkg&lt;/code&gt; with version &lt;code&gt;1.0.0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The XML name spaces are not important to us.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is how the application folder looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/VMARUS3.png" class="img-fluid" alt="Root Application Folder" /&gt;&lt;/p&gt;
&lt;h3 id="the-service-folder"&gt;The service folder&lt;/h3&gt;
&lt;p&gt;The service folder contains the service manifest and a sub-folder for each service in contains. Here is how the application manifest looks like for this demo application:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt;
&amp;lt;ServiceManifest Name=&amp;quot;CrashableServiceTypePkg&amp;quot;
                 Version=&amp;quot;1.0.0&amp;quot;
                 xmlns=&amp;quot;http://schemas.microsoft.com/2011/01/fabric&amp;quot;
                 xmlns:xsd=&amp;quot;http://www.w3.org/2001/XMLSchema&amp;quot;
                 xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;&amp;gt;
  &amp;lt;ServiceTypes&amp;gt;
    &amp;lt;StatelessServiceType ServiceTypeName=&amp;quot;CrashableServiceType&amp;quot; UseImplicitHost=&amp;quot;true&amp;quot; /&amp;gt;
  &amp;lt;/ServiceTypes&amp;gt;

  &amp;lt;CodePackage Name=&amp;quot;CrashableCodePkg&amp;quot; Version=&amp;quot;1.0.0&amp;quot;&amp;gt;
    &amp;lt;EntryPoint&amp;gt;
      &amp;lt;ExeHost&amp;gt;
        &amp;lt;Program&amp;gt;CrashableService.exe&amp;lt;/Program&amp;gt;
        &amp;lt;Arguments&amp;gt;8800&amp;lt;/Arguments&amp;gt;
      &amp;lt;/ExeHost&amp;gt;
    &amp;lt;/EntryPoint&amp;gt;
  &amp;lt;/CodePackage&amp;gt;

  &amp;lt;!-- ACL the 8800 port where the crashable service listens --&amp;gt;
  &amp;lt;Resources&amp;gt;
    &amp;lt;Endpoints&amp;gt;
      &amp;lt;Endpoint Name=&amp;quot;InputEndpoint&amp;quot; Port=&amp;quot;8800&amp;quot; Protocol=&amp;quot;http&amp;quot; Type=&amp;quot;Input&amp;quot; /&amp;gt;
    &amp;lt;/Endpoints&amp;gt;
  &amp;lt;/Resources&amp;gt;
&amp;lt;/ServiceManifest&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are several pieces of information in this manifest:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The service package: &lt;code&gt;CrashableServiceTypePkg&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The service version: &lt;code&gt;1.0.0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The service type: &lt;code&gt;CrashableServiceType&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The service type is stateless.&lt;/li&gt;
&lt;li&gt;The service code package exists in a sub-folder called &lt;code&gt;CodePkg&lt;/code&gt; and it is of version &lt;code&gt;1.0.0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The service code consists of an executable called &lt;code&gt;CrashableService.exe&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The XML name spaces are not important to us.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;Endoints&lt;/code&gt; must be specified to allow the Service Fabric to &lt;code&gt;ACL&lt;/code&gt; the port that we want opened for our service to listen on. The &lt;code&gt;Input&lt;/code&gt; type instructs SF to accepts input from the Internet.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is how the service folder looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/0bA48hy.png" class="img-fluid" alt="service Folder" /&gt;&lt;/p&gt;
&lt;p&gt;This is what it takes to package an application in Service Fabric.&lt;/p&gt;
&lt;h2 id="deployment"&gt;Deployment&lt;/h2&gt;
&lt;p&gt;Please note that the package we created in the previous step needs to be deployed to Service Fabric in order to run. To do this, we will need to use either Visual Studio or PowerShell. Since we want to use the lower level commands, we will use PowerShell instead of Visual Studio:&lt;/p&gt;
&lt;p&gt;Here is the PowerShell script that we can use:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Define equates (hard-coded):
$clusterUrl = &amp;quot;localhost&amp;quot;
$imageStoreConnectionString = &amp;quot;file:C:\SfDevCluster\Data\ImageStoreShare&amp;quot; 
$appPkgName = &amp;quot;BasicAvailabilityAppTypePkg&amp;quot;
$appTypeName = &amp;quot;BasicAvailabilityAppType&amp;quot;
$appName = &amp;quot;fabric:/BasicAvailabilityApp&amp;quot;
$serviceTypeName = &amp;quot;CrashableServiceType&amp;quot;
$serviceName = $appName + &amp;quot;/CrashableService&amp;quot;

# Connect PowerShell session to a cluster
Connect-ServiceFabricCluster -ConnectionEndpoint ${clusterUrl}:19000

# Copy the application package to the cluster
Copy-ServiceFabricApplicationPackage -ApplicationPackagePath &amp;quot;BasicAvailabilityApp&amp;quot; -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Register the application package's application type/version
Register-ServiceFabricApplicationType -ApplicationPathInImageStore $appPkgName

# After registering the package's app type/version, you can remove the package from the cluster image store
Remove-ServiceFabricApplicationPackage -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Create a named application from the registered app type/version
New-ServiceFabricApplication -ApplicationTypeName $appTypeName -ApplicationTypeVersion &amp;quot;1.0.0&amp;quot; -ApplicationName $appName 

# Create a named service within the named app from the service's type
New-ServiceFabricService -ApplicationName $appName -ServiceTypeName $serviceTypeName -ServiceName $serviceName -Stateless -PartitionSchemeSingleton -InstanceCount 1

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key commands are the last two where we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a named application name from the registered application type and version:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# Create a named application from the registered app type/version
New-ServiceFabricApplication -ApplicationTypeName $appTypeName -ApplicationTypeVersion &amp;quot;1.0.0&amp;quot; -ApplicationName $appName 
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Create a named service within the named app from the service type:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;# Create a named service within the named app from the service's type
New-ServiceFabricService -ApplicationName $appName -ServiceTypeName $serviceTypeName -ServiceName $serviceName -Stateless -PartitionSchemeSingleton -InstanceCount 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is extremely significant as it allows us to create multiple application instances within the same cluster and each named application instance has its own set of services. This is how the named application and services are related to the cluster (this is taken from Service Fabric team presentation):&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/377RP4J.png" class="img-fluid" alt="Naming Stuff" /&gt;&lt;/p&gt;
&lt;p&gt;Once the named application and the named service are deployed, the Service Fabric explorer shows it like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/tTLgIMX.png" class="img-fluid" alt="Success Deployment" /&gt;&lt;/p&gt;
&lt;p&gt;Now, if we access the service in Service Fabric, we will get a response that clearly indicates that the service is indeed running in Service Fabric:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/AI9jYOV.png" class="img-fluid" alt="Deployed in SF" /&gt;&lt;/p&gt;
&lt;p&gt;Note that the service is running in Node 1 of the Service Fabric cluster.&lt;/p&gt;
&lt;h2 id="availability"&gt;Availability&lt;/h2&gt;
&lt;p&gt;One of the major selling points of Service Fabric is its ability to make services highly available by monitoring them and restarting them if necessary.&lt;/p&gt;
&lt;p&gt;Regardless of whether the service is guest executable or Service Fabric cognizant service, Service Fabric monitors the service to make sure it runs correctly. In our case, the service will crash whenever a &lt;code&gt;crash&lt;/code&gt; command is submitted. So if you crash the service, you will see that Service Fabric detects the failure and reports a bad health on the Service Fabric Explorer:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/XdY1JHg.png" class="img-fluid" alt="Error Deployment" /&gt;&lt;/p&gt;
&lt;p&gt;You will notice that the little web server is no longer available when you try to access it. But if you wait for a few seconds and try again, you will be very happy to know that the web server is available again. This is because Service Fabric detected that the service went down, restarted it and made it available holding to the promise of &lt;code&gt;high availability&lt;/code&gt; or &lt;code&gt;self healing&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;However, there is only one little problem! The unhealthy indicators (warning or errors) on the explorer may never go away because there isn't anything that resets them. So the health checks will also be shown once they are reported. This could become a little of a problem if you have an external tool that read health check state.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The above statement is not entirely true! I have seen the latest versions of Service Fabric remove the warning/errors after a little while.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In any case, I will show a better way (in my opinion) to deal with this shortly in this post. So read on if you are interested.&lt;/p&gt;
&lt;h2 id="cleanup"&gt;Cleanup&lt;/h2&gt;
&lt;p&gt;In order to remove the named application and its services, you can issue these PowerShell commands:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Delete the named service
Remove-ServiceFabricService -ServiceName $serviceName -Force

# Delete the named application and its named services
Remove-ServiceFabricApplication -ApplicationName $appName -Force
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to delete the application type:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# If no named apps are running, you can delete the app type/version
Unregister-ServiceFabricApplicationType -ApplicationTypeName $appTypeName -ApplicationTypeVersion &amp;quot;1.0.0&amp;quot; -Force
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="versions-upgrade"&gt;Versions &amp;amp; Upgrade&lt;/h2&gt;
&lt;p&gt;It turned out that Service Fabric does not really care how you name your versions! If you name your versions as numbers like 1.0.0 or 1.1.0, this naming convention is referred to as &lt;code&gt;Semantic Versioning&lt;/code&gt;. But you are free to use whatever version naming convention you want.&lt;/p&gt;
&lt;p&gt;Let us use a different version scheme for our simple app. How about alpha, beta and productionV1, productionV2, etc. Let us cleanup our app from the cluster (as shown above), apply some changes to the &lt;code&gt;crashable&lt;/code&gt; service, update the manifest files to make the version &lt;code&gt;Beta&lt;/code&gt; and re-deploy using the beta version:&lt;/p&gt;
&lt;h3 id="the-application-manifest"&gt;The Application Manifest&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt;
&amp;lt;ApplicationManifest ApplicationTypeName=&amp;quot;BasicAvailabilityAppType&amp;quot; 
					 ApplicationTypeVersion=&amp;quot;Beta&amp;quot;
                     xmlns:xsd=&amp;quot;http://www.w3.org/2001/XMLSchema&amp;quot; 
                     xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot; 
                     xmlns=&amp;quot;http://schemas.microsoft.com/2011/01/fabric&amp;quot;&amp;gt;
   &amp;lt;ServiceManifestImport&amp;gt;
      &amp;lt;ServiceManifestRef ServiceManifestName=&amp;quot;CrashableServiceTypePkg&amp;quot; ServiceManifestVersion=&amp;quot;Beta&amp;quot; /&amp;gt;
   &amp;lt;/ServiceManifestImport&amp;gt;
&amp;lt;/ApplicationManifest&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="the-service-manifest"&gt;The Service Manifest&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt;
&amp;lt;ServiceManifest Name=&amp;quot;CrashableServiceTypePkg&amp;quot;
                 Version=&amp;quot;Beta&amp;quot;
                 xmlns=&amp;quot;http://schemas.microsoft.com/2011/01/fabric&amp;quot;
                 xmlns:xsd=&amp;quot;http://www.w3.org/2001/XMLSchema&amp;quot;
                 xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;&amp;gt;
  &amp;lt;ServiceTypes&amp;gt;
    &amp;lt;StatelessServiceType ServiceTypeName=&amp;quot;CrashableServiceType&amp;quot; UseImplicitHost=&amp;quot;true&amp;quot; /&amp;gt;
  &amp;lt;/ServiceTypes&amp;gt;

  &amp;lt;CodePackage Name=&amp;quot;CrashableCodePkg&amp;quot; Version=&amp;quot;Beta&amp;quot;&amp;gt;
    &amp;lt;EntryPoint&amp;gt;
      &amp;lt;ExeHost&amp;gt;
        &amp;lt;Program&amp;gt;CrashableService.exe&amp;lt;/Program&amp;gt;
        &amp;lt;Arguments&amp;gt;8800&amp;lt;/Arguments&amp;gt;
      &amp;lt;/ExeHost&amp;gt;
    &amp;lt;/EntryPoint&amp;gt;
  &amp;lt;/CodePackage&amp;gt;

  &amp;lt;!-- ACL the 8800 port where the crashable service listens --&amp;gt;
  &amp;lt;Resources&amp;gt;
    &amp;lt;Endpoints&amp;gt;
      &amp;lt;Endpoint Name=&amp;quot;InputEndpoint&amp;quot; Port=&amp;quot;8800&amp;quot; Protocol=&amp;quot;http&amp;quot; Type=&amp;quot;Input&amp;quot; /&amp;gt;
    &amp;lt;/Endpoints&amp;gt;
  &amp;lt;/Resources&amp;gt;
&amp;lt;/ServiceManifest&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="deployment-1"&gt;Deployment&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;# Define equates (hard-coded):
$clusterUrl = &amp;quot;localhost&amp;quot;
$imageStoreConnectionString = &amp;quot;file:C:\SfDevCluster\Data\ImageStoreShare&amp;quot; 
$appPkgName = &amp;quot;BasicAvailabilityAppTypePkg&amp;quot;
$appTypeName = &amp;quot;BasicAvailabilityAppType&amp;quot;
$appName = &amp;quot;fabric:/BasicAvailabilityApp&amp;quot;
$serviceTypeName = &amp;quot;CrashableServiceType&amp;quot;
$serviceName = $appName + &amp;quot;/CrashableService&amp;quot;

# Connect PowerShell session to a cluster
Connect-ServiceFabricCluster -ConnectionEndpoint ${clusterUrl}:19000

# Copy the application package to the cluster
Copy-ServiceFabricApplicationPackage -ApplicationPackagePath &amp;quot;BasicAvailabilityApp&amp;quot; -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Register the application package's application type/version
Register-ServiceFabricApplicationType -ApplicationPathInImageStore $appPkgName

# After registering the package's app type/version, you can remove the package from the cluster image store
Remove-ServiceFabricApplicationPackage -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Create a named application from the registered app type/version
New-ServiceFabricApplication -ApplicationTypeName $appTypeName -ApplicationTypeVersion &amp;quot;Beta&amp;quot; -ApplicationName $appName 

# Create a named service within the named app from the service's type
New-ServiceFabricService -ApplicationName $appName -ServiceTypeName $serviceTypeName -ServiceName $serviceName -Stateless -PartitionSchemeSingleton -InstanceCount 1
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="upgrade"&gt;Upgrade&lt;/h3&gt;
&lt;p&gt;Now that the beta version is deployed, let us make another change in the service, change the version to ProdutionV1 (in the application and service manifests) and issue the following PowerShell commands to register and upgrade to &lt;code&gt;ProductionV1&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Copy the application package ProductionV1 to the cluster
Copy-ServiceFabricApplicationPackage -ApplicationPackagePath &amp;quot;BasicAvailabilityApp-ProductionV1&amp;quot; -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Register the application package's application type/version
Register-ServiceFabricApplicationType -ApplicationPathInImageStore $appPkgName

# After registering the package's app type/version, you can remove the package
Remove-ServiceFabricApplicationPackage -ImageStoreConnectionString $imageStoreConnectionString -ApplicationPackagePathInImageStore $appPkgName

# Upgrade the application from Beta to ProductionV1
Start-ServiceFabricApplicationUpgrade -ApplicationName $appName -ApplicationTypeVersion &amp;quot;ProductionV1&amp;quot; -UnmonitoredAuto -UpgradeReplicaSetCheckTimeoutSec 100
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The upgrade takes place using a concept called Upgrade Domains which makes sure that the service that is being upgraded does not ever become unavailable:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/eSmVVHd.png" class="img-fluid" alt="Upgrade Domains" /&gt;&lt;/p&gt;
&lt;p&gt;Once the upgrade is done, the new application and service version is &lt;code&gt;ProductionV1&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/l5Ohfgk.png" class="img-fluid" alt="Production V1" /&gt;&lt;/p&gt;
&lt;h2 id="updates"&gt;Updates&lt;/h2&gt;
&lt;p&gt;Now that our service is in production, let us see what how we can increase and decrease its number of instances at will. This is very useful to scale the service up and down depending on parameters determined by the operations team.&lt;/p&gt;
&lt;p&gt;You may have noticed that we have always used instance count 1 when we deployed our named service:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Create a named service within the named app from the service's type
New-ServiceFabricService -ApplicationName $appName -ServiceTypeName $serviceTypeName -ServiceName $serviceName -Stateless -PartitionSchemeSingleton -InstanceCount 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us try to increase the instance count to 5 using PowerShell:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Dynamically change the named service's number of instances
Update-ServiceFabricService -ServiceName $serviceName -Stateless -InstanceCount 5 -Force
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Please note&lt;/strong&gt; that if your test cluster has less than 5 nodes, you will get health warnings from Service Fabric because SF will not be place more instances than the number of available nodes. This is because SF cannot guarantee availability if it places multiple instances on the same node.&lt;/p&gt;
&lt;p&gt;Anyway, if you get health warning or if you would like to scale back on your service, you can downgrade the number of instances using this PowerShell command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Update-ServiceFabricService -ServiceName $serviceName -Stateless -InstanceCount 1 -Force
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Please notice how fast the scaling (up or down) takes place!!&lt;/p&gt;
&lt;h2 id="better-high-availability"&gt;Better High Availability&lt;/h2&gt;
&lt;p&gt;In a previous section in this post, we deployed the &lt;code&gt;crashable&lt;/code&gt; service and watched it crash when we submitted a &lt;code&gt;crash&lt;/code&gt; command. Service Fabric reported the failure, restarted the service and made it available again. Now we will modify the deployment process to provide a better way to take care of the re-start process.&lt;/p&gt;
&lt;p&gt;To do so, we will need another service that monitors our &lt;code&gt;crashable&lt;/code&gt; service and reports health checks to Service Fabric. This new code is Service Fabric aware and is demonstrated by Jeff Richter of the Service Fabric team.&lt;/p&gt;
&lt;p&gt;Let us modify the application package to include this new code. Remember our goal is not to change the &lt;code&gt;crashable&lt;/code&gt; service at all.&lt;/p&gt;
&lt;h3 id="the-service-manifest-1"&gt;The Service Manifest&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-xml"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt;
&amp;lt;ServiceManifest Name=&amp;quot;CrashableServiceTypePkg&amp;quot;
                 Version=&amp;quot;Beta&amp;quot;
                 xmlns=&amp;quot;http://schemas.microsoft.com/2011/01/fabric&amp;quot;
                 xmlns:xsd=&amp;quot;http://www.w3.org/2001/XMLSchema&amp;quot;
                 xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;&amp;gt;
  &amp;lt;ServiceTypes&amp;gt;
    &amp;lt;StatelessServiceType ServiceTypeName=&amp;quot;CrashableServiceType&amp;quot; UseImplicitHost=&amp;quot;true&amp;quot; /&amp;gt;
  &amp;lt;/ServiceTypes&amp;gt;

  &amp;lt;!-- Code that is NOT Service-Fabric aware --&amp;gt;
  &amp;lt;!-- Remove Console Redirection in production --&amp;gt;
  &amp;lt;!-- https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-deploy-existing-app --&amp;gt;
  &amp;lt;CodePackage Name=&amp;quot;CrashableCodePkg&amp;quot; Version=&amp;quot;Beta&amp;quot;&amp;gt;
    &amp;lt;EntryPoint&amp;gt;
      &amp;lt;ExeHost&amp;gt;
        &amp;lt;Program&amp;gt;CrashableService.exe&amp;lt;/Program&amp;gt;
        &amp;lt;Arguments&amp;gt;8800&amp;lt;/Arguments&amp;gt;
		&amp;lt;ConsoleRedirection FileRetentionCount=&amp;quot;5&amp;quot; FileMaxSizeInKb=&amp;quot;2048&amp;quot;/&amp;gt;
      &amp;lt;/ExeHost&amp;gt;
    &amp;lt;/EntryPoint&amp;gt;
  &amp;lt;/CodePackage&amp;gt;

  &amp;lt;!-- Code that is Service-Fabric aware --&amp;gt;
  &amp;lt;!-- Remove Console Redirection in production --&amp;gt;
  &amp;lt;!-- https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-deploy-existing-app --&amp;gt;
  &amp;lt;CodePackage Name=&amp;quot;MonitorCodePkg&amp;quot; Version=&amp;quot;Beta&amp;quot;&amp;gt;
    &amp;lt;EntryPoint&amp;gt;
      &amp;lt;ExeHost&amp;gt;
        &amp;lt;Program&amp;gt;MonitorService.exe&amp;lt;/Program&amp;gt;
        &amp;lt;Arguments&amp;gt;8800&amp;lt;/Arguments&amp;gt;
		&amp;lt;ConsoleRedirection FileRetentionCount=&amp;quot;5&amp;quot; FileMaxSizeInKb=&amp;quot;2048&amp;quot;/&amp;gt;
      &amp;lt;/ExeHost&amp;gt;
    &amp;lt;/EntryPoint&amp;gt;
  &amp;lt;/CodePackage&amp;gt;

  &amp;lt;!-- ACL the 8800 port where the crashable service listens --&amp;gt;
  &amp;lt;Resources&amp;gt;
    &amp;lt;Endpoints&amp;gt;
      &amp;lt;Endpoint Name=&amp;quot;InputEndpoint&amp;quot; Port=&amp;quot;8800&amp;quot; Protocol=&amp;quot;http&amp;quot; Type=&amp;quot;Input&amp;quot; /&amp;gt;
    &amp;lt;/Endpoints&amp;gt;
  &amp;lt;/Resources&amp;gt;
&amp;lt;/ServiceManifest&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are several things here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Our &lt;code&gt;crashable&lt;/code&gt; service is still the same. It accepts an argumengt to tell it which port number to listen on.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ConsoleRedirection&lt;/code&gt; is added to allow us to see the console output in the SF log files. This is to be removed in production.&lt;/li&gt;
&lt;li&gt;Now there is one service i.e. &lt;code&gt;CrashableServiceType&lt;/code&gt; but two code bases: one for the original exe and another code for the monitor that will monitor our &lt;code&gt;crashable&lt;/code&gt; service. This is really nice as it allows us to add Service Fabric code to an existing service without much of intervention.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;Endoints&lt;/code&gt; must be specified to allow the Service Fabric to &lt;code&gt;ACL&lt;/code&gt; the port that we want opened for our service to listen on. The &lt;code&gt;Input&lt;/code&gt; type instructs SF to accepts input from the Internet.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The package folders look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/fYG5oLe.png" class="img-fluid" alt="Advanced Service Dir" /&gt;&lt;/p&gt;
&lt;h3 id="the-monitor-service"&gt;The Monitor Service&lt;/h3&gt;
&lt;p&gt;It is also a console app!! But it includes a Service Fabric Nuget package so it can use the &lt;code&gt;FabricClient&lt;/code&gt; to communicate health checks to the local cluster. Basically, it sets up a timer to check the performance and availability of our &lt;code&gt;crashable&lt;/code&gt; service. It reports to Service Fabric when failures take place.&lt;/p&gt;
&lt;p&gt;Doing so makes our &lt;code&gt;crashable&lt;/code&gt; service much more resilient to crashes or slow performances as it is monitored by the monitored service and re-started if necessary by Service Fabric. The health checks are also cleared much quicker.&lt;/p&gt;
&lt;h3 id="console-outputs-in-the-local-cluster"&gt;Console Outputs in the local cluster&lt;/h3&gt;
&lt;p&gt;You can use the Service Fabric cluster explorer to find out where Service Fabric stores services on disk. This is available from the Nodes section:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/bvXbjc2.png" class="img-fluid" alt="SF Cluster Nodes" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/lqq6NJ4.png" class="img-fluid" alt="Node Disk" /&gt;&lt;/p&gt;
&lt;p&gt;This directory has a &lt;code&gt;log&lt;/code&gt; folder that stores the output of each service. This can be very useful for debug purposes. To use it, however, you must have the &lt;code&gt;ConsoleRedirection&lt;/code&gt; turned on as shown above.&lt;/p&gt;
&lt;h2 id="what-is-next"&gt;What is next?&lt;/h2&gt;
&lt;p&gt;In future posts, I will use Service Fabric .NET programming model to develop and deploy stateless and stateful services to demonstrate Service Fabric fundamental concepts.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;Service Fabric is a cool technology from Microsoft! It has advanced features that allows many scenarios. But in this post, we will only cover basic concepts that are usually misunderstood by a lot of folks.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2016-11-24-presentation-evaluation" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2016-11-24-presentation-evaluation</id>
		<title>Xamarin Forms App using VS for mac</title>
		<updated>2016-11-24T00:00:00Z</updated>
		<content>&lt;p&gt;I am new to Xamarin development and I also wanted to try the newly announced VS for mac! So I created a little Camera app that can be used to evaluate presentations. The app allows the user to take a &lt;code&gt;selfie&lt;/code&gt;. When committed, the picture is sent to an Azure cognitive function to extract the gender, male and smile (a measure of emotion). The app then displays the taken picture and returned result in the app. It also sends the result to PowerBI real-time stream to allow the visualization of the evaluation results.&lt;/p&gt;
&lt;p&gt;So in essence, a user uses the app to take a selfie with a smile or a frown to indicate whether the presentation was good, not so good or somewhere in between. For example, if the user submitted a picture that looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/QPF0LI8.png" class="img-fluid" alt="Evaluation" /&gt;&lt;/p&gt;
&lt;p&gt;The cognitive result might look like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/0U4IaRH.png" class="img-fluid" alt="Cognitive Result" /&gt;&lt;/p&gt;
&lt;p&gt;and the result will be pushed in real-time to a PowerBI dashboard:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/XvnGaVs.png" class="img-fluid" alt="PowerBI" /&gt;&lt;/p&gt;
&lt;h2 id="xamarin-app"&gt;Xamarin App&lt;/h2&gt;
&lt;h3 id="taking-a-camera-picture"&gt;Taking a Camera picture&lt;/h3&gt;
&lt;p&gt;Using VS for mac, I created a blank XAML forms app solution with Android and iOS. Added the following Xamarin plugin to all of its projects (portable, iOS and Droid):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Xam.Plugin.Media&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This allows me to use the Camera without having to deal with iOs or Android. I wrote the following simple XAML in the main page:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;utf-8&amp;quot;?&amp;gt;
&amp;lt;ContentPage xmlns=&amp;quot;http://xamarin.com/schemas/2014/forms&amp;quot; 
		xmlns:x=&amp;quot;http://schemas.microsoft.com/winfx/2009/xaml&amp;quot; 
		xmlns:local=&amp;quot;clr-namespace:PresentationEvaluation&amp;quot; 
		x:Class=&amp;quot;PresentationEvaluation.PresentationEvaluationPage&amp;quot;&amp;gt;
	&amp;lt;StackLayout&amp;gt;
		&amp;lt;Button x:Name=&amp;quot;btnTakePicture&amp;quot; Clicked=&amp;quot;btnTakePicture_Clicked&amp;quot; Text=&amp;quot;Take selfie with emotion&amp;quot;/&amp;gt;
		&amp;lt;ActivityIndicator x:Name=&amp;quot;Indicator&amp;quot; Color=&amp;quot;Black&amp;quot;/&amp;gt;
		&amp;lt;StackLayout x:Name=&amp;quot;ResultPanel&amp;quot; Padding=&amp;quot;10&amp;quot;&amp;gt;
			&amp;lt;Image x:Name=&amp;quot;Image&amp;quot; HeightRequest=&amp;quot;240&amp;quot; /&amp;gt;
	        &amp;lt;StackLayout x:Name=&amp;quot;Age&amp;quot; Orientation=&amp;quot;Horizontal&amp;quot;&amp;gt;
	            &amp;lt;Label&amp;gt;Age&amp;lt;/Label&amp;gt;
	            &amp;lt;Label x:Name=&amp;quot;AgeData&amp;quot;&amp;gt;&amp;lt;/Label&amp;gt;
	        &amp;lt;/StackLayout&amp;gt;
	        &amp;lt;StackLayout x:Name=&amp;quot;Gender&amp;quot; Orientation=&amp;quot;Horizontal&amp;quot;&amp;gt;
	            &amp;lt;Label&amp;gt;Gender&amp;lt;/Label&amp;gt;
	            &amp;lt;Label x:Name=&amp;quot;GenderData&amp;quot;&amp;gt;&amp;lt;/Label&amp;gt;
	        &amp;lt;/StackLayout&amp;gt;
	        &amp;lt;StackLayout x:Name=&amp;quot;Smile&amp;quot; Orientation=&amp;quot;Horizontal&amp;quot;&amp;gt;
	            &amp;lt;Label&amp;gt;Smile&amp;lt;/Label&amp;gt;
	            &amp;lt;Label x:Name=&amp;quot;SmileData&amp;quot;&amp;gt;&amp;lt;/Label&amp;gt;
	        &amp;lt;/StackLayout&amp;gt;
	        &amp;lt;Label x:Name=&amp;quot;Result&amp;quot;&amp;gt;&amp;lt;/Label&amp;gt;
		&amp;lt;/StackLayout&amp;gt;
	&amp;lt;/StackLayout&amp;gt;
&amp;lt;/ContentPage&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and I had this in the behind code:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;namespace PresentationEvaluation
{
	public partial class PresentationEvaluationPage : ContentPage
	{
		public PresentationEvaluationPage()
		{
			InitializeComponent();
			ResultPanel.IsVisible = false;
		}

		private async void btnTakePicture_Clicked(object sender, EventArgs e)
		{
			try
			{
				await CrossMedia.Current.Initialize();

				if (!CrossMedia.Current.IsCameraAvailable || !CrossMedia.Current.IsTakeVideoSupported)
					throw new Exception($&amp;quot;There is no camera on the device!&amp;quot;);

				var file = await CrossMedia.Current.TakePhotoAsync(new Plugin.Media.Abstractions.StoreCameraMediaOptions
				{
					SaveToAlbum = true,
					Name = &amp;quot;SelfieEvaluation.jpg&amp;quot;
				});

				if (file == null)
					throw new Exception($&amp;quot;Picture not captured to disk!!&amp;quot;);

				Image.Source = ImageSource.FromStream(() =&amp;gt; file.GetStream());

				//TODO: Do something with the image 
			}
			catch (Exception ex)
			{
				await DisplayAlert(&amp;quot;Sorry&amp;quot;, &amp;quot;An error occurred: &amp;quot; + ex.Message, &amp;quot;Ok&amp;quot;);
			}
			finally
			{
			}
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="communicating-with-cognitive"&gt;Communicating with Cognitive&lt;/h3&gt;
&lt;p&gt;Now that we got the picture from the Camera, I wanted to send it to Azure Cognitive to detect the age, gender and smile. I added some NuGet packages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Microsoft.Net.Http&lt;/li&gt;
&lt;li&gt;Newton.Json&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First I had to convert the media image file to an array of bytes:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public static byte[] GetBytes(MediaFile file)
{
	byte[] fileBytes = null;
	using (var ms = new MemoryStream())
	{
		file.GetStream().CopyTo(ms);
		file.Dispose();
		fileBytes = ms.ToArray();
	}

	return fileBytes;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then submitted to the congnitive APIs:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;		byte[] picture = GetBytes(file);

		float age = -1;
		string gender = &amp;quot;&amp;quot;;
		float smile = -1;

		// Submit to Cognitive
		using (var httpClient = new HttpClient())
		{
			httpClient.DefaultRequestHeaders.Add(&amp;quot;Ocp-Apim-Subscription-Key&amp;quot;, &amp;quot;get-your-own&amp;quot;);
			HttpResponseMessage response;
			var content = new ByteArrayContent(picture);
			content.Headers.ContentType = new MediaTypeHeaderValue(&amp;quot;application/octet-stream&amp;quot;);
			response = await httpClient.PostAsync(FacialApi, content);
			string responseData = await response.Content.ReadAsStringAsync();
			if (!response.IsSuccessStatusCode)
				throw new Exception($&amp;quot;Unable to post to cognitive service: {response.StatusCode.ToString()}&amp;quot;);

			Face[] faces = JsonConvert.DeserializeObject&amp;lt;Face[]&amp;gt;(responseData);
			if (faces != null &amp;amp;&amp;amp; faces.Length &amp;gt; 0)
			{
				Face face = faces[0];
				age = face.faceAttributes.age;
				gender = face.faceAttributes.gender;
				smile = face.faceAttributes.smile;
			}
		}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where the Face classes are defined as follows (I just special pasted the docs JSON example into my Visual Studio to create these classes):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class Face
{
	public string faceId { get; set; }
	public Facerectangle faceRectangle { get; set; }
	public Faceattributes faceAttributes { get; set; }
	public string glasses { get; set; }
	public Headpose headPose { get; set; }
}

public class Facerectangle
{
	public int width { get; set; }
	public int height { get; set; }
	public int left { get; set; }
	public int top { get; set; }
}

public class Faceattributes
{
	public float age { get; set; }
	public string gender { get; set; }
	public float smile { get; set; }
	public Facialhair facialHair { get; set; }
}

public class Facialhair
{
	public float mustache { get; set; }
	public float beard { get; set; }
	public float sideburns { get; set; }
}

public class Headpose
{
	public float roll { get; set; }
	public int yaw { get; set; }
	public int pitch { get; set; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because I have a free cognitive account and I could be throttled, I created a randomizer to generate random values in case i don't wato to use the cognitive functions for testing. So I created a flag that I can change whenever I want to test without cognitive:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;// Submit to Cognitive
if (IsCognitive)
{
	///same as above
	...
}
else
{
	gender = genders.ElementAt(random.Next(genders.Count - 1));
	age = ages.ElementAt(random.Next(ages.Count - 1));
	smile = smiles.ElementAt(random.Next(smiles.Count - 1));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="powerbi"&gt;PowerBI&lt;/h3&gt;
&lt;p&gt;Once I get the result back from the cognitive function, I create a real time event (and refer to the smile range as score after I multiply it by 10) and send it to &lt;a href="https://powerbi.microsoft.com/en-us/documentation/powerbi-service-real-time-streaming/"&gt;PowerBI real-time&lt;/a&gt; very useful feature which displays visualizations in real-time:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;using (var httpClient = new HttpClient())
{
	var realTimeEvent = new
	{
		time = DateTime.Now,
		age = (int)age,
		score = (int)(smile * 10),
		gender = gender
	};

	var data = new dynamic[1];
	data[0] = realTimeEvent;
	var postData = JsonConvert.SerializeObject(data);
	HttpContent httpContent = new StringContent(postData, Encoding.UTF8, &amp;quot;application/json&amp;quot;);
	HttpResponseMessage response = await httpClient.PostAsync(PowerBIApi, httpContent);
	string responseString = await response.Content.ReadAsStringAsync();

	if (!response.IsSuccessStatusCode)
		throw new Exception(&amp;quot;Unable to post to PowerBI: &amp;quot; + response.StatusCode);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where PowerBIApi is the real-time API that you must post it. You will get this from PowerPI service when you create your own Real-Time dataset.&lt;/p&gt;
&lt;p&gt;This allows people to watch the presentation evaluation result in real-time:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/XvnGaVs.png" class="img-fluid" alt="PowerBI" /&gt;&lt;/p&gt;
&lt;p&gt;That was a nice exercise! I liked the ease of developing stuff in Xamarin forms as it shields me almost completely from Android and iOS. Visual Studio for mac (in preview), however, has a lot of room of improvement...it feels heavy, clunky and a bit buggy. Finally I would like to say that, in non-demo situations, it is probably better to send the picture to an Azure storage which will trigger an Azure Function that will send to cognitive and PowerBI.&lt;/p&gt;
&lt;p&gt;The code is available in GitHub &lt;a href="https://github.com/khaledhikmat/presentation-evaluation"&gt;here&lt;/a&gt;&lt;/p&gt;
</content>
		<summary>&lt;p&gt;I am new to Xamarin development and I also wanted to try the newly announced VS for mac! So I created a little Camera app that can be used to evaluate presentations. The app allows the user to take a &lt;code&gt;selfie&lt;/code&gt;. When committed, the picture is sent to an Azure cognitive function to extract the gender, male and smile (a measure of emotion). The app then displays the taken picture and returned result in the app. It also sends the result to PowerBI real-time stream to allow the visualization of the evaluation results.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2016-09-09-aspnet-core-file-return" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2016-09-09-aspnet-core-file-return</id>
		<title>Return a file in ASP.NET Core from a Web API</title>
		<updated>2016-09-09T00:00:00Z</updated>
		<content>&lt;p&gt;In ASP .NET 4.x, I had this code to return a file from an ASP.NET Web API. This worked well and allowed a client-side JavaScript client to download the file with a progress indicator:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[Route(&amp;quot;api/some/file&amp;quot;, Name = &amp;quot;SomeFile&amp;quot;)]
public async Task&amp;lt;HttpResponseMessage&amp;gt; GetFile()
{
    var error = &amp;quot;&amp;quot;;

    try
    {
		//TODO: Get the file in a string called contentData

        MemoryStream stream = new MemoryStream();
        StreamWriter writer = new StreamWriter(stream);
        writer.Write(contentData);
        writer.Flush();
        stream.Position = 0;

        var result = new HttpResponseMessage(HttpStatusCode.OK)
        {
            Content = new StreamContent(stream)
        };
        result.Content.Headers.ContentType = new MediaTypeHeaderValue(&amp;quot;application/octet-stream&amp;quot;);
        result.Content.Headers.ContentLength = stream.Length;
        result.Content.Headers.ContentDisposition = new ContentDispositionHeaderValue(&amp;quot;attachment&amp;quot;)
        {
            FileName = &amp;quot;content.json&amp;quot;,
            Size = stream.Length
        };

        return result;
    }
    catch (Exception e)
    {
        // The tag = ControllerName.RouteName
        error = e.Message;
		// TODO: do something with the error
        return new HttpResponseMessage(HttpStatusCode.BadRequest);
    }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Recently I created a new ASP.NET Core project for some other purpose which also had a requirement to download a file from a Web API. So naturally I copied the same code over. But that did not work...I end up getting the result in JSON....it looks something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-json"&gt;{
    &amp;quot;version&amp;quot;: {
        &amp;quot;major&amp;quot;: 1,
        &amp;quot;minor&amp;quot;: 1,
        &amp;quot;build&amp;quot;: - 1,
        &amp;quot;revision&amp;quot;: - 1,
        &amp;quot;majorRevision&amp;quot;: - 1,
        &amp;quot;minorRevision&amp;quot;: - 1
    },
    &amp;quot;content&amp;quot;: {
        &amp;quot;headers&amp;quot;: [{
            &amp;quot;key&amp;quot;: &amp;quot;Content-Type&amp;quot;,
            &amp;quot;value&amp;quot;: [&amp;quot;application/octet-stream&amp;quot;]
        }, {
            &amp;quot;key&amp;quot;: &amp;quot;Content-Length&amp;quot;,
            &amp;quot;value&amp;quot;: [&amp;quot;2346262&amp;quot;]
        }, {
            &amp;quot;key&amp;quot;: &amp;quot;Content-Disposition&amp;quot;,
            &amp;quot;value&amp;quot;: [&amp;quot;attachment; filename=content.json; size=2346262&amp;quot;]
        }
        ]
    },
    &amp;quot;statusCode&amp;quot;: 200,
    &amp;quot;reasonPhrase&amp;quot;: &amp;quot;OK&amp;quot;,
    &amp;quot;headers&amp;quot;: [],
    &amp;quot;requestMessage&amp;quot;: null,
    &amp;quot;isSuccessStatusCode&amp;quot;: true
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After several attempts, I eventually I found out that this below code works well in ASP.NET Core and my JavaScript is able to show a download progress bar:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[Route(&amp;quot;api/some/file&amp;quot;, Name = &amp;quot;SomeFile&amp;quot;)]
public async Task&amp;lt;HttpResponseMessage&amp;gt; GetFile()
{
    var error = &amp;quot;&amp;quot;;

    try
    {
		//TODO: Get the file in a string called contentData

	    HttpContext.Response.ContentType = &amp;quot;application/json&amp;quot;;
	    HttpContext.Response.ContentLength = Encoding.ASCII.GetBytes(contentData).Length;
	    HttpContext.Response.Headers[&amp;quot;Content-Disposition&amp;quot;] = new ContentDispositionHeaderValue(&amp;quot;attachment&amp;quot;)
	    {
	        FileName = &amp;quot;content.json&amp;quot;,
	        Size = HttpContext.Response.ContentLength
	    }.ToString();
	    HttpContext.Response.Headers[&amp;quot;Content-Length&amp;quot;] = &amp;quot;&amp;quot; + HttpContext.Response.ContentLength;
	
	    FileContentResult result = new FileContentResult(Encoding.ASCII.GetBytes(contentData), &amp;quot;application/octet-stream&amp;quot;)
	    {
	        FileDownloadName = &amp;quot;content.json&amp;quot;
	    };
	
	    return result;
	}
	catch (Exception e)
	{
	    // TODO: Handle error
	    HttpContext.Response.StatusCode = 400;
		...
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope this tip helps someone!&lt;/p&gt;
</content>
		<summary>&lt;p&gt;In ASP .NET 4.x, I had this code to return a file from an ASP.NET Web API. This worked well and allowed a client-side JavaScript client to download the file with a progress indicator:&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2016-04-30-kicking-powerapps-tires" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2016-04-30-kicking-powerapps-tires</id>
		<title>Kicking PowerApps Tires</title>
		<updated>2016-04-30T00:00:00Z</updated>
		<content>&lt;p&gt;PowerApps is a newly released platform/service to build Line-of-business applications from Microsoft. Reading some documentation and attending some online webcasts, I think the PowerApps product is well positioned for LOB! There is definitely a need to create LOB mobile apps at the enterprise level and distribute them seamlessly without the friction of the app stores.&lt;/p&gt;
&lt;p&gt;The thing is that the last two (at least) platforms that Microsoft created to build LOB applications were eventually abandoned i.e. SilverLight and LightSwitch. Hence there could be some resistance from some developers to start learning this new platform knowing that it might also have the same fate as its predecessors. However, from the first couple of hours that I spent on PowerApps, it seems to be a very capable environment and really easy to do stuff in. So I wanted to show off a very simple app that demonstrates some simple but important capabilities.&lt;/p&gt;
&lt;p&gt;Another thing to note is that although, at the time of writing, the product had just made it to public preview from a gated preview, the &lt;a href="https://powerapps.microsoft.com/en-us/tutorials/getting-started/"&gt;documentation&lt;/a&gt; looks really complete and actually quite good. There also seems to be strong and engaging community!!&lt;/p&gt;
&lt;h3 id="simple-app"&gt;Simple App&lt;/h3&gt;
&lt;p&gt;I am building an app that presents tabular sales data to users and allows them to drill through the hierarchical nature of the data. For example, at the top level, users will see sales data for different regions and then can drill down to see the sales data for individual countries:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/Lz0ydEr.png" class="img-fluid" alt="Hierarchy Sales Data" /&gt;&lt;/p&gt;
&lt;p&gt;So I wanted the ability to navigate to the same screen in PowerApps but with different data. The sample then shows how I solved this particular problem in PowerApps.&lt;/p&gt;
&lt;h3 id="navigation-scheme"&gt;Navigation Scheme&lt;/h3&gt;
&lt;p&gt;I created two screens: the first is an initial page to present the users certain options and allows them to start viewing the sales data and the second one is the sales data screen that will be navigated to and from in order to view the drill through sales data.&lt;/p&gt;
&lt;p&gt;In order to manage the navigation, I created a Stack collection (in PowerApps it is called a collection...but it is like a table) that holds the navigation history of each screen. Upon initial navigation from the initial screen, I clear the collection. When the user drills down, I push (i.e. add) to the collection the screen id of the screen that I just navigated from. When the user drills up, I pop (i.e. remove the last item) from the collection the screen id that I must navigate to. For this app, the only column that I have in this collection is the screen id:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/Ky1EmUL.png" class="img-fluid" alt="Navigation Scheme" /&gt;&lt;/p&gt;
&lt;h3 id="initial-screen"&gt;Initial Screen&lt;/h3&gt;
&lt;p&gt;In PowerApps studio, this is how the initial screen looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/XKmqkp5.png" class="img-fluid" alt="Initial Screen" /&gt;&lt;/p&gt;
&lt;p&gt;So when the user taps the initial drill down icon, the following script is executed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Clear(Stack); Navigate(Screen2, ScreenTransition.Fade, {Screen:{Id: 1}})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script consists of 3 main things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Clear(Stack);&lt;/code&gt; clears the navigation collection that I named &lt;code&gt;Stack&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Navigate(Screen2, ScreenTransition.Fade, {Screen:{Id: 1}})&lt;/code&gt; navigates to screen2 (which is the data screen) with a fade&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{Screen:{Id: 1}}&lt;/code&gt; adds a context variable called &lt;code&gt;Screen&lt;/code&gt; with a single column called &lt;code&gt;Id&lt;/code&gt; that contains the value 1 i.e. Level 1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This &lt;a href="https://powerapps.microsoft.com/en-us/tutorials/function-updatecontext/"&gt;context variable&lt;/a&gt; is short-lived as it is only available within the boundary of a single screen. I use it to pass the screen id from one screen to another.&lt;/p&gt;
&lt;h3 id="data-screen-drill-down"&gt;Data Screen - Drill Down&lt;/h3&gt;
&lt;p&gt;In PowerApps studio, the data screen drill down looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/qnQjFwz.png" class="img-fluid" alt="Data Screen Drill Down" /&gt;&lt;/p&gt;
&lt;p&gt;So when the user taps the drill down icon, the following script is executed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Collect(Stack, {Id: Screen.Id}); Navigate(Screen2, ScreenTransition.Fade, {Screen:{Id: Last(Stack).Id + 1}})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script consists of 3 main things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Collect(Stack, {Id: Screen.Id});&lt;/code&gt; adds (i.e. collects in PowerApps terminology) the screen id that was passed from the initial screen or the screen that I navigated from. The collection stores the screen ID.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Navigate(Screen2, ScreenTransition.Fade, {Screen:{Id: Last(Stack).Id + 1}})&lt;/code&gt; navigates to the same screen (screen2) with a fade&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{Screen:{Id: Last(Stack).Id + 1}}&lt;/code&gt; adds a context variable called &lt;code&gt;Screen&lt;/code&gt; with a single column called &lt;code&gt;Id&lt;/code&gt; that contains the value of the last item in the Stack i.e. &lt;code&gt;Last(Stack).Id&lt;/code&gt; plus one! This what makes the context variable so powerful.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="data-screen-drill-up"&gt;Data Screen - Drill Up&lt;/h3&gt;
&lt;p&gt;In PowerApps studio, the data screen drill up looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/S1buIWP.png" class="img-fluid" alt="Data Screen Drill Up" /&gt;&lt;/p&gt;
&lt;p&gt;So when the user taps the drill up icon, the following script is executed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Navigate(Screen2, ScreenTransition.Fade, {Screen:{Id: Last(Stack).Id}}); Remove(Stack, Last( Stack))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script consists of 3 main things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Navigate(Screen2, ScreenTransition.Fade, {Screen:{Id: Last(Stack).Id}});&lt;/code&gt; navigates to the same screen (screen2) with a fade&lt;/li&gt;
&lt;li&gt;&lt;code&gt;{Screen:{Id: Last(Stack).Id}}&lt;/code&gt; adds a context variable called &lt;code&gt;Screen&lt;/code&gt; with a single column called &lt;code&gt;Id&lt;/code&gt; that contains the value of the last item in the Stack i.e. &lt;code&gt;Last(Stack).Id&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Remove(Stack, Last( Stack))&lt;/code&gt; removes the last item from the Stack! Effectively we are doing a pop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to prevent un-supported drill ups, the drill up icon has a visibility property that is controlled by the following script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;If (CountRows(Stack) &amp;gt; 0, true)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So as long as there are items in the Stack collection, the drill-up icon is visible.&lt;/p&gt;
&lt;p&gt;I hope this is a helpful short post to show how powerful and useful PowerApps can be. I am hoping to be able to add more posts about PowerApps in future posts.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;PowerApps is a newly released platform/service to build Line-of-business applications from Microsoft. Reading some documentation and attending some online webcasts, I think the PowerApps product is well positioned for LOB! There is definitely a need to create LOB mobile apps at the enterprise level and distribute them seamlessly without the friction of the app stores.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2016-04-25-open-azure-vm-port" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2016-04-25-open-azure-vm-port</id>
		<title>Open Azure VM Port</title>
		<updated>2016-04-25T00:00:00Z</updated>
		<content>&lt;p&gt;For a project I was working on, I needed to create a Windows VS2015 VM for testing. It is quit easy to spawn a VM in Azure ...it only takes a couple of seconds to do it from the portal. The next task was to open up port 8080 on that machine as I needed to access that port for testing.&lt;/p&gt;
&lt;h3 id="windows-server-2012"&gt;Windows Server 2012&lt;/h3&gt;
&lt;p&gt;Since the VM is a Windows Server 2012, all I needed to do is to go the server's Server Manager =&amp;gt; Local Server and access the Windows Firewall. At the firewall, I access the advanced setting to add a new inbound rule for protocol type TCP and local port is 8080:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/gqnnSyV.png" class="img-fluid" alt="Inbound Rule" /&gt;&lt;/p&gt;
&lt;h3 id="azure-endpoints"&gt;Azure Endpoints&lt;/h3&gt;
&lt;p&gt;The above step is not enough to expose port 8080! What we also need is to let the VM's Network Security Group about this new endpoint that we want to allow. To do that, you also need to locate the VM's Resource Group. The new ARM-based Azure VMs have several things in the resource group:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Virtual Machine&lt;/li&gt;
&lt;li&gt;Network Interface&lt;/li&gt;
&lt;li&gt;Network Security Group&lt;/li&gt;
&lt;li&gt;Public IP Address&lt;/li&gt;
&lt;li&gt;Virtual Network&lt;/li&gt;
&lt;li&gt;Storage Account&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We access the Network Security Group:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/VIJyh6U.png" class="img-fluid" alt="Network Security Group" /&gt;&lt;/p&gt;
&lt;p&gt;and add the Inbound Rule for port 8080:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/Pi32vAf.png" class="img-fluid" alt="NSG Inbound Rule" /&gt;&lt;/p&gt;
&lt;p&gt;This will allow us to access port 8080 in the VM.&lt;/p&gt;
&lt;p&gt;Please note that the instructions above are for the Azure ARM-based VMs...not the classic ones.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;For a project I was working on, I needed to create a Windows VS2015 VM for testing. It is quit easy to spawn a VM in Azure ...it only takes a couple of seconds to do it from the portal. The next task was to open up port 8080 on that machine as I needed to access that port for testing.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2016-02-19-asp-net-api-versioning" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2016-02-19-asp-net-api-versioning</id>
		<title>ASP.NET API Versioning</title>
		<updated>2016-02-19T00:00:00Z</updated>
		<content>&lt;p&gt;A while back, I created an ASP.NET Web API 2 to be a back-end for a mobile app. I used basic authentication to make it easier for mobile apps to consume the Web API. I now decided to provide better security so I wanted to move to a token-based authentication. The problem is that if I change the Web API to a token-based, all existing mobile apps in the field will not function as they will be refused Web API connection.&lt;/p&gt;
&lt;p&gt;The answer is to use Web API versioning! This way existing mobile users can continue to use the current version that uses basic authentication until the app is upgraded. The updated app version will switch over to use the new version which is based on token authentication. This post will discuss how I accomplished this versioning scheme.&lt;/p&gt;
&lt;h2 id="controller-selector"&gt;Controller Selector&lt;/h2&gt;
&lt;p&gt;The first step is to configure the ASP.NET framework to use a custom controller selector. In the &lt;code&gt;WebApiConfig&lt;/code&gt; &lt;code&gt;Register&lt;/code&gt; method, we tell the framework to use the custom selector:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;config.Services.Replace(typeof(IHttpControllerSelector), new VersionAwareControllerSelector(config));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The selector is coded as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class VersionAwareControllerSelector : DefaultHttpControllerSelector
{
    private const string VERSION_HEADER_NAME = &amp;quot;some-value&amp;quot;;
    private const string VERSION_QUERY_NAME = &amp;quot;v&amp;quot;;

    private HttpConfiguration _configuration;

    public VersionAwareControllerSelector(HttpConfiguration configuration)
        : base(configuration)
    {
        _configuration = configuration;
    }

    // This works for Web API 2 and Attributed Routing
    // FROM: http://stackoverflow.com/questions/19835015/versioning-asp-net-web-api-2-with-media-types/19882371#19882371
    // BLOG: http://webstackoflove.com/asp-net-web-api-versioning-with-media-types/
    public override HttpControllerDescriptor SelectController(HttpRequestMessage request)
    {
        HttpControllerDescriptor controllerDescriptor = null;

        // Get a list of all controllers provided by the default selector
        IDictionary&amp;lt;string, HttpControllerDescriptor&amp;gt; controllers = GetControllerMapping();

        IHttpRouteData routeData = request.GetRouteData();

        if (routeData == null)
        {
            throw new HttpResponseException(HttpStatusCode.NotFound);
        }

        // Pick up the API Version from the header...but we could also do query string
        var apiVersion = GetVersionFromHeader(request);

        // Check if this route is actually an attribute route
        IEnumerable&amp;lt;IHttpRouteData&amp;gt; attributeSubRoutes = routeData.GetSubRoutes();

        if (attributeSubRoutes == null)
        {
            string controllerName = GetRouteVariable&amp;lt;string&amp;gt;(routeData, &amp;quot;controller&amp;quot;);
            if (controllerName == null)
            {
                throw new HttpResponseException(HttpStatusCode.NotFound);
            }

            string newControllerName = String.Concat(controllerName, apiVersion);

            if (controllers.TryGetValue(newControllerName, out controllerDescriptor))
            {
                return controllerDescriptor;
            }
            else
            {
                throw new HttpResponseException(HttpStatusCode.NotFound);
            }
        }
        else
        {
            string newControllerNameSuffix = String.Concat(&amp;quot;V&amp;quot;, apiVersion); ;

            IEnumerable&amp;lt;IHttpRouteData&amp;gt; filteredSubRoutes = attributeSubRoutes.Where(attrRouteData =&amp;gt;
            {
                HttpControllerDescriptor currentDescriptor = GetControllerDescriptor(attrRouteData);
                bool match = currentDescriptor.ControllerName.EndsWith(newControllerNameSuffix);

                if (match &amp;amp;&amp;amp; (controllerDescriptor == null))
                {
                    controllerDescriptor = currentDescriptor;
                }

                return match;
            });

            routeData.Values[&amp;quot;MS_SubRoutes&amp;quot;] = filteredSubRoutes.ToArray();
        }

        return controllerDescriptor;
    }

    private HttpControllerDescriptor GetControllerDescriptor(IHttpRouteData routeData)
    {
        return ((HttpActionDescriptor[])routeData.Route.DataTokens[&amp;quot;actions&amp;quot;]).First().ControllerDescriptor;
    }

    // Get a value from the route data, if present.
    private static T GetRouteVariable&amp;lt;T&amp;gt;(IHttpRouteData routeData, string name)
    {
        object result = null;
        if (routeData.Values.TryGetValue(name, out result))
        {
            return (T)result;
        }
        return default(T);
    }
    

    private string GetVersionFromHeader(HttpRequestMessage request)
    {
        if (request.Headers.Contains(VERSION_HEADER_NAME))
        {
            var header = request.Headers.GetValues(VERSION_HEADER_NAME).FirstOrDefault();
            if (header != null)
            {
                return header;
            }
        }

        return &amp;quot;1&amp;quot;;
    }

    private string GetVersionFromQueryString(HttpRequestMessage request)
    {
        var query = HttpUtility.ParseQueryString(request.RequestUri.Query);

        var version = query[VERSION_QUERY_NAME];
        if (version != null)
        {
            return version;
        }

        return &amp;quot;1&amp;quot;;
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As demonstarted above, I chose to send the version information as a header value! There are other options ...one of them is to pass it in the query string such as &lt;code&gt;http://example.com/api/stats?v=2&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="controller-versions"&gt;Controller Versions&lt;/h2&gt;
&lt;p&gt;The above allows us to have versioned controllers with the following naming convention:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/5jxk3S5.png" class="img-fluid" alt="Controller Versions" /&gt;&lt;/p&gt;
&lt;p&gt;The framework will pick version 1 (i.e. &lt;code&gt;StatsV1Controller&lt;/code&gt;) by default unless the request's header contains a version header value. If the value is 2, then &lt;code&gt;StatsV2Controller&lt;/code&gt; will be picked.&lt;/p&gt;
&lt;p&gt;The V1 controller is defined this way:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[BasicAuthorize()]
public class StatsV1Controller : ApiController
{
    [Route(&amp;quot;api/stats&amp;quot;, Name = &amp;quot;Stats&amp;quot;)]
    public virtual IHttpActionResult GetStats()
    {
		....
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;while the V2 controller is defined this way:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[TokenAuthorize()]
public class StatsV2Controller : StatsV1Controller
{
    [Route(&amp;quot;api/stats&amp;quot;, Name = &amp;quot;StatsV2&amp;quot;)]
    public override IHttpActionResult GetStats()
    {
		return base.GetStats();
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;The V1 controller uses basic authorization (as decorated by the attribute on top of the controller class) and V2 uses token authentication as decorated.&lt;/li&gt;
&lt;li&gt;The V2 controller inherits from the V1 controller so there is no need to re-implement the methods.&lt;/li&gt;
&lt;li&gt;However, there is a need to supply a different route name for the V2 controller otherwise we will get a conflict. This is done by giving the V2 controller a route name that ends with V1 i.e. StatsV2. This is a little unfortunate but this is how it is. Had it not been for this, we could have simply inherited from V1 without having to repeat any method.&lt;/li&gt;
&lt;li&gt;Since V2 inherits from V1, I noticed that both authentication filters run per request. This means that when V2 is picked, the token authorize filer will run first and then followed by the basic authorize filter. This can cause problems. So what I did is at the end of the token authorize filter, I inject a value in the request properties. In the basic authorize filter, I check if the value exists, and, it if it does, I abort the basic authorize filter since the token filter has already run.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="request-property"&gt;Request Property&lt;/h2&gt;
&lt;p&gt;Here is one way to inject a property in the request in the token filter:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;actionContext.Request.Properties[&amp;quot;some-key&amp;quot;] = &amp;quot;some-value&amp;quot;;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then in the basic filter, I check for this property existence. If it does exist, it means the request is authenticated and there is no need to perform basic authentication.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;string accessToken;
if (!actionContext.Request.Properties.TryGetValue(&amp;quot;sone-key&amp;quot;, out accessToken))            
{
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I hope someone finds this post helpful. Having multiple versions has provided me with a way to transition my mobile app users from basic authentication to token authentication without breaking the existing mobile apps.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;A while back, I created an ASP.NET Web API 2 to be a back-end for a mobile app. I used basic authentication to make it easier for mobile apps to consume the Web API. I now decided to provide better security so I wanted to move to a token-based authentication. The problem is that if I change the Web API to a token-based, all existing mobile apps in the field will not function as they will be refused Web API connection.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2015-11-15-azure-logicapps" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2015-11-15-azure-logicapps</id>
		<title>Azure Logic Apps Push Trigger</title>
		<updated>2015-11-15T00:00:00Z</updated>
		<content>&lt;p&gt;I have been toying with Azure Logic Apps for a project I am working on. I wanted to run a Cloud-based work flow triggered by some events within some web app. This post discusses things I had to to go through to get push triggers working as triggered from web events.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Please refer to Nicholas Hauenstein's Azure session referenced below in the reference section for a great introduction&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="use-case"&gt;Use Case&lt;/h2&gt;
&lt;p&gt;As part of a loyalty program system, membership voucher redemption requests arrive to a server via software agents installed at different hotels. Here is how the hotel POS Agent communicates with our Web API:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/xquyGYG.png" class="img-fluid" alt="POS Architecture" /&gt;&lt;/p&gt;
&lt;p&gt;The Web API validates, processes the validation requests and updates a legacy database. It has been requested to enhance this functionality to add the following things when a voucher redemption takes place:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Notify the member via SMS that a voucher has been successfully redeemed at certain hotel&lt;/li&gt;
&lt;li&gt;Notify the member via push notification that a voucher has been redeemed&lt;/li&gt;
&lt;li&gt;Notify customer support via Email that member so and so has successfully redeemed a voucher at certain hotel&lt;/li&gt;
&lt;li&gt;Register an event with the analytic server&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Instead of building all the functionality in the Web API layer, we decided to trigger an Azure Logic app work flow that handles all these events in a Cloud service.&lt;/p&gt;
&lt;h2 id="push-triggers"&gt;Push Triggers&lt;/h2&gt;
&lt;p&gt;Logic Apps can be triggered using many different mechanisms. In our use case, we would like to use a push trigger so that the Web API can trigger the Logic App whenever a voucher request is successfully processed.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/VzqsehB.png" class="img-fluid" alt="Trigger Architecture" /&gt;&lt;/p&gt;
&lt;p&gt;But how does the Web API know how to trigger the Logic App? It turned out that there is a need to create a special purpose API App whose main job is to receive callback registrations from the Logic apps so that when a service or a piece of software needs to trigger the logic app, it would look up the callback URL and trigger the logic app.&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/bu9XEze.png" class="img-fluid" alt="Callback Architecture" /&gt;&lt;/p&gt;
&lt;p&gt;Ok....great...but more questions come to mind. How does the logic app register its callback URL with the the API App? It turned out that the minute you drop a push trigger API App in the Logic App overflow (via the designer or the JSON template), the logic app would register its callback URL by putting (using HTTP &lt;code&gt;PUT&lt;/code&gt; verb) against the API App. This happens the second you save the work flow and also every hour (just to make sure that API App is aware of the callback URL).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This is one thing that took me a while to figure out. In order to make sure that the logic app re-registers the callback, you need to remove the push trigger API from the work flow, save and then put it back and re-save.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;What does the API App have to do to be considered as a push trigger API App? Well...it has to support at least one PUT verb with something similar to this signature:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;[HttpPut]
[Metadata(&amp;quot;Mosaic Voucher Redemption Trigger&amp;quot;)]
[Trigger(TriggerType.Push, typeof(VoucherRedemptionPushTriggerOutput))]
[Route(&amp;quot;api/mosaic/voucherredemption/callbacks/{triggerId}&amp;quot;, Name = &amp;quot;MosaicVoucherRedemptionTriggerCallback&amp;quot;)]
public HttpResponseMessage RegisterMosaicVoucherRedemptionCallback(
    string triggerId,
    [FromBody]TriggerInput&amp;lt;VoucherRedemptionPushTriggerConfiguration, VoucherRedemptionPushTriggerOutput&amp;gt; parameters)
{
    try
    {
        // TODO: Add things here
        
        // Report back to the logic app that everything is happy
        return Request.PushTriggerRegistered(parameters.GetCallback());
    }
    catch (Exception e)
    {
        return Request.CreateErrorResponse(HttpStatusCode.BadRequest, e.Message);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An API App can contain multiple PUT signatures and hence it will be able to support multiple trigger callback registration endpoints.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;triggerId&lt;/code&gt; is usually the name of the Logic App. This allows the API App to serve multiple Logic Apps using multiple trigger ids. The &lt;code&gt;VoucherRedemptionPushTriggerConfiguration&lt;/code&gt; defines the API App configuration model and finally the &lt;code&gt;VoucherRedemptionPushTriggerOutput&lt;/code&gt; is the API app output model (i.e. input to the Logic app). In our case, here is how the configuration is defined:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class VoucherRedemptionPushTriggerConfiguration
{
    [Metadata(&amp;quot;Web API Url&amp;quot;)]
    public string Url { get; set; }

    [Metadata(&amp;quot;Web API User Id&amp;quot;)]
    public string UserId { get; set; }

    [Metadata(&amp;quot;Web API Password&amp;quot;)]
    public string Password { get; set; }

    [Metadata(&amp;quot;Is Restricted?&amp;quot;)]
    public bool IsRestricted { get; set; }

    [Metadata(&amp;quot;Permitted agent codes (comma delimited)&amp;quot;)]
    public string PermittedAgentCodes { get; set; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above shows the configuration properties that are available for the push trigger API App to control the workflow.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public class VoucherRedemptionPushTriggerOutput
{
    [Metadata(&amp;quot;Activation Code&amp;quot;)]
    public string ActivationCode { get; set; }

    [Metadata(&amp;quot;Voucher Code&amp;quot;)]
    public string VoucherCode { get; set; }

    [Metadata(&amp;quot;Card Number&amp;quot;)]
    public string CardNumber { get; set; }

    [Metadata(&amp;quot;Transaction Date&amp;quot;)]
    public DateTime TransactionDate { get; set; }

    [Metadata(&amp;quot;Hotel&amp;quot;)]
    public string Hotel { get; set; }

    [Metadata(&amp;quot;Outlet&amp;quot;)]
    public string Outlet { get; set; }

    [Metadata(&amp;quot;City&amp;quot;)]
    public string City { get; set; }

    [Metadata(&amp;quot;Country&amp;quot;)]
    public string Country { get; set; }

    [Metadata(&amp;quot;First Name&amp;quot;)]
    public string FirstName { get; set; }

    [Metadata(&amp;quot;Last Name&amp;quot;)]
    public string LastName { get; set; }

    [Metadata(&amp;quot;Email&amp;quot;)]
    public string Email { get; set; }

    [Metadata(&amp;quot;Mobile Number&amp;quot;)]
    public string MobileNumber { get; set; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above shows the API App output properties (which is an input to the logic app).&lt;/p&gt;
&lt;p&gt;So in summary, the logic app calls the PUT method of the Api APP and provides configuration structure (based on &lt;code&gt;VoucherRedemptionPushTriggerConfiguration&lt;/code&gt;) and then expects to receive a call back from the trigger source providing data that matches the &lt;code&gt;VoucherRedemptionPushTriggerOutput&lt;/code&gt;. The API App is not the trigger source...in our case, the trigger source is the Web API. Hence whenever the API App receives a callback registration from the a logic app, it posts the callback info to the Web API via an endpoint and return to the Logic app. In this case, the API App is nothing but a middle man that receives a callback info and knows how to distribute it to its intended recipients. This is because the logic app cannot communicate directly to the Web API.&lt;/p&gt;
&lt;p&gt;Of course, the API App may send the callback info to multiple trigger sources. All of them will become eligible to trigger the logic app.&lt;/p&gt;
&lt;p&gt;Here is an illustration of the trigger phases:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/TM0MHgZ.png" class="img-fluid" alt="Trigger Phases" /&gt;&lt;/p&gt;
&lt;p&gt;Please note that during the trigger phase, the API App is no longer involved. The trigger source reads the callback info from its database and calls upon the logic app  without any involvement from the API App.&lt;/p&gt;
&lt;h2 id="logic-app"&gt;Logic App&lt;/h2&gt;
&lt;p&gt;Finally, you must deploy the API App to the same resource group in which the logic app resides in. This will make the API App available as a selection in the available connectors:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/ufzzZQN.png" class="img-fluid" alt="Available Connectors" /&gt;&lt;/p&gt;
&lt;p&gt;Using the Azure portal editor, you simply drop in the trigger API App and choose one of the triggers available in the API App (as mentioned, one API App may support multiple triggers). You fill out the configuration information:&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/vP76X5O.png" class="img-fluid" alt="Trigger Configuration Info" /&gt;&lt;/p&gt;
&lt;p&gt;and see the expected input from the trigger source (this will you to use the Logic App expression language to read the available properties):&lt;/p&gt;
&lt;p&gt;&lt;img src="http://i.imgur.com/5ACczU2.png" class="img-fluid" alt="Trigger Input" /&gt;&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;There are many many scenarios where Logic Apps can be extremely useful. I recommend watching Nicholas Hauenstein presentation (below) to see a great use case for logic apps.&lt;/p&gt;
&lt;p&gt;Some people may try to compare IFTTT (If this, then that) or Zapier with Logic Apps! They are substantially different. IFTTT and Zapier are meant for individuals creating cloud-based (two-step-only) scenarios to handle their day-to-day workflows. Logic Apps on the other hand are general-purpose Cloud-based work flows that have a sophisticated definition and work flow languages.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Nicholas Hauenstein - 2015 Azure Con session on [https://azure.microsoft.com/en-us/documentation/videos/azurecon-2015-processing-nfc-tag-reads-in-an-azure-app-service-logic-app/](Processing NFC tag reads in an Azure APP service service)&lt;/li&gt;
&lt;/ul&gt;
</content>
		<summary>&lt;p&gt;I have been toying with Azure Logic Apps for a project I am working on. I wanted to run a Cloud-based work flow triggered by some events within some web app. This post discusses things I had to to go through to get push triggers working as triggered from web events.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2015-09-26-github-platform" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2015-09-26-github-platform</id>
		<title>GitHub Platform</title>
		<updated>2015-09-26T00:00:00Z</updated>
		<content>&lt;p&gt;We have most of our source code and documentation on GitHub. They are organized as organizations with private repositories. While the privacy is really needed, our executives are not able to see what is going on unless they sign in to GitHub, hop from one repository to another....something they are not particularly happy about. So the following are my objectives to change that and provide our executives a much easier view:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide some sort of analytics to show who is doing what and when to the source code and documentation repositories&lt;/li&gt;
&lt;li&gt;Extract the source code and documentation repositories, status, last update from GitHub using APIs&lt;/li&gt;
&lt;li&gt;Expose some of our non-sensitive documentation as HTML or PDF&lt;/li&gt;
&lt;li&gt;Provide all of the above on a site available publicly so it will be really convenient to view&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The rest of the post will describe in details how I managed the objectives as outlined above.&lt;/p&gt;
&lt;h3 id="analytics"&gt;Analytics&lt;/h3&gt;
&lt;p&gt;In order to see what is going on with the GitHub repositories, I needed a way to have GitHub let me know whatis going whenever an event takes place. This is usualy called Webhooks. Sure enough GitHub has great Webhooks at the repository level and at the organization level. I am interested in the organization level because it gives what I need without having to set up a Webhook for each repository (we have about 75).&lt;/p&gt;
&lt;p&gt;As I was reading the GitHub documentation and searching, I stumbled across a very nice ASP.NET library that does exactly what I needed. The ASP.NET team tools division announced the &lt;a href="http://blogs.msdn.com/b/webdev/archive/2015/09/04/introducing-microsoft-asp-net-webhooks-preview.aspx"&gt;ASP.NET WebHooks preview&lt;/a&gt;. This is what they said about it:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;WebHooks is a lightweight HTTP pattern providing a simple pub/sub model for wiring together Web APIs and SaaS services. When an event happens in a service, a notification is sent in the form of an HTTP POST request to registered subscribers. The POST request contains information about the event which makes it possible for the receiver to act accordingly.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The best part is they already have GitHub support in the box and the example they showed on their blog is actually for GitHub Webhooks. In literally a few minutes I had GitHub report to me the push events that are happening against our organization repositories.&lt;/p&gt;
&lt;p&gt;Essentially the ASP.NET Webhook solution is quite simple. You create a handler to receive the Webhook events and they handle everything else including security, choreography and anything else in between. Basically the code boils down to this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;public override Task ExecuteAsync(string receiver, WebHookHandlerContext context)
{
	try
	{
		string action = context.Actions.First();
		JObject data = context.GetDataOrDefault&amp;lt;JObject&amp;gt;();

		// The action should contain the event name
		// The data should contain the actual notification

		// We are only watching the 'push' event from GitHub...so there is no need to do anything
		// Catsing to a dynamic object makes it really aeasy to access the JSON structure
		dynamic json = data;
		
		//TODO: Do something with the data

	}
	catch (Exception e)
	{
		GetLoggerService().Log(LogLevels.Debug, &amp;quot;GitHubHandler&amp;quot;, &amp;quot;WebHook.ExecuteAsync caused an exception: &amp;quot; + e.Message);
	}

	return Task.FromResult(true);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that I am being notified whenever a push event takes place, I wanted to do something with this data to make it reportable. After some research, I found a really cool analytics company that allows me to send events and run some analytics queries against them. This is realy exactly what I needed. So I signed up to &lt;a href="https://keen.io"&gt;Keen&lt;/a&gt;, set up a project and started pumping my push events to Keen. This is what Keen says about themselves:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Deliver fast, flexible analytics to your teams &amp;amp; customers. With Keen’s developer-friendly APIs, it’s easy to embed custom dashboards and reports in any app or website.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It is really quite simple to pump data as they have a .NET library that does most of the work. Querying the data turned out to be quite simple also. The great thing about their solution is that you can embed charts right into web sites via a very lightweight JavaScript library. They also have an explorer that one can experiment to fine tune the queries. So I can produce something like this in no time:&lt;/p&gt;
&lt;p&gt;![Distribution by committer]({{ site.baseurl }}/images/2015-09-26/hmccode-analysis.png)&lt;/p&gt;
&lt;p&gt;Ah....looks great and it is exactly what I want. Of course, the GitHub push events that arrive at my handler contain a plothora of other useful information that I can use to chart and report on. But this is a great start.&lt;/p&gt;
&lt;h3 id="extraction"&gt;Extraction&lt;/h3&gt;
&lt;p&gt;Because our repositories are mostly private, I needed a way to collect the repository information from GitHub using their APIs. I opted to use an Azure Web Job that is triggered every day or so and update a JSON file that I host on a blob storage. This JSON file contains all the repository and organization information.&lt;/p&gt;
&lt;p&gt;In order to interact with GitHub APIs, I used the excellent &lt;a href="https://github.com/octokit/octokit.net"&gt;Octokit .NET libray&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The following is a code snippet that allows me to pull the user's organizations and repsitories:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;var githubclient = new GitHubClient(new Octokit.ProductHeaderValue(&amp;quot;some-cool-name&amp;quot;));
githubclient.Credentials = new Credentials(personalCode);

var user = await githubclient.User.Current();
_logger.WriteLine(&amp;quot;Logged in user : &amp;quot; + user.Login);

var orgs = await githubclient.Organization.GetAllForCurrent();
foreach (var org in orgs)
{
	_logger.WriteLine(&amp;quot;Organization - URL: &amp;quot; + org.Url + &amp;quot; - Login: &amp;quot; + org.Login);

	var orgObject = await githubclient.Organization.Get(org.Login);

	var reps = await githubclient.Repository.GetAllForOrg(org.Login);
	foreach (var rep in reps)
	{
		_logger.WriteLine(&amp;quot;Repository : &amp;quot; + rep.Name + &amp;quot;-&amp;quot; + rep.FullName + &amp;quot; - Collaborators: &amp;quot; + rep.StargazersCount);
	}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;personalCode&lt;/code&gt; is something I generated from my GitHub account setting to allow me to have a programmtic access without having to do the oAUth authentication flow dance. You can read about them &lt;a href="https://github.com/blog/1509-personal-api-tokens"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I collect the organization and repository structure into a .NET structure that looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;class MyGitHub
{
	public MyGitHub()
	{
		Organizations = new List&amp;lt;MyOrganization&amp;gt;();
	}

	public List&amp;lt;MyOrganization&amp;gt; Organizations { get; set; }
}

class MyOrganization
{
	public &amp;lt;MyOrganization()
	{
		Repositories = new List&amp;lt;Repository&amp;gt;();
	}

	public Organization Organization { get; set; }
	public List&amp;lt;Repository&amp;gt; Repositories { get; set; }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;Organization&lt;/code&gt; and &lt;code&gt;Repository&lt;/code&gt; are classes defined by the &lt;a href="https://github.com/octokit/octokit.net"&gt;Octokit .NET libray&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once I collect all the data, I then use Json.NET to serialize the object in JSON:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-charp"&gt;string formattedJson = JsonConvert.SerializeObject(myGitHub, Formatting.Indented, new JsonSerializerSettings
{
	ContractResolver = new CamelCasePropertyNamesContractResolver()
});
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I send the serialized JSON to a blob storage:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;CloudBlockBlob blob = _myBlobContainer.GetBlockBlobReference(blobName);
blob.Properties.ContentType = &amp;quot;application/json; charset=utf-8&amp;quot;;

byte[] byteArray = Encoding.UTF8.GetBytes(formattedJson);
MemoryStream memStream = new MemoryStream(byteArray);
blob.UploadFromStream(memStream);
return blob.Uri.ToString()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I have a URL that I can use from JavaScript, for example, to bind the views to. One more thing though...if you are planning to consume the JSON file from a browser JavaScript, you need to set the storage to honor CORS. One way of doing this programatically using the .NET storage SDK is something like &lt;a href="http://blogs.msdn.com/b/windowsazurestorage/archive/2014/02/03/windows-azure-storage-introducing-cors.aspx"&gt;this&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;// Get context object for working with blobs, and 
// set a default retry policy appropriate for a web user interface.
var blobClient = myStorageAccount.CreateCloudBlobClient();

// CORS should be enabled once at service startup
// Given a BlobClient, download the current Service Properties 
ServiceProperties blobServiceProperties = blobClient.GetServiceProperties();

// Enable and Configure CORS
ConfigureCors(blobServiceProperties);

// Commit the CORS changes into the Service Properties
blobClient.SetServiceProperties(blobServiceProperties);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where enabling &lt;code&gt;ConfigureCors&lt;/code&gt; is defined as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-csharp"&gt;private static void ConfigureCors(ServiceProperties serviceProperties)
{
	serviceProperties.Cors = new CorsProperties();
	serviceProperties.Cors.CorsRules.Add(new CorsRule()
	{
		AllowedHeaders = new List&amp;lt;string&amp;gt;() { &amp;quot;*&amp;quot; },
		AllowedMethods = CorsHttpMethods.Put | CorsHttpMethods.Get | CorsHttpMethods.Head | CorsHttpMethods.Post,
		AllowedOrigins = new List&amp;lt;string&amp;gt;() { &amp;quot;*&amp;quot; },
		ExposedHeaders = new List&amp;lt;string&amp;gt;() { &amp;quot;*&amp;quot; },
		MaxAgeInSeconds = 1800 // 30 minutes
	});
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="exports"&gt;Exports&lt;/h3&gt;
&lt;p&gt;Some of our documenation may need to be made available to the outside world. Perhaps we want to let a client or an internal user access to it. Again...because our documentation repositories are private, we needed a way to export to HTML and perhaps embed in some web site pages.&lt;/p&gt;
&lt;p&gt;To do this, I create in each documentation repository two destination directories: &lt;code&gt;dest-html&lt;/code&gt; and &lt;code&gt;dest-pdf&lt;/code&gt; that are ignored by &lt;code&gt;git&lt;/code&gt; (i.e. via &lt;code&gt;.gitignore&lt;/code&gt;). I also provide two gulp tasks that will walk though all the .MD files and convert them to HTMLs and PDFs respectively.&lt;/p&gt;
&lt;p&gt;To do this, we need to have &lt;a href="https://nodejs.org/en/"&gt;Node&lt;/a&gt; and &lt;a href="http://gulpjs.com/"&gt;Gulp&lt;/a&gt; installed. Here are local commands to run on the root of the web site:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;npm install gulp --save-dev
npm install gulp-markdown --save-dev
npm install gulp-markdown-pdf --save-dev 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once completed, we can now run a gulp task that creates the HTML or PDF files (following the same directory structure) in the said &lt;code&gt;dest&lt;/code&gt; folder:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gulp html
gulp pdf
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;.gulpfile&lt;/code&gt; might look something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;var gulp = require('gulp');
var markdownpdf = require('gulp-markdown-pdf');
var markdownhtml = require('gulp-markdown');

gulp.task('pdf', function () {
        return gulp.src(['**/*.md', '!node_modules/**/*.md'])
        .pipe(markdownpdf())
        .pipe(gulp.dest('dest-pdf'));
});

gulp.task('html', function () {
        return gulp.src(['**/*.md', '!node_modules/**/*.md'])
        .pipe(markdownhtml())
        .pipe(gulp.dest('dest-html'));
});
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="web-site"&gt;Web Site&lt;/h3&gt;
&lt;p&gt;Now that we have everthing we want to expose, I needed a web site to host all of this stuff. It turned out that GitHub provides one public site per repository or per one organization. These web sites are hosted by GitHub and they are referred to as &lt;a href="https://pages.github.com/"&gt;GitHub pages&lt;/a&gt; and they powered by &lt;a href="http://jekyllrb.com/"&gt;Jekyll&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I did not want to start making GitHub API calls directly from JavaScript as there are some security concerns. So the web site reads directly from the JSON file (that O prepared above) and bind the data to the views using any JavaScript binding library such as &lt;a href="http://knockoutjs.com/"&gt;Knockout&lt;/a&gt; or full JavaScript framework such &lt;a href="https://angularjs.org/"&gt;AngularJS&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The site might contain:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Organization information&lt;/li&gt;
&lt;li&gt;Repository Information showing stats, etc&lt;/li&gt;
&lt;li&gt;Selected Documentation&lt;/li&gt;
&lt;li&gt;Blog Posts&lt;/li&gt;
&lt;li&gt;Analytic Charts&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;GitHub provides an excellent platform for source code and documentation. With a little bit of help from their Webhooks and Web APIs, we can build a useful public site that can be used by executives to monitor progress of our IT efforts without having to log in to GitHub.&lt;/p&gt;
</content>
		<summary>&lt;p&gt;We have most of our source code and documentation on GitHub. They are organized as organizations with private repositories. While the privacy is really needed, our executives are not able to see what is going on unless they sign in to GitHub, hop from one repository to another....something they are not particularly happy about. So the following are my objectives to change that and provide our executives a much easier view:&lt;/p&gt;</summary>
	</entry>
	<entry>
		<link href="http://khaledhikmat.github.io/posts/2014-07-07-generic-local-storage-win8-apps" />
		<link rel="enclosure" type="image" href="http://khaledhikmat.github.io/images/liquid.jpg" />
		<id>http://khaledhikmat.github.io/posts/2014-07-07-generic-local-storage-win8-apps</id>
		<title>Generic Local Storage Class for Win8 Apps</title>
		<updated>2014-07-07T00:00:00Z</updated>
		<content>&lt;p&gt;A class in C# that can be used to store and retrieve complex objects to/from Win8 local storage. The class requires JSON.NET Nuget library and of course it relies on the Win8 StorageFile class (hence the Windows.Storage namespace).&lt;/p&gt;
&lt;h2 id="the-local-storage-interface"&gt;The local storage interface&lt;/h2&gt;
&lt;p&gt;Assuming you have a complex object represented here for simplicity by a Book. But it can be anything:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;	public class Book
	{
        public string Title { get; set; }
        public string Author { get; set; }
	}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us create an interface for storing and retrieving a book from the local repository:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    public interface ILocalStorageService
    {
        Task&amp;lt;Book&amp;gt; RetrieveBook();
        Task&amp;lt;bool&amp;gt; StoreBook(Book book);
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="the-local-storage-implementation"&gt;The local storage implementation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;	public class LocalStorageService : ILocalStorageService
    {
        const string BookFileName = &amp;quot;localBook.json&amp;quot;;

        public async Task&amp;lt;Book&amp;gt; RetrieveBook()
        {
            return await RetrieveObject&amp;lt;Book&amp;gt;(BookFileName);
        }

        public async Task&amp;lt;bool&amp;gt; StoreBook(Book book)
        {
            return await StoreObject&amp;lt;Book&amp;gt;(book, BookFileName);
        }

        /*** PRIVATE METHODS */
        private async Task&amp;lt;T&amp;gt; RetrieveObject&amp;lt;T&amp;gt;(string storageFileName)
        {
            StorageFile storageFile;

            try
            {
                storageFile = await ApplicationData.Current.LocalFolder.GetFileAsync(storageFileName);
            }
            catch (FileNotFoundException ex)
            {
                storageFile = null;
            }

            if (storageFile != null)
            {
                string data = await FileIO.ReadTextAsync(storageFile);
                return FromJson&amp;lt;T&amp;gt;(data);
            }

            return default(T);
        }

        private async Task&amp;lt;bool&amp;gt; StoreObject&amp;lt;T&amp;gt;(T requestObject, string storageFileName)
        {
            try
            {
                var data = ToJson&amp;lt;T&amp;gt;(requestObject);
                StorageFile localFile = await ApplicationData.Current.LocalFolder.CreateFileAsync(storageFileName, CreationCollisionOption.ReplaceExisting);
                await FileIO.WriteTextAsync(localFile, data);
                return true;
            }
            catch (FileNotFoundException ex)
            {
            }

            return false;
        }

        private string ToJson&amp;lt;T&amp;gt;(T requestObject)
        {
            var data = &amp;quot;&amp;quot;;

            try
            {
                data = JsonConvert.SerializeObject(requestObject,
                                                      new JsonSerializerSettings()
                                                      {
                                                          NullValueHandling = NullValueHandling.Ignore
                                                      });
            }
            catch (Exception ex)
            {
            }

            return data;
        }

        private T FromJson&amp;lt;T&amp;gt;(string data)
        {
            T returnObject = JsonConvert.DeserializeObject&amp;lt;T&amp;gt;(data);
            return returnObject;
        }
    }
&lt;/code&gt;&lt;/pre&gt;
</content>
		<summary>&lt;p&gt;A class in C# that can be used to store and retrieve complex objects to/from Win8 local storage. The class requires JSON.NET Nuget library and of course it relies on the Win8 StorageFile class (hence the Windows.Storage namespace).&lt;/p&gt;</summary>
	</entry>
</feed>